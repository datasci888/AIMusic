{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-29T22:42:16.859100Z","iopub.execute_input":"2023-06-29T22:42:16.859590Z","iopub.status.idle":"2023-06-29T22:42:16.882738Z","shell.execute_reply.started":"2023-06-29T22:42:16.859553Z","shell.execute_reply":"2023-06-29T22:42:16.881376Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"/kaggle/input/TransformersAllYears/dfTransformersAllYears.csv\n/kaggle/input/2023-kaggle-ai-report/sample_submission.csv\n/kaggle/input/2023-kaggle-ai-report/arxiv_metadata_20230510.json\n/kaggle/input/2023-kaggle-ai-report/kaggle_writeups_20230510.csv\n/kaggle/input/AIMusic1000/AIMusic1000.csv\n/kaggle/input/Input/Music_2022_2023_final.csv\n/kaggle/input/Input/Music_B42021_final.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import re","metadata":{"execution":{"iopub.status.busy":"2023-06-29T22:42:17.159590Z","iopub.execute_input":"2023-06-29T22:42:17.160387Z","iopub.status.idle":"2023-06-29T22:42:17.165944Z","shell.execute_reply.started":"2023-06-29T22:42:17.160347Z","shell.execute_reply":"2023-06-29T22:42:17.164574Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', None)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T22:42:17.493620Z","iopub.execute_input":"2023-06-29T22:42:17.494161Z","iopub.status.idle":"2023-06-29T22:42:17.500101Z","shell.execute_reply.started":"2023-06-29T22:42:17.494125Z","shell.execute_reply":"2023-06-29T22:42:17.498886Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df = pd.read_json('/kaggle/input/2023-kaggle-ai-report/arxiv_metadata_20230510.json', lines=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T22:42:17.832507Z","iopub.execute_input":"2023-06-29T22:42:17.833014Z","iopub.status.idle":"2023-06-29T22:45:07.293509Z","shell.execute_reply.started":"2023-06-29T22:42:17.832957Z","shell.execute_reply":"2023-06-29T22:45:07.291290Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df.abstract[0]","metadata":{"execution":{"iopub.status.busy":"2023-06-29T22:45:07.320331Z","iopub.execute_input":"2023-06-29T22:45:07.320969Z","iopub.status.idle":"2023-06-29T22:45:07.380313Z","shell.execute_reply.started":"2023-06-29T22:45:07.320932Z","shell.execute_reply":"2023-06-29T22:45:07.378554Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'  A fully differential calculation in perturbative quantum chromodynamics is\\npresented for the production of massive photon pairs at hadron colliders. All\\nnext-to-leading order perturbative contributions from quark-antiquark,\\ngluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\\nall-orders resummation of initial-state gluon radiation valid at\\nnext-to-next-to-leading logarithmic accuracy. The region of phase space is\\nspecified in which the calculation is most reliable. Good agreement is\\ndemonstrated with data from the Fermilab Tevatron, and predictions are made for\\nmore detailed tests with CDF and DO data. Predictions are shown for\\ndistributions of diphoton pairs produced at the energy of the Large Hadron\\nCollider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\\nboson are contrasted with those produced from QCD processes at the LHC, showing\\nthat enhanced sensitivity to the signal can be obtained with judicious\\nselection of events.\\n'"},"metadata":{}}]},{"cell_type":"code","source":"# df.to_csv('/kaggle/working/musicAI.csv', index=True, header=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T22:46:31.669422Z","iopub.execute_input":"2023-06-29T22:46:31.669903Z","iopub.status.idle":"2023-06-29T22:46:31.677039Z","shell.execute_reply.started":"2023-06-29T22:46:31.669872Z","shell.execute_reply":"2023-06-29T22:46:31.675372Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2023-06-29T22:46:31.691171Z","iopub.execute_input":"2023-06-29T22:46:31.691581Z","iopub.status.idle":"2023-06-29T22:46:31.701351Z","shell.execute_reply.started":"2023-06-29T22:46:31.691552Z","shell.execute_reply":"2023-06-29T22:46:31.700009Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"Index(['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi',\n       'report-no', 'categories', 'license', 'abstract', 'versions',\n       'update_date', 'authors_parsed'],\n      dtype='object')"},"metadata":{}}]},{"cell_type":"markdown","source":"### PLOT Number of Papers by Year","metadata":{}},{"cell_type":"code","source":"dfVersions = pd.DataFrame(df['versions'].apply(lambda x: [version['created'][-17:-13] for version in x if version['version'] == 'v1'][0] if any(version['version'] == 'v1' for version in x) else None))","metadata":{"execution":{"iopub.status.busy":"2023-06-29T22:46:31.704007Z","iopub.execute_input":"2023-06-29T22:46:31.704850Z","iopub.status.idle":"2023-06-29T22:46:37.832971Z","shell.execute_reply.started":"2023-06-29T22:46:31.704803Z","shell.execute_reply":"2023-06-29T22:46:37.831979Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"dfVersions","metadata":{"execution":{"iopub.status.busy":"2023-06-29T22:46:37.835240Z","iopub.execute_input":"2023-06-29T22:46:37.835957Z","iopub.status.idle":"2023-06-29T22:46:37.849774Z","shell.execute_reply.started":"2023-06-29T22:46:37.835917Z","shell.execute_reply":"2023-06-29T22:46:37.848337Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"        versions\n0           2007\n1           2007\n2           2007\n3           2007\n4           2007\n...          ...\n2250218     1996\n2250219     1996\n2250220     1996\n2250221     1996\n2250222     1996\n\n[2250223 rows x 1 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>versions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2250218</th>\n      <td>1996</td>\n    </tr>\n    <tr>\n      <th>2250219</th>\n      <td>1996</td>\n    </tr>\n    <tr>\n      <th>2250220</th>\n      <td>1996</td>\n    </tr>\n    <tr>\n      <th>2250221</th>\n      <td>1996</td>\n    </tr>\n    <tr>\n      <th>2250222</th>\n      <td>1996</td>\n    </tr>\n  </tbody>\n</table>\n<p>2250223 rows × 1 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"dfVersions['versions'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-06-29T22:46:37.852193Z","iopub.execute_input":"2023-06-29T22:46:37.852839Z","iopub.status.idle":"2023-06-29T22:46:38.356516Z","shell.execute_reply.started":"2023-06-29T22:46:37.852789Z","shell.execute_reply":"2023-06-29T22:46:38.355096Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"2022    185976\n2021    181599\n2020    178275\n2019    155917\n2018    140377\n2017    123781\n2016    113440\n2015    105130\n2014     97590\n2013     92875\n2012     84374\n2011     76602\n2010     70288\n2023     64429\n2009     64071\n2008     58810\n2007     55749\n2006     50305\n2005     46874\n2004     43713\n2003     39392\n2002     36105\n2001     33140\n2000     30669\n1999     27700\n1998     24170\n1997     19610\n1996     15872\n1995     13006\n1994     10078\n1993      6729\n1992      3190\n1991       353\n1990        26\n1989         6\n1988         1\n1986         1\nName: versions, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"dfVersions.max()","metadata":{"execution":{"iopub.status.busy":"2023-06-29T22:46:38.360292Z","iopub.execute_input":"2023-06-29T22:46:38.360728Z","iopub.status.idle":"2023-06-29T22:46:39.373617Z","shell.execute_reply.started":"2023-06-29T22:46:38.360690Z","shell.execute_reply":"2023-06-29T22:46:39.372210Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"versions    2023\ndtype: object"},"metadata":{}}]},{"cell_type":"code","source":"dfVersions['versions'].value_counts().dtypes","metadata":{"execution":{"iopub.status.busy":"2023-06-29T22:46:39.375547Z","iopub.execute_input":"2023-06-29T22:46:39.376117Z","iopub.status.idle":"2023-06-29T22:46:39.841595Z","shell.execute_reply.started":"2023-06-29T22:46:39.376078Z","shell.execute_reply":"2023-06-29T22:46:39.839929Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"dtype('int64')"},"metadata":{}}]},{"cell_type":"code","source":"import plotly.graph_objects as go\n\nyears = dfVersions['versions'].value_counts().index.tolist()\nno_of_papers = dfVersions['versions'].value_counts().values.tolist()\n\nindex_2023 = years.index('2023')\nyears.insert(0, years.pop(index_2023))\nno_of_papers.insert(0, no_of_papers.pop(index_2023))\n\ncolors = ['blue' if year in ['2022', '2023'] else 'lightblue' for year in years]\n\nfig = go.Figure(data=[go.Bar(x=years, y=no_of_papers, marker_color=colors)])\n\nfig.update_layout(\n    title='Number of Papers by Year',\n    xaxis_title='Years',\n    yaxis_title='No of papers',\n    xaxis_tickangle=-45,\n    plot_bgcolor='black',\n    paper_bgcolor='black',\n    font=dict(color='white'),\n)\n\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-06-29T22:46:39.843834Z","iopub.execute_input":"2023-06-29T22:46:39.844381Z","iopub.status.idle":"2023-06-29T22:46:41.174120Z","shell.execute_reply.started":"2023-06-29T22:46:39.844336Z","shell.execute_reply":"2023-06-29T22:46:41.172849Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/html":"        <script type=\"text/javascript\">\n        window.PlotlyConfig = {MathJaxConfig: 'local'};\n        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n        if (typeof require !== 'undefined') {\n        require.undef(\"plotly\");\n        requirejs.config({\n            paths: {\n                'plotly': ['https://cdn.plot.ly/plotly-2.20.0.min']\n            }\n        });\n        require(['plotly'], function(Plotly) {\n            window._Plotly = Plotly;\n        });\n        }\n        </script>\n        "},"metadata":{}},{"output_type":"display_data","data":{"text/html":"<div>                            <div id=\"015f51ab-9430-4082-8a30-765bcb2cfa2a\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"015f51ab-9430-4082-8a30-765bcb2cfa2a\")) {                    Plotly.newPlot(                        \"015f51ab-9430-4082-8a30-765bcb2cfa2a\",                        [{\"marker\":{\"color\":[\"blue\",\"blue\",\"lightblue\",\"lightblue\",\"lightblue\",\"lightblue\",\"lightblue\",\"lightblue\",\"lightblue\",\"lightblue\",\"lightblue\",\"lightblue\",\"lightblue\",\"lightblue\",\"lightblue\",\"lightblue\",\"lightblue\",\"lightblue\",\"lightblue\",\"lightblue\",\"lightblue\",\"lightblue\",\"lightblue\",\"lightblue\",\"lightblue\",\"lightblue\",\"lightblue\",\"lightblue\",\"lightblue\",\"lightblue\",\"lightblue\",\"lightblue\",\"lightblue\",\"lightblue\",\"lightblue\",\"lightblue\",\"lightblue\"]},\"x\":[\"2023\",\"2022\",\"2021\",\"2020\",\"2019\",\"2018\",\"2017\",\"2016\",\"2015\",\"2014\",\"2013\",\"2012\",\"2011\",\"2010\",\"2009\",\"2008\",\"2007\",\"2006\",\"2005\",\"2004\",\"2003\",\"2002\",\"2001\",\"2000\",\"1999\",\"1998\",\"1997\",\"1996\",\"1995\",\"1994\",\"1993\",\"1992\",\"1991\",\"1990\",\"1989\",\"1988\",\"1986\"],\"y\":[64429,185976,181599,178275,155917,140377,123781,113440,105130,97590,92875,84374,76602,70288,64071,58810,55749,50305,46874,43713,39392,36105,33140,30669,27700,24170,19610,15872,13006,10078,6729,3190,353,26,6,1,1],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"title\":{\"text\":\"Years\"},\"tickangle\":-45},\"font\":{\"color\":\"white\"},\"title\":{\"text\":\"Number of Papers by Year\"},\"yaxis\":{\"title\":{\"text\":\"No of papers\"}},\"plot_bgcolor\":\"black\",\"paper_bgcolor\":\"black\"},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('015f51ab-9430-4082-8a30-765bcb2cfa2a');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Filter CS papers","metadata":{}},{"cell_type":"code","source":"CSkeywords = ['computer science', 'algorithm', 'programming', 'data structure', 'artificial intelligence',\n            'machine learning', 'data mining', 'computer vision', 'natural language processing',\n            'software engineering', 'human-computer interaction', 'database', 'networking', 'cybersecurity',\n            'cloud computing', 'big data', 'web development', 'image processing', 'parallel computing',\n            'operating systems', 'information retrieval', 'computer graphics', 'computer architecture',\n            'computer networks', 'distributed systems', 'data science', 'bioinformatics', 'AI',\n            'Artificial Intelligence', 'ML', 'Machine Learning', 'Transformers', 'LLMS',\n            'Large Language Models', 'Machine Vision', 'LSTM', 'CNN', 'Deep Learning',\n            'Neural Networks', 'Natural Language Processing', 'Computer Vision',\n            'Reinforcement Learning', 'Generative Models', 'Robotic Process Automation',\n            'Speech Recognition', 'Virtual Assistants', 'Recommendation Systems',\n            'Data Science', 'Data Mining', 'Big Data', 'Pattern Recognition',\n            'Predictive Analytics', 'Image Recognition', 'Supervised Learning',\n            'Unsupervised Learning', 'Semi-Supervised Learning', 'Transfer Learning',\n            'AutoML', 'Data Engineering', 'Data Preprocessing', 'Data Visualization',\n            'Feature Extraction', 'Dimensionality Reduction', 'Ensemble Learning',\n            'Cloud Computing', 'Internet of Things', 'Chatbots',\n            'Deep Reinforcement Learning', 'Self-Supervised Learning',\n            'Explainable AI', 'Bias in AI', 'Ethical AI', 'AI Governance', 'AI Ethics']\n","metadata":{"execution":{"iopub.status.busy":"2023-06-29T22:46:41.176023Z","iopub.execute_input":"2023-06-29T22:46:41.176490Z","iopub.status.idle":"2023-06-29T22:46:41.186294Z","shell.execute_reply.started":"2023-06-29T22:46:41.176455Z","shell.execute_reply":"2023-06-29T22:46:41.185312Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"dfMusicCS = df[ df['title'].apply(lambda abstract: any(keyword in abstract for keyword in CSkeywords))|\n    df['abstract'].apply(lambda abstract: any(keyword in abstract for keyword in CSkeywords))]","metadata":{"execution":{"iopub.status.busy":"2023-06-29T22:46:41.187598Z","iopub.execute_input":"2023-06-29T22:46:41.188019Z","iopub.status.idle":"2023-06-29T22:49:07.715580Z","shell.execute_reply.started":"2023-06-29T22:46:41.187971Z","shell.execute_reply":"2023-06-29T22:49:07.714129Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"dfCS = dfMusicCS","metadata":{"execution":{"iopub.status.busy":"2023-06-29T22:49:07.717358Z","iopub.execute_input":"2023-06-29T22:49:07.717766Z","iopub.status.idle":"2023-06-29T22:49:07.724956Z","shell.execute_reply.started":"2023-06-29T22:49:07.717733Z","shell.execute_reply":"2023-06-29T22:49:07.722926Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2023-06-29T22:49:07.731369Z","iopub.execute_input":"2023-06-29T22:49:07.731807Z","iopub.status.idle":"2023-06-29T22:49:07.745012Z","shell.execute_reply.started":"2023-06-29T22:49:07.731777Z","shell.execute_reply":"2023-06-29T22:49:07.743618Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"(2250223, 14)"},"metadata":{}}]},{"cell_type":"code","source":"dfCS.shape","metadata":{"execution":{"iopub.status.busy":"2023-06-29T22:49:07.747235Z","iopub.execute_input":"2023-06-29T22:49:07.747696Z","iopub.status.idle":"2023-06-29T22:49:07.763492Z","shell.execute_reply.started":"2023-06-29T22:49:07.747663Z","shell.execute_reply":"2023-06-29T22:49:07.762103Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"(343497, 14)"},"metadata":{}}]},{"cell_type":"code","source":"dfCS.columns","metadata":{"execution":{"iopub.status.busy":"2023-06-29T22:49:07.765453Z","iopub.execute_input":"2023-06-29T22:49:07.766173Z","iopub.status.idle":"2023-06-29T22:49:07.781084Z","shell.execute_reply.started":"2023-06-29T22:49:07.765939Z","shell.execute_reply":"2023-06-29T22:49:07.779411Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"Index(['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi',\n       'report-no', 'categories', 'license', 'abstract', 'versions',\n       'update_date', 'authors_parsed'],\n      dtype='object')"},"metadata":{}}]},{"cell_type":"code","source":"dftitleabstract = dfCS[['title', 'abstract']]","metadata":{"execution":{"iopub.status.busy":"2023-06-29T22:49:07.783028Z","iopub.execute_input":"2023-06-29T22:49:07.783524Z","iopub.status.idle":"2023-06-29T22:49:07.830374Z","shell.execute_reply.started":"2023-06-29T22:49:07.783479Z","shell.execute_reply":"2023-06-29T22:49:07.829146Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"dftitleabstract.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-29T22:49:07.832428Z","iopub.execute_input":"2023-06-29T22:49:07.832909Z","iopub.status.idle":"2023-06-29T22:49:07.847372Z","shell.execute_reply.started":"2023-06-29T22:49:07.832854Z","shell.execute_reply":"2023-06-29T22:49:07.846062Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"                                                                                                             title  \\\n1                                                                         Sparsity-certifying Graph Decompositions   \n10  Computing genus 2 Hilbert-Siegel modular forms over $\\Q(\\sqrt{5})$ via\\n  the Jacquet-Langlands correspondence   \n47            Inference on white dwarf binary systems using the first round Mock LISA\\n  Data Challenges data sets   \n48                                                    An algorithm for the classification of smooth Fano polytopes   \n61                                                  On-line Viterbi Algorithm and Its Relationship to Random Walks   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     abstract  \n1     We describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use\\nit obtain a characterization of the family of $(k,\\ell)$-sparse graphs and\\nalgorithmic solutions to a family of problems concerning tree decompositions of\\ngraphs. Special instances of sparse graphs appear in rigidity theory and have\\nreceived increased attention in recent years. In particular, our colored\\npebbles generalize and strengthen the previous results of Lee and Streinu and\\ngive a new proof of the Tutte-Nash-Williams characterization of arboricity. We\\nalso present a new decomposition that certifies sparsity based on the\\n$(k,\\ell)$-pebble game with colors. Our work also exposes connections between\\npebble game algorithms and previous sparse graph algorithms by Gabow, Gabow and\\nWestermann and Hendrickson.\\n  \n10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    In this paper we present an algorithm for computing Hecke eigensystems of\\nHilbert-Siegel cusp forms over real quadratic fields of narrow class number\\none. We give some illustrative examples using the quadratic field\\n$\\Q(\\sqrt{5})$. In those examples, we identify Hilbert-Siegel eigenforms that\\nare possible lifts from Hilbert eigenforms.\\n  \n47                                                             We report on the analysis of selected single source data sets from the first\\nround of the Mock LISA Data Challenges (MLDC) for white dwarf binaries. We\\nimplemented an end-to-end pipeline consisting of a grid-based coherent\\npre-processing unit for signal detection, and an automatic Markov Chain Monte\\nCarlo post-processing unit for signal evaluation. We demonstrate that signal\\ndetection with our coherent approach is secure and accurate, and is increased\\nin accuracy and supplemented with additional information on the signal\\nparameters by our Markov Chain Monte Carlo approach. We also demonstrate that\\nthe Markov Chain Monte Carlo routine is additionally able to determine\\naccurately the noise level in the frequency window of interest.\\n  \n48                                                                                                                                                                                                                                                                                                                                                                                                                                            We present an algorithm that produces the classification list of smooth Fano\\nd-polytopes for any given d. The input of the algorithm is a single number,\\nnamely the positive integer d. The algorithm has been used to classify smooth\\nFano d-polytopes for d<=7. There are 7622 isomorphism classes of smooth Fano\\n6-polytopes and 72256 isomorphism classes of smooth Fano 7-polytopes.\\n  \n61                                                                                In this paper, we introduce the on-line Viterbi algorithm for decoding hidden\\nMarkov models (HMMs) in much smaller than linear space. Our analysis on\\ntwo-state HMMs suggests that the expected maximum memory used to decode\\nsequence of length $n$ with $m$-state HMM can be as low as $\\Theta(m\\log n)$,\\nwithout a significant slow-down compared to the classical Viterbi algorithm.\\nClassical Viterbi algorithm requires $O(mn)$ space, which is impractical for\\nanalysis of long DNA sequences (such as complete human genome chromosomes) and\\nfor continuous data streams. We also experimentally demonstrate the performance\\nof the on-line Viterbi algorithm on a simple HMM for gene finding on both\\nsimulated and real DNA sequences.\\n  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>abstract</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>Sparsity-certifying Graph Decompositions</td>\n      <td>We describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use\\nit obtain a characterization of the family of $(k,\\ell)$-sparse graphs and\\nalgorithmic solutions to a family of problems concerning tree decompositions of\\ngraphs. Special instances of sparse graphs appear in rigidity theory and have\\nreceived increased attention in recent years. In particular, our colored\\npebbles generalize and strengthen the previous results of Lee and Streinu and\\ngive a new proof of the Tutte-Nash-Williams characterization of arboricity. We\\nalso present a new decomposition that certifies sparsity based on the\\n$(k,\\ell)$-pebble game with colors. Our work also exposes connections between\\npebble game algorithms and previous sparse graph algorithms by Gabow, Gabow and\\nWestermann and Hendrickson.\\n</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Computing genus 2 Hilbert-Siegel modular forms over $\\Q(\\sqrt{5})$ via\\n  the Jacquet-Langlands correspondence</td>\n      <td>In this paper we present an algorithm for computing Hecke eigensystems of\\nHilbert-Siegel cusp forms over real quadratic fields of narrow class number\\none. We give some illustrative examples using the quadratic field\\n$\\Q(\\sqrt{5})$. In those examples, we identify Hilbert-Siegel eigenforms that\\nare possible lifts from Hilbert eigenforms.\\n</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>Inference on white dwarf binary systems using the first round Mock LISA\\n  Data Challenges data sets</td>\n      <td>We report on the analysis of selected single source data sets from the first\\nround of the Mock LISA Data Challenges (MLDC) for white dwarf binaries. We\\nimplemented an end-to-end pipeline consisting of a grid-based coherent\\npre-processing unit for signal detection, and an automatic Markov Chain Monte\\nCarlo post-processing unit for signal evaluation. We demonstrate that signal\\ndetection with our coherent approach is secure and accurate, and is increased\\nin accuracy and supplemented with additional information on the signal\\nparameters by our Markov Chain Monte Carlo approach. We also demonstrate that\\nthe Markov Chain Monte Carlo routine is additionally able to determine\\naccurately the noise level in the frequency window of interest.\\n</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>An algorithm for the classification of smooth Fano polytopes</td>\n      <td>We present an algorithm that produces the classification list of smooth Fano\\nd-polytopes for any given d. The input of the algorithm is a single number,\\nnamely the positive integer d. The algorithm has been used to classify smooth\\nFano d-polytopes for d&lt;=7. There are 7622 isomorphism classes of smooth Fano\\n6-polytopes and 72256 isomorphism classes of smooth Fano 7-polytopes.\\n</td>\n    </tr>\n    <tr>\n      <th>61</th>\n      <td>On-line Viterbi Algorithm and Its Relationship to Random Walks</td>\n      <td>In this paper, we introduce the on-line Viterbi algorithm for decoding hidden\\nMarkov models (HMMs) in much smaller than linear space. Our analysis on\\ntwo-state HMMs suggests that the expected maximum memory used to decode\\nsequence of length $n$ with $m$-state HMM can be as low as $\\Theta(m\\log n)$,\\nwithout a significant slow-down compared to the classical Viterbi algorithm.\\nClassical Viterbi algorithm requires $O(mn)$ space, which is impractical for\\nanalysis of long DNA sequences (such as complete human genome chromosomes) and\\nfor continuous data streams. We also experimentally demonstrate the performance\\nof the on-line Viterbi algorithm on a simple HMM for gene finding on both\\nsimulated and real DNA sequences.\\n</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"dftitleabstract.to_csv('dfCS.csv')","metadata":{"execution":{"iopub.status.busy":"2023-06-29T22:49:07.849212Z","iopub.execute_input":"2023-06-29T22:49:07.849607Z","iopub.status.idle":"2023-06-29T22:49:28.375892Z","shell.execute_reply.started":"2023-06-29T22:49:07.849558Z","shell.execute_reply":"2023-06-29T22:49:28.374636Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"### Filter AI ML papers","metadata":{}},{"cell_type":"code","source":"AI_keywords = ['AI', 'Artificial Intelligence', 'ML', 'Machine Learning', 'Transformers', 'LLMS', 'Large Language Models', 'Machine Vision', 'LSTM', 'CNN', 'Deep Learning', 'Neural Networks', 'Natural Language Processing', 'Computer Vision', 'Reinforcement Learning', 'Generative Models', 'Robotic Process Automation', 'Speech Recognition', 'Virtual Assistants', 'Recommendation Systems', 'Data Science', 'Data Mining', 'Big Data', 'Pattern Recognition', 'Predictive Analytics', 'Image Recognition', 'Supervised Learning', 'Unsupervised Learning', 'Semi-Supervised Learning', 'Transfer Learning', 'AutoML', 'Data Engineering', 'Data Preprocessing', 'Data Visualization', 'Feature Extraction', 'Dimensionality Reduction', 'Ensemble Learning', 'Cloud Computing', 'Internet of Things', 'Chatbots', 'Deep Reinforcement Learning', 'Self-Supervised Learning', 'Explainable AI', 'Bias in AI', 'Ethical AI', 'AI Governance', 'AI Ethics']","metadata":{"execution":{"iopub.status.busy":"2023-06-29T22:49:28.377921Z","iopub.execute_input":"2023-06-29T22:49:28.378341Z","iopub.status.idle":"2023-06-29T22:49:28.387935Z","shell.execute_reply.started":"2023-06-29T22:49:28.378310Z","shell.execute_reply":"2023-06-29T22:49:28.386405Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"df['abstract'][0:2]","metadata":{"execution":{"iopub.status.busy":"2023-06-29T22:49:28.389639Z","iopub.execute_input":"2023-06-29T22:49:28.390320Z","iopub.status.idle":"2023-06-29T22:49:28.407865Z","shell.execute_reply.started":"2023-06-29T22:49:28.390289Z","shell.execute_reply":"2023-06-29T22:49:28.406443Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"0      A fully differential calculation in perturbative quantum chromodynamics is\\npresented for the production of massive photon pairs at hadron colliders. All\\nnext-to-leading order perturbative contributions from quark-antiquark,\\ngluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\\nall-orders resummation of initial-state gluon radiation valid at\\nnext-to-next-to-leading logarithmic accuracy. The region of phase space is\\nspecified in which the calculation is most reliable. Good agreement is\\ndemonstrated with data from the Fermilab Tevatron, and predictions are made for\\nmore detailed tests with CDF and DO data. Predictions are shown for\\ndistributions of diphoton pairs produced at the energy of the Large Hadron\\nCollider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\\nboson are contrasted with those produced from QCD processes at the LHC, showing\\nthat enhanced sensitivity to the signal can be obtained with judicious\\nselection of events.\\n\n1                                                                                                                                                                                                  We describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use\\nit obtain a characterization of the family of $(k,\\ell)$-sparse graphs and\\nalgorithmic solutions to a family of problems concerning tree decompositions of\\ngraphs. Special instances of sparse graphs appear in rigidity theory and have\\nreceived increased attention in recent years. In particular, our colored\\npebbles generalize and strengthen the previous results of Lee and Streinu and\\ngive a new proof of the Tutte-Nash-Williams characterization of arboricity. We\\nalso present a new decomposition that certifies sparsity based on the\\n$(k,\\ell)$-pebble game with colors. Our work also exposes connections between\\npebble game algorithms and previous sparse graph algorithms by Gabow, Gabow and\\nWestermann and Hendrickson.\\n\nName: abstract, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"df['abstract'] = df['abstract'].apply(lambda text: re.sub(r'[^a-zA-Z\\s]', '', text.replace(\"\\n\", \"\")).strip())","metadata":{"execution":{"iopub.status.busy":"2023-06-29T22:49:28.409923Z","iopub.execute_input":"2023-06-29T22:49:28.410771Z","iopub.status.idle":"2023-06-29T22:51:10.296825Z","shell.execute_reply.started":"2023-06-29T22:49:28.410724Z","shell.execute_reply":"2023-06-29T22:51:10.295418Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"df['title'] = df['title'].apply(lambda text: re.sub(r'[^a-zA-Z\\s]', '', text.replace(\"\\n\", \"\")).strip())","metadata":{"execution":{"iopub.status.busy":"2023-06-29T22:51:10.298723Z","iopub.execute_input":"2023-06-29T22:51:10.299107Z","iopub.status.idle":"2023-06-29T22:51:22.970686Z","shell.execute_reply.started":"2023-06-29T22:51:10.299068Z","shell.execute_reply":"2023-06-29T22:51:22.969123Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"df['abstract'][0:2]","metadata":{"execution":{"iopub.status.busy":"2023-06-29T22:51:22.972386Z","iopub.execute_input":"2023-06-29T22:51:22.973596Z","iopub.status.idle":"2023-06-29T22:51:22.982562Z","shell.execute_reply.started":"2023-06-29T22:51:22.973559Z","shell.execute_reply":"2023-06-29T22:51:22.981261Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"0    A fully differential calculation in perturbative quantum chromodynamics ispresented for the production of massive photon pairs at hadron colliders Allnexttoleading order perturbative contributions from quarkantiquarkgluonantiquark and gluongluon subprocesses are included as well asallorders resummation of initialstate gluon radiation valid atnexttonexttoleading logarithmic accuracy The region of phase space isspecified in which the calculation is most reliable Good agreement isdemonstrated with data from the Fermilab Tevatron and predictions are made formore detailed tests with CDF and DO data Predictions are shown fordistributions of diphoton pairs produced at the energy of the Large HadronCollider LHC Distributions of the diphoton pairs from the decay of a Higgsboson are contrasted with those produced from QCD processes at the LHC showingthat enhanced sensitivity to the signal can be obtained with judiciousselection of events\n1                                                                                                                                                                                                We describe a new algorithm the kellpebble game with colors and useit obtain a characterization of the family of kellsparse graphs andalgorithmic solutions to a family of problems concerning tree decompositions ofgraphs Special instances of sparse graphs appear in rigidity theory and havereceived increased attention in recent years In particular our coloredpebbles generalize and strengthen the previous results of Lee and Streinu andgive a new proof of the TutteNashWilliams characterization of arboricity Wealso present a new decomposition that certifies sparsity based on thekellpebble game with colors Our work also exposes connections betweenpebble game algorithms and previous sparse graph algorithms by Gabow Gabow andWestermann and Hendrickson\nName: abstract, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"df['title'][0:2]","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:14:06.273337Z","iopub.status.idle":"2023-06-29T23:14:06.273970Z","shell.execute_reply.started":"2023-06-29T23:14:06.273662Z","shell.execute_reply":"2023-06-29T23:14:06.273690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2023-06-29T22:51:23.008815Z","iopub.execute_input":"2023-06-29T22:51:23.009424Z","iopub.status.idle":"2023-06-29T22:51:23.073752Z","shell.execute_reply.started":"2023-06-29T22:51:23.009375Z","shell.execute_reply":"2023-06-29T22:51:23.072178Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"                       id           submitter  \\\n0               0704.0001      Pavel Nadolsky   \n1               0704.0002        Louis Theran   \n2               0704.0003         Hongjun Pan   \n3               0704.0004        David Callan   \n4               0704.0005  Alberto Torchinsky   \n...                   ...                 ...   \n2250218  supr-con/9608008     Ruslan Prozorov   \n2250219  supr-con/9609001  Durga P. Choudhury   \n2250220  supr-con/9609002  Durga P. Choudhury   \n2250221  supr-con/9609003   Hasegawa Yasumasa   \n2250222  supr-con/9609004    Masanori Ichioka   \n\n                                                                                                                                                            authors  \\\n0                                                                                                             C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-P. Yuan   \n1                                                                                                                                   Ileana Streinu and Louis Theran   \n2                                                                                                                                                       Hongjun Pan   \n3                                                                                                                                                      David Callan   \n4                                                                                                                          Wael Abu-Shammala and Alberto Torchinsky   \n...                                                                                                                                                             ...   \n2250218                                                                  R. Prozorov, M. Konczykowski, B. Schmidt, Y. Yeshurun, A. Shaulov, C.\\n  Villard, G. Koren   \n2250219  Durga P. Choudhury, Balam A. Willemsen, John S. Derov and S. Sridhar\\n  (Physics Department, Northeastern University and Rome Laboratory, Hanscom\\n  AFB.)   \n2250220                             Balam A. Willemsen, J. S. Derov and S.Sridhar (Physics Department,\\n  Northeastern University and Rome Laboratory, Hanscom AFB)   \n2250221                                                                                                          Yasumasa Hasegawa (Himeji Institute of Technology)   \n2250222                                                                                       Naoki Enomoto, Masanori Ichioka and Kazushige Machida (Okayama Univ.)   \n\n                                                                                                            title  \\\n0                          Calculation of prompt diphoton production cross sections at Tevatron and  LHC energies   \n1                                                                         Sparsitycertifying Graph Decompositions   \n2                               The evolution of the EarthMoon system based on the dark matter field  fluid model   \n3                         A determinant of Stirling cycle numbers counts unlabeled acyclic  singlesource automata   \n4                                                                          From dyadic Lambdaalpha to Lambdaalpha   \n...                                                                                                           ...   \n2250218         On the origin of the irreversibility line in thin YBaCuO films with and  without columnar defects   \n2250219                Nonlinear Response of HTSC Thin Film Microwave Resonators in an Applied  DC Magnetic Field   \n2250220                    Critical State Flux Penetration and Linear Microwave Vortex Response in  YBaCuOx Films   \n2250221  Density of States and NMR Relaxation Rate in Anisotropic  Superconductivity with Intersecting Line Nodes   \n2250222                    Ginzburg Landau theory for dwave pairing and fourfold symmetric vortex  core structure   \n\n                                                                                                                                                                                                                  comments  \\\n0                                                                                                                                                                                  37 pages, 15 figures; published version   \n1                                                                                                                                                                                    To appear in Graphs and Combinatorics   \n2                                                                                                                                                                                                      23 pages, 3 figures   \n3                                                                                                                                                                                                                 11 pages   \n4                                                                                                                                                                                                                     None   \n...                                                                                                                                                                                                                    ...   \n2250218                                                                                                                      19 pages, LaTex, 6 PostScript figures; Author's Homepage:\\n  http://www.biu.ac.il:80/~prozorr   \n2250219  4 pages, LaTeX type, Uses IEEE style files, 600 dpi PostScript file\\n  with color figures available at http://sagar.physics.neu.edu/preprints.html\\n  Submitted to IEEE Transactions on Applied Superconductivity   \n2250220                 20 pages, LaTeX type, Uses REVTeX style files, Submitted to Physical\\n  Review B, 600 dpi PostScript file with high resolution figures available at\\n  http://sagar.physics.neu.edu/preprints.html   \n2250221                                                                                                                                               7 pages, 4 PostScript Figures, LaTeX, to appear in J. Phys. Soc. Jpn   \n2250222                                                                                                                                                        12 pages including 8 eps figs, LaTeX with jpsj.sty & epsfig   \n\n                                       journal-ref  \\\n0                         Phys.Rev.D76:013009,2007   \n1                                             None   \n2                                             None   \n3                                             None   \n4        Illinois J. Math. 52 (2008) no.2, 681-689   \n...                                            ...   \n2250218                                       None   \n2250219                                       None   \n2250220                                       None   \n2250221                                       None   \n2250222         J. Phys. Soc. Jpn. 66, 204 (1997).   \n\n                                doi         report-no  \\\n0        10.1103/PhysRevD.76.013009  ANL-HEP-PR-07-12   \n1                              None              None   \n2                              None              None   \n3                              None              None   \n4                              None              None   \n...                             ...               ...   \n2250218   10.1103/PhysRevB.54.15530              None   \n2250219           10.1109/77.620744              None   \n2250220   10.1103/PhysRevB.56.11989              None   \n2250221        10.1143/JPSJ.65.3131              None   \n2250222         10.1143/JPSJ.66.204              None   \n\n                         categories  \\\n0                            hep-ph   \n1                     math.CO cs.CG   \n2                    physics.gen-ph   \n3                           math.CO   \n4                   math.CA math.FA   \n...                             ...   \n2250218  supr-con cond-mat.supr-con   \n2250219  supr-con cond-mat.supr-con   \n2250220  supr-con cond-mat.supr-con   \n2250221  supr-con cond-mat.supr-con   \n2250222  supr-con cond-mat.supr-con   \n\n                                                     license  \\\n0                                                       None   \n1        http://arxiv.org/licenses/nonexclusive-distrib/1.0/   \n2                                                       None   \n3                                                       None   \n4                                                       None   \n...                                                      ...   \n2250218                                                 None   \n2250219                                                 None   \n2250220                                                 None   \n2250221                                                 None   \n2250222                                                 None   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 abstract  \\\n0                                                                           A fully differential calculation in perturbative quantum chromodynamics ispresented for the production of massive photon pairs at hadron colliders Allnexttoleading order perturbative contributions from quarkantiquarkgluonantiquark and gluongluon subprocesses are included as well asallorders resummation of initialstate gluon radiation valid atnexttonexttoleading logarithmic accuracy The region of phase space isspecified in which the calculation is most reliable Good agreement isdemonstrated with data from the Fermilab Tevatron and predictions are made formore detailed tests with CDF and DO data Predictions are shown fordistributions of diphoton pairs produced at the energy of the Large HadronCollider LHC Distributions of the diphoton pairs from the decay of a Higgsboson are contrasted with those produced from QCD processes at the LHC showingthat enhanced sensitivity to the signal can be obtained with judiciousselection of events   \n1                                                                                                                                                                                                                                                                       We describe a new algorithm the kellpebble game with colors and useit obtain a characterization of the family of kellsparse graphs andalgorithmic solutions to a family of problems concerning tree decompositions ofgraphs Special instances of sparse graphs appear in rigidity theory and havereceived increased attention in recent years In particular our coloredpebbles generalize and strengthen the previous results of Lee and Streinu andgive a new proof of the TutteNashWilliams characterization of arboricity Wealso present a new decomposition that certifies sparsity based on thekellpebble game with colors Our work also exposes connections betweenpebble game algorithms and previous sparse graph algorithms by Gabow Gabow andWestermann and Hendrickson   \n2                                                                                                                                                                                                                          The evolution of EarthMoon system is described by the dark matter fieldfluid model proposed in the Meeting of Division of Particle and Field American Physical Society The current behavior of the EarthMoon system agreeswith this model very well and the general pattern of the evolution of theMoonEarth system described by this model agrees with geological and fossilevidence The closest distance of the Moon to Earth was about  km at billion years ago which is far beyond the Roches limit The result suggeststhat the tidal friction may not be the primary cause for the evolution of theEarthMoon system The average dark matter field fluid constant derived fromEarthMoon system data is  x  sm This model predictsthat the Marss rotation is also slowing with the angular acceleration rateabout  x  rad s   \n3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          We show that a determinant of Stirling cycle numbers counts unlabeled acyclicsinglesource automata The proof involves a bijection from these automata tocertain marked lattice paths and a signreversing involution to evaluate thedeterminant   \n4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    In this paper we show how to compute the Lambdaalpha norm alphage using the dyadic grid This result is a consequence of the description ofthe Hardy spaces HpRN in terms of dyadic and special atoms   \n...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ...   \n2250218                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       We report on measurements of the angular dependence of the irreversibilitytemperature Tirrtheta  in YBaCuOdelta  thin films definedby the onset of a third harmonic signal and measured by a miniature Hall probeFrom the functional form of Tirrtheta we conclude that the origin ofthe irreversibility line in unirradiated films is a dynamic crossover from anunpinned to a pinned vortex liquid In irradiated films the irreversibilitytemperature is determined by the trapping angle   \n2250219                                                                                                                                                                                                                                                                                                                                                                                               The nonlinear microwave surface impedance of patterned YBCO thin films wasmeasured using a suspended line resonator in the presence of a perpendicular DCmagnetic field of magnitude comparable to that of the microwave fieldSignature of the virgin state was found to be absent even for relatively lowmicrowave power levels The microwave loss was initially found to decrease forsmall applied DC field before increasing again Also nonlinearities inherentin the sample were found to be substantially suppressed at low powers at theseapplied fields These two features together can lead to significant improvementin device performance   \n2250220  The vortex contribution to the dc field H dependent microwave surfaceimpedance Zs  RsiXs of YBaCuOx thin films was measured usingsuspended patterned resonators ZsH is shown to be a direct measure of theflux density BH enabling a very precise test of models of flux penetrationThree regimes of fielddependent behavior were observed  Initial fluxpenetration occurs on very low field scales HiK Oe  At moderatefields the flux penetration into the virgin state is in excellent agreementwith calculations based upon the fieldinduced Bean critical state for thinfilm geometry parametrized by a field scale HsK Jcd T  for veryhigh fields H Hs the flux density is uniform and the measurements enabledirect determination of vortex parameters such as pinning force constantsalphap and vortex viscosity eta However hysteresis loops are indisagreement with the thin film Bean model and instead are governed by the lowfield scale Hi rather than by Hs Geometric barriers are insufficient toaccount for the observed results   \n2250221                                                                                                                                                                                                                                                                                                                                                                                                                                      We show that the density of states in an anisotropic superconductor withintersecting line nodes in the gap function is proportional to E log alphaDelta E for E  Delta where Delta is the maximum value ofthe gap function and alpha is constant while it is proportional to E ifthe line nodes do not intersect As a result a logarithmic correction appearsin the temperature dependence of the NMR relaxation rate and the specific heatwhich can be observed experimentally By comparing with those for the heavyfermion superconductors we can obtain information about the symmetry of thegap function   \n2250222                                                                                                                                                                                                                                                                                                                                                                       The Ginzburg Landau theory for dxywave superconductors isconstructed by starting from the Gorkov equation with including correctionterms up to the next order of lnTcT Some of the nonlocal correction termsare found to break the cylindrical symmetry and lead to the fourfold symmetriccore structure reflecting the internal degree of freedom in the pairpotential Using this extended Ginzburg Landau theory we investigate thefourfold symmetric structure of the pair potential current and magnetic fieldaround an isolated single vortex and clarify concretely how the vortex corestructure deviates from the cylindrical symmetry in the dxywavesuperconductors   \n\n                                                                                                                                                                                            versions  \\\n0                                                                      [{'version': 'v1', 'created': 'Mon, 2 Apr 2007 19:18:42 GMT'}, {'version': 'v2', 'created': 'Tue, 24 Jul 2007 20:10:27 GMT'}]   \n1                                                                     [{'version': 'v1', 'created': 'Sat, 31 Mar 2007 02:26:18 GMT'}, {'version': 'v2', 'created': 'Sat, 13 Dec 2008 17:26:00 GMT'}]   \n2        [{'version': 'v1', 'created': 'Sun, 1 Apr 2007 20:46:54 GMT'}, {'version': 'v2', 'created': 'Sat, 8 Dec 2007 23:47:24 GMT'}, {'version': 'v3', 'created': 'Sun, 13 Jan 2008 00:36:28 GMT'}]   \n3                                                                                                                                    [{'version': 'v1', 'created': 'Sat, 31 Mar 2007 03:16:14 GMT'}]   \n4                                                                                                                                     [{'version': 'v1', 'created': 'Mon, 2 Apr 2007 18:09:58 GMT'}]   \n...                                                                                                                                                                                              ...   \n2250218                                                                                                                              [{'version': 'v1', 'created': 'Mon, 26 Aug 1996 15:08:35 GMT'}]   \n2250219                                                                                                                              [{'version': 'v1', 'created': 'Sat, 31 Aug 1996 17:34:38 GMT'}]   \n2250220                                                                                                                               [{'version': 'v1', 'created': 'Tue, 3 Sep 1996 14:08:26 GMT'}]   \n2250221                                                                                                                              [{'version': 'v1', 'created': 'Wed, 18 Sep 1996 07:57:29 GMT'}]   \n2250222                                                                                                                              [{'version': 'v1', 'created': 'Wed, 25 Sep 1996 14:17:09 GMT'}]   \n\n        update_date  \\\n0        2008-11-26   \n1        2008-12-13   \n2        2008-01-13   \n3        2007-05-23   \n4        2013-10-15   \n...             ...   \n2250218  2009-10-30   \n2250219  2016-11-18   \n2250220  2009-10-30   \n2250221  2009-10-30   \n2250222  2009-10-30   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                          authors_parsed  \n0                                                                                                                                                                                                                                                                                                                                                             [[Balázs, C., ], [Berger, E. L., ], [Nadolsky, P. M., ], [Yuan, C. -P., ]]  \n1                                                                                                                                                                                                                                                                                                                                                                                               [[Streinu, Ileana, ], [Theran, Louis, ]]  \n2                                                                                                                                                                                                                                                                                                                                                                                                                     [[Pan, Hongjun, ]]  \n3                                                                                                                                                                                                                                                                                                                                                                                                                    [[Callan, David, ]]  \n4                                                                                                                                                                                                                                                                                                                                                                                      [[Abu-Shammala, Wael, ], [Torchinsky, Alberto, ]]  \n...                                                                                                                                                                                                                                                                                                                                                                                                                                  ...  \n2250218                                                                                                                                                                                                                                                                                                     [[Prozorov, R., ], [Konczykowski, M., ], [Schmidt, B., ], [Yeshurun, Y., ], [Shaulov, A., ], [Villard, C., ], [Koren, G., ]]  \n2250219  [[Choudhury, Durga P., , Physics Department, Northeastern University and Rome Laboratory, Hanscom\\n  AFB.], [Willemsen, Balam A., , Physics Department, Northeastern University and Rome Laboratory, Hanscom\\n  AFB.], [Derov, John S., , Physics Department, Northeastern University and Rome Laboratory, Hanscom\\n  AFB.], [Sridhar, S., , Physics Department, Northeastern University and Rome Laboratory, Hanscom\\n  AFB.]]  \n2250220                                                                                                                  [[Willemsen, Balam A., , Physics Department,\\n  Northeastern University and Rome Laboratory, Hanscom AFB], [Derov, J. S., , Physics Department,\\n  Northeastern University and Rome Laboratory, Hanscom AFB], [Sridhar, S., , Physics Department,\\n  Northeastern University and Rome Laboratory, Hanscom AFB]]  \n2250221                                                                                                                                                                                                                                                                                                                                                                         [[Hasegawa, Yasumasa, , Himeji Institute of Technology]]  \n2250222                                                                                                                                                                                                                                                                                                                 [[Enomoto, Naoki, , Okayama Univ.], [Ichioka, Masanori, , Okayama Univ.], [Machida, Kazushige, , Okayama Univ.]]  \n\n[2250223 rows x 14 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>submitter</th>\n      <th>authors</th>\n      <th>title</th>\n      <th>comments</th>\n      <th>journal-ref</th>\n      <th>doi</th>\n      <th>report-no</th>\n      <th>categories</th>\n      <th>license</th>\n      <th>abstract</th>\n      <th>versions</th>\n      <th>update_date</th>\n      <th>authors_parsed</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0704.0001</td>\n      <td>Pavel Nadolsky</td>\n      <td>C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-P. Yuan</td>\n      <td>Calculation of prompt diphoton production cross sections at Tevatron and  LHC energies</td>\n      <td>37 pages, 15 figures; published version</td>\n      <td>Phys.Rev.D76:013009,2007</td>\n      <td>10.1103/PhysRevD.76.013009</td>\n      <td>ANL-HEP-PR-07-12</td>\n      <td>hep-ph</td>\n      <td>None</td>\n      <td>A fully differential calculation in perturbative quantum chromodynamics ispresented for the production of massive photon pairs at hadron colliders Allnexttoleading order perturbative contributions from quarkantiquarkgluonantiquark and gluongluon subprocesses are included as well asallorders resummation of initialstate gluon radiation valid atnexttonexttoleading logarithmic accuracy The region of phase space isspecified in which the calculation is most reliable Good agreement isdemonstrated with data from the Fermilab Tevatron and predictions are made formore detailed tests with CDF and DO data Predictions are shown fordistributions of diphoton pairs produced at the energy of the Large HadronCollider LHC Distributions of the diphoton pairs from the decay of a Higgsboson are contrasted with those produced from QCD processes at the LHC showingthat enhanced sensitivity to the signal can be obtained with judiciousselection of events</td>\n      <td>[{'version': 'v1', 'created': 'Mon, 2 Apr 2007 19:18:42 GMT'}, {'version': 'v2', 'created': 'Tue, 24 Jul 2007 20:10:27 GMT'}]</td>\n      <td>2008-11-26</td>\n      <td>[[Balázs, C., ], [Berger, E. L., ], [Nadolsky, P. M., ], [Yuan, C. -P., ]]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0704.0002</td>\n      <td>Louis Theran</td>\n      <td>Ileana Streinu and Louis Theran</td>\n      <td>Sparsitycertifying Graph Decompositions</td>\n      <td>To appear in Graphs and Combinatorics</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>math.CO cs.CG</td>\n      <td>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</td>\n      <td>We describe a new algorithm the kellpebble game with colors and useit obtain a characterization of the family of kellsparse graphs andalgorithmic solutions to a family of problems concerning tree decompositions ofgraphs Special instances of sparse graphs appear in rigidity theory and havereceived increased attention in recent years In particular our coloredpebbles generalize and strengthen the previous results of Lee and Streinu andgive a new proof of the TutteNashWilliams characterization of arboricity Wealso present a new decomposition that certifies sparsity based on thekellpebble game with colors Our work also exposes connections betweenpebble game algorithms and previous sparse graph algorithms by Gabow Gabow andWestermann and Hendrickson</td>\n      <td>[{'version': 'v1', 'created': 'Sat, 31 Mar 2007 02:26:18 GMT'}, {'version': 'v2', 'created': 'Sat, 13 Dec 2008 17:26:00 GMT'}]</td>\n      <td>2008-12-13</td>\n      <td>[[Streinu, Ileana, ], [Theran, Louis, ]]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0704.0003</td>\n      <td>Hongjun Pan</td>\n      <td>Hongjun Pan</td>\n      <td>The evolution of the EarthMoon system based on the dark matter field  fluid model</td>\n      <td>23 pages, 3 figures</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>physics.gen-ph</td>\n      <td>None</td>\n      <td>The evolution of EarthMoon system is described by the dark matter fieldfluid model proposed in the Meeting of Division of Particle and Field American Physical Society The current behavior of the EarthMoon system agreeswith this model very well and the general pattern of the evolution of theMoonEarth system described by this model agrees with geological and fossilevidence The closest distance of the Moon to Earth was about  km at billion years ago which is far beyond the Roches limit The result suggeststhat the tidal friction may not be the primary cause for the evolution of theEarthMoon system The average dark matter field fluid constant derived fromEarthMoon system data is  x  sm This model predictsthat the Marss rotation is also slowing with the angular acceleration rateabout  x  rad s</td>\n      <td>[{'version': 'v1', 'created': 'Sun, 1 Apr 2007 20:46:54 GMT'}, {'version': 'v2', 'created': 'Sat, 8 Dec 2007 23:47:24 GMT'}, {'version': 'v3', 'created': 'Sun, 13 Jan 2008 00:36:28 GMT'}]</td>\n      <td>2008-01-13</td>\n      <td>[[Pan, Hongjun, ]]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0704.0004</td>\n      <td>David Callan</td>\n      <td>David Callan</td>\n      <td>A determinant of Stirling cycle numbers counts unlabeled acyclic  singlesource automata</td>\n      <td>11 pages</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>math.CO</td>\n      <td>None</td>\n      <td>We show that a determinant of Stirling cycle numbers counts unlabeled acyclicsinglesource automata The proof involves a bijection from these automata tocertain marked lattice paths and a signreversing involution to evaluate thedeterminant</td>\n      <td>[{'version': 'v1', 'created': 'Sat, 31 Mar 2007 03:16:14 GMT'}]</td>\n      <td>2007-05-23</td>\n      <td>[[Callan, David, ]]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0704.0005</td>\n      <td>Alberto Torchinsky</td>\n      <td>Wael Abu-Shammala and Alberto Torchinsky</td>\n      <td>From dyadic Lambdaalpha to Lambdaalpha</td>\n      <td>None</td>\n      <td>Illinois J. Math. 52 (2008) no.2, 681-689</td>\n      <td>None</td>\n      <td>None</td>\n      <td>math.CA math.FA</td>\n      <td>None</td>\n      <td>In this paper we show how to compute the Lambdaalpha norm alphage using the dyadic grid This result is a consequence of the description ofthe Hardy spaces HpRN in terms of dyadic and special atoms</td>\n      <td>[{'version': 'v1', 'created': 'Mon, 2 Apr 2007 18:09:58 GMT'}]</td>\n      <td>2013-10-15</td>\n      <td>[[Abu-Shammala, Wael, ], [Torchinsky, Alberto, ]]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2250218</th>\n      <td>supr-con/9608008</td>\n      <td>Ruslan Prozorov</td>\n      <td>R. Prozorov, M. Konczykowski, B. Schmidt, Y. Yeshurun, A. Shaulov, C.\\n  Villard, G. Koren</td>\n      <td>On the origin of the irreversibility line in thin YBaCuO films with and  without columnar defects</td>\n      <td>19 pages, LaTex, 6 PostScript figures; Author's Homepage:\\n  http://www.biu.ac.il:80/~prozorr</td>\n      <td>None</td>\n      <td>10.1103/PhysRevB.54.15530</td>\n      <td>None</td>\n      <td>supr-con cond-mat.supr-con</td>\n      <td>None</td>\n      <td>We report on measurements of the angular dependence of the irreversibilitytemperature Tirrtheta  in YBaCuOdelta  thin films definedby the onset of a third harmonic signal and measured by a miniature Hall probeFrom the functional form of Tirrtheta we conclude that the origin ofthe irreversibility line in unirradiated films is a dynamic crossover from anunpinned to a pinned vortex liquid In irradiated films the irreversibilitytemperature is determined by the trapping angle</td>\n      <td>[{'version': 'v1', 'created': 'Mon, 26 Aug 1996 15:08:35 GMT'}]</td>\n      <td>2009-10-30</td>\n      <td>[[Prozorov, R., ], [Konczykowski, M., ], [Schmidt, B., ], [Yeshurun, Y., ], [Shaulov, A., ], [Villard, C., ], [Koren, G., ]]</td>\n    </tr>\n    <tr>\n      <th>2250219</th>\n      <td>supr-con/9609001</td>\n      <td>Durga P. Choudhury</td>\n      <td>Durga P. Choudhury, Balam A. Willemsen, John S. Derov and S. Sridhar\\n  (Physics Department, Northeastern University and Rome Laboratory, Hanscom\\n  AFB.)</td>\n      <td>Nonlinear Response of HTSC Thin Film Microwave Resonators in an Applied  DC Magnetic Field</td>\n      <td>4 pages, LaTeX type, Uses IEEE style files, 600 dpi PostScript file\\n  with color figures available at http://sagar.physics.neu.edu/preprints.html\\n  Submitted to IEEE Transactions on Applied Superconductivity</td>\n      <td>None</td>\n      <td>10.1109/77.620744</td>\n      <td>None</td>\n      <td>supr-con cond-mat.supr-con</td>\n      <td>None</td>\n      <td>The nonlinear microwave surface impedance of patterned YBCO thin films wasmeasured using a suspended line resonator in the presence of a perpendicular DCmagnetic field of magnitude comparable to that of the microwave fieldSignature of the virgin state was found to be absent even for relatively lowmicrowave power levels The microwave loss was initially found to decrease forsmall applied DC field before increasing again Also nonlinearities inherentin the sample were found to be substantially suppressed at low powers at theseapplied fields These two features together can lead to significant improvementin device performance</td>\n      <td>[{'version': 'v1', 'created': 'Sat, 31 Aug 1996 17:34:38 GMT'}]</td>\n      <td>2016-11-18</td>\n      <td>[[Choudhury, Durga P., , Physics Department, Northeastern University and Rome Laboratory, Hanscom\\n  AFB.], [Willemsen, Balam A., , Physics Department, Northeastern University and Rome Laboratory, Hanscom\\n  AFB.], [Derov, John S., , Physics Department, Northeastern University and Rome Laboratory, Hanscom\\n  AFB.], [Sridhar, S., , Physics Department, Northeastern University and Rome Laboratory, Hanscom\\n  AFB.]]</td>\n    </tr>\n    <tr>\n      <th>2250220</th>\n      <td>supr-con/9609002</td>\n      <td>Durga P. Choudhury</td>\n      <td>Balam A. Willemsen, J. S. Derov and S.Sridhar (Physics Department,\\n  Northeastern University and Rome Laboratory, Hanscom AFB)</td>\n      <td>Critical State Flux Penetration and Linear Microwave Vortex Response in  YBaCuOx Films</td>\n      <td>20 pages, LaTeX type, Uses REVTeX style files, Submitted to Physical\\n  Review B, 600 dpi PostScript file with high resolution figures available at\\n  http://sagar.physics.neu.edu/preprints.html</td>\n      <td>None</td>\n      <td>10.1103/PhysRevB.56.11989</td>\n      <td>None</td>\n      <td>supr-con cond-mat.supr-con</td>\n      <td>None</td>\n      <td>The vortex contribution to the dc field H dependent microwave surfaceimpedance Zs  RsiXs of YBaCuOx thin films was measured usingsuspended patterned resonators ZsH is shown to be a direct measure of theflux density BH enabling a very precise test of models of flux penetrationThree regimes of fielddependent behavior were observed  Initial fluxpenetration occurs on very low field scales HiK Oe  At moderatefields the flux penetration into the virgin state is in excellent agreementwith calculations based upon the fieldinduced Bean critical state for thinfilm geometry parametrized by a field scale HsK Jcd T  for veryhigh fields H Hs the flux density is uniform and the measurements enabledirect determination of vortex parameters such as pinning force constantsalphap and vortex viscosity eta However hysteresis loops are indisagreement with the thin film Bean model and instead are governed by the lowfield scale Hi rather than by Hs Geometric barriers are insufficient toaccount for the observed results</td>\n      <td>[{'version': 'v1', 'created': 'Tue, 3 Sep 1996 14:08:26 GMT'}]</td>\n      <td>2009-10-30</td>\n      <td>[[Willemsen, Balam A., , Physics Department,\\n  Northeastern University and Rome Laboratory, Hanscom AFB], [Derov, J. S., , Physics Department,\\n  Northeastern University and Rome Laboratory, Hanscom AFB], [Sridhar, S., , Physics Department,\\n  Northeastern University and Rome Laboratory, Hanscom AFB]]</td>\n    </tr>\n    <tr>\n      <th>2250221</th>\n      <td>supr-con/9609003</td>\n      <td>Hasegawa Yasumasa</td>\n      <td>Yasumasa Hasegawa (Himeji Institute of Technology)</td>\n      <td>Density of States and NMR Relaxation Rate in Anisotropic  Superconductivity with Intersecting Line Nodes</td>\n      <td>7 pages, 4 PostScript Figures, LaTeX, to appear in J. Phys. Soc. Jpn</td>\n      <td>None</td>\n      <td>10.1143/JPSJ.65.3131</td>\n      <td>None</td>\n      <td>supr-con cond-mat.supr-con</td>\n      <td>None</td>\n      <td>We show that the density of states in an anisotropic superconductor withintersecting line nodes in the gap function is proportional to E log alphaDelta E for E  Delta where Delta is the maximum value ofthe gap function and alpha is constant while it is proportional to E ifthe line nodes do not intersect As a result a logarithmic correction appearsin the temperature dependence of the NMR relaxation rate and the specific heatwhich can be observed experimentally By comparing with those for the heavyfermion superconductors we can obtain information about the symmetry of thegap function</td>\n      <td>[{'version': 'v1', 'created': 'Wed, 18 Sep 1996 07:57:29 GMT'}]</td>\n      <td>2009-10-30</td>\n      <td>[[Hasegawa, Yasumasa, , Himeji Institute of Technology]]</td>\n    </tr>\n    <tr>\n      <th>2250222</th>\n      <td>supr-con/9609004</td>\n      <td>Masanori Ichioka</td>\n      <td>Naoki Enomoto, Masanori Ichioka and Kazushige Machida (Okayama Univ.)</td>\n      <td>Ginzburg Landau theory for dwave pairing and fourfold symmetric vortex  core structure</td>\n      <td>12 pages including 8 eps figs, LaTeX with jpsj.sty &amp; epsfig</td>\n      <td>J. Phys. Soc. Jpn. 66, 204 (1997).</td>\n      <td>10.1143/JPSJ.66.204</td>\n      <td>None</td>\n      <td>supr-con cond-mat.supr-con</td>\n      <td>None</td>\n      <td>The Ginzburg Landau theory for dxywave superconductors isconstructed by starting from the Gorkov equation with including correctionterms up to the next order of lnTcT Some of the nonlocal correction termsare found to break the cylindrical symmetry and lead to the fourfold symmetriccore structure reflecting the internal degree of freedom in the pairpotential Using this extended Ginzburg Landau theory we investigate thefourfold symmetric structure of the pair potential current and magnetic fieldaround an isolated single vortex and clarify concretely how the vortex corestructure deviates from the cylindrical symmetry in the dxywavesuperconductors</td>\n      <td>[{'version': 'v1', 'created': 'Wed, 25 Sep 1996 14:17:09 GMT'}]</td>\n      <td>2009-10-30</td>\n      <td>[[Enomoto, Naoki, , Okayama Univ.], [Ichioka, Masanori, , Okayama Univ.], [Machida, Kazushige, , Okayama Univ.]]</td>\n    </tr>\n  </tbody>\n</table>\n<p>2250223 rows × 14 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# dfAI = df[df['title'].apply(lambda abstract: any(keyword in abstract for keyword in AI_keywords))|\n#     df['abstract'].apply(lambda abstract: any(keyword in abstract for keyword in AI_keywords))]","metadata":{"execution":{"iopub.status.busy":"2023-06-29T22:51:23.076337Z","iopub.execute_input":"2023-06-29T22:51:23.076846Z","iopub.status.idle":"2023-06-29T22:51:23.082652Z","shell.execute_reply.started":"2023-06-29T22:51:23.076801Z","shell.execute_reply":"2023-06-29T22:51:23.081523Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# # Filter df on 'title' and 'abstract' columns | ignore case\n# dfAI1 = df[\n#     df['title'].str.contains('|'.join(AI_keywords), case=False) |\n#     df['abstract'].str.contains('|'.join(AI_keywords), case=False)]","metadata":{"execution":{"iopub.status.busy":"2023-06-29T22:51:23.084418Z","iopub.execute_input":"2023-06-29T22:51:23.085783Z","iopub.status.idle":"2023-06-29T22:51:23.097046Z","shell.execute_reply.started":"2023-06-29T22:51:23.085746Z","shell.execute_reply":"2023-06-29T22:51:23.096052Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"dfAI1 = pd.DataFrame()","metadata":{"execution":{"iopub.status.busy":"2023-06-29T22:51:23.098893Z","iopub.execute_input":"2023-06-29T22:51:23.099350Z","iopub.status.idle":"2023-06-29T22:51:23.112007Z","shell.execute_reply.started":"2023-06-29T22:51:23.099317Z","shell.execute_reply":"2023-06-29T22:51:23.110621Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"dfAI1 = df[df['title'].str.lower().str.contains('|'.join(AI_keywords), case=False) |\n           df['abstract'].str.lower().str.contains('|'.join(AI_keywords), case=False)]\n","metadata":{"execution":{"iopub.status.busy":"2023-06-29T22:51:23.113762Z","iopub.execute_input":"2023-06-29T22:51:23.114254Z","iopub.status.idle":"2023-06-29T23:13:34.963172Z","shell.execute_reply.started":"2023-06-29T22:51:23.114213Z","shell.execute_reply":"2023-06-29T23:13:34.961693Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:13:34.965088Z","iopub.execute_input":"2023-06-29T23:13:34.965479Z","iopub.status.idle":"2023-06-29T23:13:34.976397Z","shell.execute_reply.started":"2023-06-29T23:13:34.965449Z","shell.execute_reply":"2023-06-29T23:13:34.975184Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"(2250223, 14)"},"metadata":{}}]},{"cell_type":"code","source":"dfAI1.shape","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:13:35.556816Z","iopub.execute_input":"2023-06-29T23:13:35.557772Z","iopub.status.idle":"2023-06-29T23:13:35.565639Z","shell.execute_reply.started":"2023-06-29T23:13:35.557728Z","shell.execute_reply":"2023-06-29T23:13:35.564147Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"(1485549, 14)"},"metadata":{}}]},{"cell_type":"code","source":"dfAI1.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:13:35.567895Z","iopub.execute_input":"2023-06-29T23:13:35.568465Z","iopub.status.idle":"2023-06-29T23:13:35.612964Z","shell.execute_reply.started":"2023-06-29T23:13:35.568419Z","shell.execute_reply":"2023-06-29T23:13:35.611393Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"          id          submitter  \\\n0  0704.0001     Pavel Nadolsky   \n1  0704.0002       Louis Theran   \n3  0704.0004       David Callan   \n5  0704.0006       Yue Hin Pong   \n6  0704.0007  Alejandro Corichi   \n\n                                                   authors  \\\n0    C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-P. Yuan   \n1                          Ileana Streinu and Louis Theran   \n3                                             David Callan   \n5                                 Y. H. Pong and C. K. Law   \n6  Alejandro Corichi, Tatjana Vukasinac and Jose A. Zapata   \n\n                                                                                     title  \\\n0   Calculation of prompt diphoton production cross sections at Tevatron and  LHC energies   \n1                                                  Sparsitycertifying Graph Decompositions   \n3  A determinant of Stirling cycle numbers counts unlabeled acyclic  singlesource automata   \n5                               Bosonic characters of atomic Cooper pairs across resonance   \n6                                        Polymer Quantum Mechanics and its Continuum Limit   \n\n                                                           comments  \\\n0                           37 pages, 15 figures; published version   \n1                             To appear in Graphs and Combinatorics   \n3                                                          11 pages   \n5                               6 pages, 4 figures, accepted by PRA   \n6  16 pages, no figures. Typos corrected to match published version   \n\n                journal-ref                         doi         report-no  \\\n0  Phys.Rev.D76:013009,2007  10.1103/PhysRevD.76.013009  ANL-HEP-PR-07-12   \n1                      None                        None              None   \n3                      None                        None              None   \n5                      None  10.1103/PhysRevA.75.043613              None   \n6  Phys.Rev.D76:044016,2007  10.1103/PhysRevD.76.044016      IGPG-07/03-2   \n\n          categories                                              license  \\\n0             hep-ph                                                 None   \n1      math.CO cs.CG  http://arxiv.org/licenses/nonexclusive-distrib/1.0/   \n3            math.CO                                                 None   \n5  cond-mat.mes-hall                                                 None   \n6              gr-qc                                                 None   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       abstract  \\\n0                                                                 A fully differential calculation in perturbative quantum chromodynamics ispresented for the production of massive photon pairs at hadron colliders Allnexttoleading order perturbative contributions from quarkantiquarkgluonantiquark and gluongluon subprocesses are included as well asallorders resummation of initialstate gluon radiation valid atnexttonexttoleading logarithmic accuracy The region of phase space isspecified in which the calculation is most reliable Good agreement isdemonstrated with data from the Fermilab Tevatron and predictions are made formore detailed tests with CDF and DO data Predictions are shown fordistributions of diphoton pairs produced at the energy of the Large HadronCollider LHC Distributions of the diphoton pairs from the decay of a Higgsboson are contrasted with those produced from QCD processes at the LHC showingthat enhanced sensitivity to the signal can be obtained with judiciousselection of events   \n1                                                                                                                                                                                                                                                             We describe a new algorithm the kellpebble game with colors and useit obtain a characterization of the family of kellsparse graphs andalgorithmic solutions to a family of problems concerning tree decompositions ofgraphs Special instances of sparse graphs appear in rigidity theory and havereceived increased attention in recent years In particular our coloredpebbles generalize and strengthen the previous results of Lee and Streinu andgive a new proof of the TutteNashWilliams characterization of arboricity Wealso present a new decomposition that certifies sparsity based on thekellpebble game with colors Our work also exposes connections betweenpebble game algorithms and previous sparse graph algorithms by Gabow Gabow andWestermann and Hendrickson   \n3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                We show that a determinant of Stirling cycle numbers counts unlabeled acyclicsinglesource automata The proof involves a bijection from these automata tocertain marked lattice paths and a signreversing involution to evaluate thedeterminant   \n5                                                                                                                                      We study the twoparticle wave function of paired atoms in a Fermi gas withtunable interaction strengths controlled by Feshbach resonance The Cooper pairwave function is examined for its bosonic characters which is quantified bythe correction of Bose enhancement factor associated with the creation andannihilation composite particle operators An example is given for athreedimensional uniform gas Two definitions of Cooper pair wave function areexamined One of which is chosen to reflect the offdiagonal long range orderODLRO Another one corresponds to a pair projection of a BCS state On theside with negative scattering length we found that paired atoms described byODLRO are more bosonic than the pair projected definition It is also foundthat at kF a ge  both definitions give similar results where morethan  of the atoms occupy the corresponding molecular condensates   \n6  A rather nonstandard quantum representation of the canonical commutationrelations of quantum mechanics systems known as the polymer representation hasgained some attention in recent years due to its possible relation with Planckscale physics In particular this approach has been followed in a symmetricsector of loop quantum gravity known as loop quantum cosmology Here we exploredifferent aspects of the relation between the ordinary Schroedinger theory andthe polymer description The paper has two parts In the first one we derivethe polymer quantum mechanics starting from the ordinary Schroedinger theoryand show that the polymer description arises as an appropriate limit In thesecond part we consider the continuum limit of this theory namely the reverseprocess in which one starts from the discrete theory and tries to recover backthe ordinary Schroedinger quantum mechanics We consider several examples ofinterest including the harmonic oscillator the free particle and a simplecosmological model   \n\n                                                                                                                         versions  \\\n0   [{'version': 'v1', 'created': 'Mon, 2 Apr 2007 19:18:42 GMT'}, {'version': 'v2', 'created': 'Tue, 24 Jul 2007 20:10:27 GMT'}]   \n1  [{'version': 'v1', 'created': 'Sat, 31 Mar 2007 02:26:18 GMT'}, {'version': 'v2', 'created': 'Sat, 13 Dec 2008 17:26:00 GMT'}]   \n3                                                                 [{'version': 'v1', 'created': 'Sat, 31 Mar 2007 03:16:14 GMT'}]   \n5                                                                 [{'version': 'v1', 'created': 'Sat, 31 Mar 2007 04:24:59 GMT'}]   \n6  [{'version': 'v1', 'created': 'Sat, 31 Mar 2007 04:27:22 GMT'}, {'version': 'v2', 'created': 'Wed, 22 Aug 2007 22:42:11 GMT'}]   \n\n  update_date  \\\n0  2008-11-26   \n1  2008-12-13   \n3  2007-05-23   \n5  2015-05-13   \n6  2008-11-26   \n\n                                                               authors_parsed  \n0  [[Balázs, C., ], [Berger, E. L., ], [Nadolsky, P. M., ], [Yuan, C. -P., ]]  \n1                                    [[Streinu, Ileana, ], [Theran, Louis, ]]  \n3                                                         [[Callan, David, ]]  \n5                                           [[Pong, Y. H., ], [Law, C. K., ]]  \n6       [[Corichi, Alejandro, ], [Vukasinac, Tatjana, ], [Zapata, Jose A., ]]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>submitter</th>\n      <th>authors</th>\n      <th>title</th>\n      <th>comments</th>\n      <th>journal-ref</th>\n      <th>doi</th>\n      <th>report-no</th>\n      <th>categories</th>\n      <th>license</th>\n      <th>abstract</th>\n      <th>versions</th>\n      <th>update_date</th>\n      <th>authors_parsed</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0704.0001</td>\n      <td>Pavel Nadolsky</td>\n      <td>C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-P. Yuan</td>\n      <td>Calculation of prompt diphoton production cross sections at Tevatron and  LHC energies</td>\n      <td>37 pages, 15 figures; published version</td>\n      <td>Phys.Rev.D76:013009,2007</td>\n      <td>10.1103/PhysRevD.76.013009</td>\n      <td>ANL-HEP-PR-07-12</td>\n      <td>hep-ph</td>\n      <td>None</td>\n      <td>A fully differential calculation in perturbative quantum chromodynamics ispresented for the production of massive photon pairs at hadron colliders Allnexttoleading order perturbative contributions from quarkantiquarkgluonantiquark and gluongluon subprocesses are included as well asallorders resummation of initialstate gluon radiation valid atnexttonexttoleading logarithmic accuracy The region of phase space isspecified in which the calculation is most reliable Good agreement isdemonstrated with data from the Fermilab Tevatron and predictions are made formore detailed tests with CDF and DO data Predictions are shown fordistributions of diphoton pairs produced at the energy of the Large HadronCollider LHC Distributions of the diphoton pairs from the decay of a Higgsboson are contrasted with those produced from QCD processes at the LHC showingthat enhanced sensitivity to the signal can be obtained with judiciousselection of events</td>\n      <td>[{'version': 'v1', 'created': 'Mon, 2 Apr 2007 19:18:42 GMT'}, {'version': 'v2', 'created': 'Tue, 24 Jul 2007 20:10:27 GMT'}]</td>\n      <td>2008-11-26</td>\n      <td>[[Balázs, C., ], [Berger, E. L., ], [Nadolsky, P. M., ], [Yuan, C. -P., ]]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0704.0002</td>\n      <td>Louis Theran</td>\n      <td>Ileana Streinu and Louis Theran</td>\n      <td>Sparsitycertifying Graph Decompositions</td>\n      <td>To appear in Graphs and Combinatorics</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>math.CO cs.CG</td>\n      <td>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</td>\n      <td>We describe a new algorithm the kellpebble game with colors and useit obtain a characterization of the family of kellsparse graphs andalgorithmic solutions to a family of problems concerning tree decompositions ofgraphs Special instances of sparse graphs appear in rigidity theory and havereceived increased attention in recent years In particular our coloredpebbles generalize and strengthen the previous results of Lee and Streinu andgive a new proof of the TutteNashWilliams characterization of arboricity Wealso present a new decomposition that certifies sparsity based on thekellpebble game with colors Our work also exposes connections betweenpebble game algorithms and previous sparse graph algorithms by Gabow Gabow andWestermann and Hendrickson</td>\n      <td>[{'version': 'v1', 'created': 'Sat, 31 Mar 2007 02:26:18 GMT'}, {'version': 'v2', 'created': 'Sat, 13 Dec 2008 17:26:00 GMT'}]</td>\n      <td>2008-12-13</td>\n      <td>[[Streinu, Ileana, ], [Theran, Louis, ]]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0704.0004</td>\n      <td>David Callan</td>\n      <td>David Callan</td>\n      <td>A determinant of Stirling cycle numbers counts unlabeled acyclic  singlesource automata</td>\n      <td>11 pages</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>math.CO</td>\n      <td>None</td>\n      <td>We show that a determinant of Stirling cycle numbers counts unlabeled acyclicsinglesource automata The proof involves a bijection from these automata tocertain marked lattice paths and a signreversing involution to evaluate thedeterminant</td>\n      <td>[{'version': 'v1', 'created': 'Sat, 31 Mar 2007 03:16:14 GMT'}]</td>\n      <td>2007-05-23</td>\n      <td>[[Callan, David, ]]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0704.0006</td>\n      <td>Yue Hin Pong</td>\n      <td>Y. H. Pong and C. K. Law</td>\n      <td>Bosonic characters of atomic Cooper pairs across resonance</td>\n      <td>6 pages, 4 figures, accepted by PRA</td>\n      <td>None</td>\n      <td>10.1103/PhysRevA.75.043613</td>\n      <td>None</td>\n      <td>cond-mat.mes-hall</td>\n      <td>None</td>\n      <td>We study the twoparticle wave function of paired atoms in a Fermi gas withtunable interaction strengths controlled by Feshbach resonance The Cooper pairwave function is examined for its bosonic characters which is quantified bythe correction of Bose enhancement factor associated with the creation andannihilation composite particle operators An example is given for athreedimensional uniform gas Two definitions of Cooper pair wave function areexamined One of which is chosen to reflect the offdiagonal long range orderODLRO Another one corresponds to a pair projection of a BCS state On theside with negative scattering length we found that paired atoms described byODLRO are more bosonic than the pair projected definition It is also foundthat at kF a ge  both definitions give similar results where morethan  of the atoms occupy the corresponding molecular condensates</td>\n      <td>[{'version': 'v1', 'created': 'Sat, 31 Mar 2007 04:24:59 GMT'}]</td>\n      <td>2015-05-13</td>\n      <td>[[Pong, Y. H., ], [Law, C. K., ]]</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0704.0007</td>\n      <td>Alejandro Corichi</td>\n      <td>Alejandro Corichi, Tatjana Vukasinac and Jose A. Zapata</td>\n      <td>Polymer Quantum Mechanics and its Continuum Limit</td>\n      <td>16 pages, no figures. Typos corrected to match published version</td>\n      <td>Phys.Rev.D76:044016,2007</td>\n      <td>10.1103/PhysRevD.76.044016</td>\n      <td>IGPG-07/03-2</td>\n      <td>gr-qc</td>\n      <td>None</td>\n      <td>A rather nonstandard quantum representation of the canonical commutationrelations of quantum mechanics systems known as the polymer representation hasgained some attention in recent years due to its possible relation with Planckscale physics In particular this approach has been followed in a symmetricsector of loop quantum gravity known as loop quantum cosmology Here we exploredifferent aspects of the relation between the ordinary Schroedinger theory andthe polymer description The paper has two parts In the first one we derivethe polymer quantum mechanics starting from the ordinary Schroedinger theoryand show that the polymer description arises as an appropriate limit In thesecond part we consider the continuum limit of this theory namely the reverseprocess in which one starts from the discrete theory and tries to recover backthe ordinary Schroedinger quantum mechanics We consider several examples ofinterest including the harmonic oscillator the free particle and a simplecosmological model</td>\n      <td>[{'version': 'v1', 'created': 'Sat, 31 Mar 2007 04:27:22 GMT'}, {'version': 'v2', 'created': 'Wed, 22 Aug 2007 22:42:11 GMT'}]</td>\n      <td>2008-11-26</td>\n      <td>[[Corichi, Alejandro, ], [Vukasinac, Tatjana, ], [Zapata, Jose A., ]]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"dfAI = dfAI1","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:13:35.614916Z","iopub.execute_input":"2023-06-29T23:13:35.615300Z","iopub.status.idle":"2023-06-29T23:13:35.621202Z","shell.execute_reply.started":"2023-06-29T23:13:35.615271Z","shell.execute_reply":"2023-06-29T23:13:35.619596Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"dfAI['abstract'][0:10]","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:13:35.623364Z","iopub.execute_input":"2023-06-29T23:13:35.623700Z","iopub.status.idle":"2023-06-29T23:13:35.642745Z","shell.execute_reply.started":"2023-06-29T23:13:35.623673Z","shell.execute_reply":"2023-06-29T23:13:35.641081Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"0                                                                                                                                                                                                                                                                                                                                                                                                                                                          A fully differential calculation in perturbative quantum chromodynamics ispresented for the production of massive photon pairs at hadron colliders Allnexttoleading order perturbative contributions from quarkantiquarkgluonantiquark and gluongluon subprocesses are included as well asallorders resummation of initialstate gluon radiation valid atnexttonexttoleading logarithmic accuracy The region of phase space isspecified in which the calculation is most reliable Good agreement isdemonstrated with data from the Fermilab Tevatron and predictions are made formore detailed tests with CDF and DO data Predictions are shown fordistributions of diphoton pairs produced at the energy of the Large HadronCollider LHC Distributions of the diphoton pairs from the decay of a Higgsboson are contrasted with those produced from QCD processes at the LHC showingthat enhanced sensitivity to the signal can be obtained with judiciousselection of events\n1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      We describe a new algorithm the kellpebble game with colors and useit obtain a characterization of the family of kellsparse graphs andalgorithmic solutions to a family of problems concerning tree decompositions ofgraphs Special instances of sparse graphs appear in rigidity theory and havereceived increased attention in recent years In particular our coloredpebbles generalize and strengthen the previous results of Lee and Streinu andgive a new proof of the TutteNashWilliams characterization of arboricity Wealso present a new decomposition that certifies sparsity based on thekellpebble game with colors Our work also exposes connections betweenpebble game algorithms and previous sparse graph algorithms by Gabow Gabow andWestermann and Hendrickson\n3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         We show that a determinant of Stirling cycle numbers counts unlabeled acyclicsinglesource automata The proof involves a bijection from these automata tocertain marked lattice paths and a signreversing involution to evaluate thedeterminant\n5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               We study the twoparticle wave function of paired atoms in a Fermi gas withtunable interaction strengths controlled by Feshbach resonance The Cooper pairwave function is examined for its bosonic characters which is quantified bythe correction of Bose enhancement factor associated with the creation andannihilation composite particle operators An example is given for athreedimensional uniform gas Two definitions of Cooper pair wave function areexamined One of which is chosen to reflect the offdiagonal long range orderODLRO Another one corresponds to a pair projection of a BCS state On theside with negative scattering length we found that paired atoms described byODLRO are more bosonic than the pair projected definition It is also foundthat at kF a ge  both definitions give similar results where morethan  of the atoms occupy the corresponding molecular condensates\n6                                                                                                                                                                                                                                                                                                                                                                                           A rather nonstandard quantum representation of the canonical commutationrelations of quantum mechanics systems known as the polymer representation hasgained some attention in recent years due to its possible relation with Planckscale physics In particular this approach has been followed in a symmetricsector of loop quantum gravity known as loop quantum cosmology Here we exploredifferent aspects of the relation between the ordinary Schroedinger theory andthe polymer description The paper has two parts In the first one we derivethe polymer quantum mechanics starting from the ordinary Schroedinger theoryand show that the polymer description arises as an appropriate limit In thesecond part we consider the continuum limit of this theory namely the reverseprocess in which one starts from the discrete theory and tries to recover backthe ordinary Schroedinger quantum mechanics We consider several examples ofinterest including the harmonic oscillator the free particle and a simplecosmological model\n9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Partial cubes are isometric subgraphs of hypercubes Structures on a graphdefined by means of semicubes and Djokovics and Winklers relations playan important role in the theory of partial cubes These structures are employedin the paper to characterize bipartite graphs and partial cubes of arbitrarydimension New characterizations are established and new proofs of some knownresults are given  The operations of Cartesian product and pasting and expansion andcontraction processes are utilized in the paper to construct new partial cubesfrom old ones In particular the isometric and lattice dimensions of finitepartial cubes obtained by means of these operations are calculated\n11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Recently Bruinier and Ono classified cusp forms fz  sumninftyafnq n in SlambdaGammaNchicap mathbbZq that doesnot satisfy a certain distribution property for modulo odd primes p In thispaper using RankinCohen Bracket we extend this result to modular forms ofhalf integral weight for primes p geq  As applications of our main theoremwe derive distribution properties for modulo primes pgeq of traces ofsingular moduli and Hurwitz class number We also study an analogue of Newmansconjecture for overpartitions\n12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Serre obtained the padic limit of the integral Fourier coefficient ofmodular forms on SLmathbbZ for p In this paper we extendthe result of Serre to weakly holomorphic modular forms of half integral weighton GammaN for N A proof is based on linear relations amongFourier coefficients of modular forms of half integral weight As applicationswe obtain congruences of Borcherds exponents congruences of quotient ofEisentein series and congruences of values of Lfunctions at a certain pointare also studied Furthermore the congruences of the Fourier coefficients ofSiegel modular forms on Maass Space are obtained using Ikeda lifting\n15                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             In this work we evaluate the lifetimes of the doubly charmed baryonsXicc Xicc and Omegacc We carefully calculatethe nonspectator contributions at the quark level where the Cabibbosuppresseddiagrams are also included The hadronic matrix elements are evaluated in thesimple nonrelativistic harmonic oscillator model Our numerical results aregenerally consistent with that obtained by other authors who used the diquarkmodel However all the theoretical predictions on the lifetimes are one orderlarger than the upper limit set by the recent SELEX measurement Thisdiscrepancy would be clarified by the future experiment if more accurateexperiment still confirms the value of the SELEX collaboration there must besome unknown mechanism to be explored\n16    Results from spectroscopic observations of the Intermediate Polar IP EX Hyain quiescence during  and  are presented Spinmodulated radialvelocities consistent with an outer disc origin were detected for the firsttime in an IP The spin pulsation was modulated with velocities near kms These velocities are consistent with those of material circulating at theouter edge of the accretion disc suggesting corotation of the accretioncurtain with material near the Roche lobe radius Furthermore spin Dopplertomograms have revealed evidence of the accretion curtain emission extendingfrom velocities of  kms to  kms These findings have confirmed thetheoretical model predictions of King  Wynn  Belle et al  andNorton et al  for EX Hya which predict large accretion curtains thatextend to a distance close to the Roche lobe radius in this system Evidencefor overflow stream of material falling onto the magnetosphere was observedconfirming the result of Belle et al  that disc overflow in EX Hya ispresent during quiescence as well as outburst It appears that the hbeta andhgamma spin radial velocities originated from the rotation of the funnel at theouter disc edge while those of halpha were produced due to the flow ofmaterial along the field lines far from the white dwarf narrow component andclose to the white dwarf broadbase component in agreement with theaccretion curtain model\nName: abstract, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"# categories = dfAI['categories'].unique()","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:13:35.644555Z","iopub.execute_input":"2023-06-29T23:13:35.645051Z","iopub.status.idle":"2023-06-29T23:13:35.653284Z","shell.execute_reply.started":"2023-06-29T23:13:35.645013Z","shell.execute_reply":"2023-06-29T23:13:35.651974Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# # Get the count of each category\n# category_counts = dfAI['categories'].value_counts()\n\n# # Create a DataFrame from the category counts\n# category_table = pd.DataFrame({'Category': category_counts.index, 'Count': category_counts.values})\n\n# # Display the category count table\n# print(category_table)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:13:35.654940Z","iopub.execute_input":"2023-06-29T23:13:35.655410Z","iopub.status.idle":"2023-06-29T23:13:35.667707Z","shell.execute_reply.started":"2023-06-29T23:13:35.655378Z","shell.execute_reply":"2023-06-29T23:13:35.666677Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"### Filtering Music Papers only","metadata":{}},{"cell_type":"code","source":"music_keywords = ['Music', 'Audio Processing', 'Music Information Retrieval', 'Music Generation',\n                  'Music Classification', 'Music Recommendation', 'Music Analysis', 'Audio Recognition',\n                  'Music Transcription', 'Music Synthesis', 'Music Emotion Recognition', 'Music Style Transfer',\n                  'Music Composition', 'Music Perception', 'Music Production']","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:13:35.669472Z","iopub.execute_input":"2023-06-29T23:13:35.669892Z","iopub.status.idle":"2023-06-29T23:13:35.685903Z","shell.execute_reply.started":"2023-06-29T23:13:35.669861Z","shell.execute_reply":"2023-06-29T23:13:35.684899Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"dfMusicAI = dfAI[ dfAI['title'].apply(lambda abstract: any(keyword in abstract for keyword in music_keywords))|\n    dfAI['abstract'].apply(lambda abstract: any(keyword in abstract for keyword in music_keywords))]","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:13:35.687590Z","iopub.execute_input":"2023-06-29T23:13:35.687958Z","iopub.status.idle":"2023-06-29T23:14:04.341974Z","shell.execute_reply.started":"2023-06-29T23:13:35.687921Z","shell.execute_reply":"2023-06-29T23:14:04.340563Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"dfMusicAI","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:14:04.344550Z","iopub.execute_input":"2023-06-29T23:14:04.345185Z","iopub.status.idle":"2023-06-29T23:14:04.403815Z","shell.execute_reply.started":"2023-06-29T23:14:04.345135Z","shell.execute_reply":"2023-06-29T23:14:04.402335Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"                       id            submitter  \\\n8087            0705.4085           David Wood   \n14065           0707.0895    Damian H. Zanette   \n34765           0711.1873         Thomas Fiore   \n79827           0808.3695         Erica Bisesi   \n83506           0809.3214  Soubhik Chakraborty   \n...                   ...                  ...   \n2218712   physics/0511067        Frank Wilczek   \n2219165   physics/0512266         Javier Buldu   \n2226329     q-bio/0310025          Lev Koyrakh   \n2227910     q-bio/0610037        Simone Bianco   \n2236785  quant-ph/0403087    Gavriel Segre Dr.   \n\n                                                                                                                                                  authors  \\\n8087     Erik D. Demaine, Francisco Gomez-Martin, Henk Meijer, David Rappaport,\\n  Perouz Taslakian, Godfried T. Toussaint, Terry Winograd, David R. Wood   \n14065                                                                                                                                   Damian H. Zanette   \n34765                                                                                               Alissa S. Crans, Thomas M. Fiore, and Ramon Satyendra   \n79827                                                                                                                  Erica Bisesi, and Marisa Michelini   \n83506                                 Soubhik Chakraborty, Sandeep Singh Solanki, Sayan Roy, Shivee Chauhan,\\n  Sanjaya Shankar Tripathy and Kartik Mahto   \n...                                                                                                                                                   ...   \n2218712                                                                                                                                     Frank Wilczek   \n2219165                                                                                   Pedro Cano, Oscar Celma, Markus Koppenberger, Javier M. Buld\\'u   \n2226329                                                                                                                                       Lev Koyrakh   \n2227910                                           Simone Bianco, Massimiliano Ignaccolo, Mark S. Rider, Mary J. Ross,\\n  Phil Winsor, and Paolo Grigolini   \n2236785                                                                                                                                     Gavriel Segre   \n\n                                                                                               title  \\\n8087                                                                  The Distance Geometry of Music   \n14065                                     Segmentation and Context of Literary and Musical Sequences   \n34765                                                             Musical Actions of Dihedral Groups   \n79827    Planning Curricular Proposals on Sound and Music with Prospective  SecondarySchool Teachers   \n83506                          A Statistical Approach to Modeling Indian Classical Music Performance   \n...                                                                                              ...   \n2218712                                                              The Universe is a Strange Place   \n2219165                                                The Topology of Music Recommendation Networks   \n2226329                                    Pattern ExcitationBased Processing The Music of The Brain   \n2227910                                                 Brain Music and nonPoisson Renewal Processes   \n2236785                                      A remark about the MerminSquires Music Halls inteludium   \n\n                                                                                                                                                                                                                                                    comments  \\\n8087                                                     This is the full version of the paper: \"The distance geometry of deep\\n  rhythms and scales.\" 17th Canadian Conference on Computational Geometry (CCCG\\n  '05), University of Windsor, Canada, 2005   \n14065                                                                                                                                                                                                                           To appear in Complex Systems   \n34765                                                                                                                                                                                  27 pages, 11 figures. To appear in the American Mathematical Monthly.   \n79827                                                                                           12 pages with 5 figures. Submitted for publication in Physics\\n  Curriculum Design, Development and Validation - GIREP 2008 book of selected\\n  papers, 2008   \n83506                                                                                            24 pages,11 figures;a replacement paper is being uploaded because the\\n  the nyas swars of Yaman were not correctly given;some typos have been\\n  corrected   \n...                                                                                                                                                                                                                                                      ...   \n2218712  Public lecture delivered at Lepton-Photon 2005, Uppsala, Sweden, and\\n  in related forms on several other occasions. To be published in the\\n  Proceedings. 14 pages, 6 figures. v2: Two paragraphs that were inadvertently\\n  deleted are restored   \n2219165                                                                                                                                                                                                                                  15 pages, 3 figures   \n2226329                                                                                                                                                                                                           15 pages, 7 figures, a reference corrected   \n2227910                                                                                                                                                                                                14 pages, 4 figures. Updated content. Updated figures   \n2236785                                                                                                                                                                                                                                                 None   \n\n                               journal-ref                         doi  \\\n8087                                  None                        None   \n14065                                 None                        None   \n34765                                 None                        None   \n79827                                 None                        None   \n83506                                 None                        None   \n...                                    ...                         ...   \n2218712  Int.J.Mod.Phys.A21:2011-2025,2006   10.1142/S0217751X06032940   \n2219165           Chaos, 16, 013107 (2006)           10.1063/1.2137622   \n2226329                               None                        None   \n2227910                               None  10.1103/PhysRevE.75.061911   \n2236785                               None                        None   \n\n            report-no                     categories  \\\n8087             None                          cs.CG   \n14065            None          cs.CL physics.data-an   \n34765            None                math.GR math.AT   \n79827            None   physics.ed-ph physics.soc-ph   \n83506            None                  cs.SD stat.AP   \n...               ...                            ...   \n2218712  MIT-CTP-3701   physics.pop-ph hep-ph hep-th   \n2219165          None                 physics.soc-ph   \n2226329          None  q-bio.NC cs.NE physics.bio-ph   \n2227910          None                       q-bio.NC   \n2236785          None                       quant-ph   \n\n                                                     license  \\\n8087                                                    None   \n14065                                                   None   \n34765    http://arxiv.org/licenses/nonexclusive-distrib/1.0/   \n79827    http://arxiv.org/licenses/nonexclusive-distrib/1.0/   \n83506    http://arxiv.org/licenses/nonexclusive-distrib/1.0/   \n...                                                      ...   \n2218712                                                 None   \n2219165                                                 None   \n2226329                                                 None   \n2227910                                                 None   \n2236785                                                 None   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               abstract  \\\n8087                                                                                                                                                                                                                                                                                                   We demonstrate relationships between the classic Euclidean algorithm and manyother fields of study particularly in the context of music and distancegeometry Specifically we show how the structure of the Euclidean algorithmdefines a family of rhythms which encompass over forty timelinesemphostinatos from traditional world music We prove that theseemphEuclidean rhythms have the mathematical property that their onsetpatterns are distributed as evenly as possible they maximize the sum of theEuclidean distances between all pairs of onsets viewing onsets as points on acircle Indeed Euclidean rhythms are the unique rhythms that maximize thisnotion of emphevenness We also show that essentially all Euclidean rhythmsare emphdeep each distinct distance between onsets occurs with a uniquemultiplicity and these multiplicies form an interval k Finallywe characterize all deep rhythms showing that they form a subclass ofgenerated rhythms which in turn proves a useful property called shelling Allof our results for musical rhythms apply equally well to musical scales Inaddition many of the problems we explore are interesting in their own right asdistance geometry problems on the circle some of the same problems wereexplored by ErdHos in the plane   \n14065                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           We test a segmentation algorithm based on the calculation of theJensenShannon divergence between probability distributions to two symbolicsequences of literary and musical origin The first sequence represents thesuccessive appearance of characters in a theatrical play and the secondrepresents the succession of tones from the twelvetone scale in a keyboardsonata The algorithm divides the sequences into segments of maximalcompositional divergence between them For the play these segments are relatedto changes in the frequency of appearance of different characters and in thegeographical setting of the action For the sonata the segments correspond totonal domains and reveal in detail the characteristic tonal progression of suchkind of musical composition   \n34765                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  The sequence of pitches which form a musical melody can be transposed orinverted Since the s music theorists have modeled musical transpositionand inversion in terms of an action of the dihedral group of order  Morerecently music theorists have found an intriguing second way that the dihedralgroup of order  acts on the set of major and minor chords We illustrate bothgeometrically and algebraically how these two actions are it dual Bothactions and their duality have been used to analyze works of music as diverseas Hindemith and the Beatles   \n79827                                                                                                                                                                                                                                                                                                                                                   Sound is a preferred context to build foundations on wave phenomena one ofthe most important disciplinary referents in physics It is also one of thebestset frameworks to achieve transversality overcoming scholastic level andactivating emotional aspects which are naturally connected with every day lifeas well as with music and perception Looking at sound and music by atransversal perspective  a borderline approach between science and art isthe adopted statement for a teaching proposal using metacognition as astrategy in scientific education This work analyzes curricular proposals onmusical acoustics planned by prospective secondaryschool teachers in theframework of a Formative Intervention Module answering the expectation ofmaking more effective teaching scientific subjects by improving creativecapabilities as well as leading to build logical and scientificcategorizations able to consciously discipline artistic activity in musicstudents With this aim a particular emphasis is given to those concepts like sound parameters and structural elements of a musical piece which arebest fitted to be addressed on a transversal perspective involvingsimultaneously physics psychophysics and music   \n83506                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        A raga is a melodic structure with fixed notes and a set of rulescharacterizing a certain mood endorsed through performance By a vadi swar ismeant that note which plays the most significant role in expressing the raga Asamvadi swar similarly is the second most significant note However thedetermination of their significance has an element of subjectivity and hence weare motivated to find some truths through an objective analysis The paperproposes a probabilistic method of note detection and demonstrates how therelative frequency relative number of occurrences of the pitch of the moreimportant notes stabilize far more quickly than that of others In addition acount for distinct transitory and similar looking nontransitory fundamentalfrequency movements but possibly embedding distinct emotions between thenotes is also taken depicting the varnalankars or musical ornaments decoratingthe notes and note sequences as rendered by the artist They reflect certainstructural properties of the ragas Several case studies are presented   \n...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ...   \n2218712                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Our understanding of ordinary matter is remarkably accurate and complete butit is based on principles that are very strange and unfamiliar As Illexplain weve come to understand matter to be a Music of the Void in aremarkably literal sense Just as we physicists finalized that wonderfulunderstanding towards the end of the twentieth century astronomers gave usback our humility by informing us that ordinary matter  what we andchemists and biologists and astronomers themselves have been studying allthese centuries constitutes only about  of the mass of the universe as awhole Ill describe some of our promising attempts to rise to this challengeby improving rather than merely complicating our description of the world   \n2219165                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        We study the topology of several music recommendation networks which risefrom relationships between artist cooccurrence of songs in playlists orexperts recommendation The analysis uncovers the emergence of complex networkphenomena in this kind of recommendation networks built considering artists asnodes and their resemblance as links We observe structural properties thatprovide some hints on navigation and possible optimizations on the design ofmusic recommendation systems Finally the analysis derived from existing musicknowledge sources provides a deeper understanding of the human music similarityperceptions   \n2226329                                                                                                                                                                                                                                                                                                                                                                                                            An approach to information processing based on the excitation of patterns ofactivity by nonlinear active resonators in response to their input patterns isproposed Arguments are presented to show that any computation performed by aconventional Turing machinebased computer called Tmachine in this papercould also be performed by the pattern excitationbased machine which will becalled Pmachine A realization of this processing scheme by neural networks isdiscussed In this realization the role of the resonators is played by neuralpattern excitation networks which are the neural circuits capable of excitingdifferent spatiotemporal patterns of activity in response to different inputsLearning in the neural pattern excitation networks is also considered It isshown that there is a duality between pattern excitation and patternrecognition neural networks which allows to create new pattern excitationmodes corresponding to recognizable input patterns based on Hebbian learningrules Hierarchically organized such networks can produce complex behaviorAnimal behavior human language and thought are treated as examples produced bysuch networks   \n2227910  In this paper we show that both music composition and brain function asrevealed by the Electroencephalogram EEG analysis are renewal nonPoissonprocesses living in the nonergodic dominion To reach this importantconclusion we process the data with the minimum spanning tree method so as todetect significant events thereby building a sequence of times which is thetime series to analyze Then we show that in both cases EEG and musiccomposition these significant events are the signature of a nonPoissonrenewal process This conclusion is reached using a techniques of statisticalanalysis recently developed by our group the Aging Experiment AE First wefind that in both cases the distances between two consecutive events aredescribed by nonexponential histograms thereby proving the nonPoisson natureof these processes The corresponding survival probabilities Psit are wellfitted by stretched exponentials Psit propto expgamma talphawith  alpha  The second step rests on the adoption of the AE whichshows that these are renewal processes We show that the renewal stretchedexponential is the emerging tip of an iceberg whose underwater part has slowtails with an inverse power law structure with power index mu    alphaWe find that both EEG and music composition yield mu   On the basis ofthe recently discovered complexity matching effect according to which acomplex system S with muS   responds only to a complex driving signalP with muP  muS we conclude that the results of our analysis mayexplain the influence of music on human brain   \n2236785                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    The MerminSquires Music Hall inteludium on the EinsteinPodolskyRosenaffair is analyzed by showing the fallacity of the OneBorelNormalityCriterion and the necessity of replacing it with the more restrictiveAlgorithmicRandomness Criterion   \n\n                                                                                                                                                                                              versions  \\\n8087                                                                                                                                   [{'version': 'v1', 'created': 'Mon, 28 May 2007 18:36:19 GMT'}]   \n14065                                                                                                                                   [{'version': 'v1', 'created': 'Fri, 6 Jul 2007 01:45:05 GMT'}]   \n34765                                                                   [{'version': 'v1', 'created': 'Mon, 12 Nov 2007 21:52:13 GMT'}, {'version': 'v2', 'created': 'Fri, 13 Jun 2008 17:43:29 GMT'}]   \n79827                                                                                                                                  [{'version': 'v1', 'created': 'Wed, 27 Aug 2008 12:56:16 GMT'}]   \n83506                                                                   [{'version': 'v1', 'created': 'Thu, 18 Sep 2008 17:38:57 GMT'}, {'version': 'v2', 'created': 'Sat, 25 Oct 2008 14:10:00 GMT'}]   \n...                                                                                                                                                                                                ...   \n2218712                                                                  [{'version': 'v1', 'created': 'Tue, 8 Nov 2005 17:14:12 GMT'}, {'version': 'v2', 'created': 'Fri, 11 Nov 2005 19:33:02 GMT'}]   \n2219165                                                                                                                                [{'version': 'v1', 'created': 'Thu, 29 Dec 2005 18:57:12 GMT'}]   \n2226329  [{'version': 'v1', 'created': 'Mon, 20 Oct 2003 19:36:04 GMT'}, {'version': 'v2', 'created': 'Tue, 21 Oct 2003 19:50:28 GMT'}, {'version': 'v3', 'created': 'Tue, 21 Oct 2003 22:07:29 GMT'}]   \n2227910  [{'version': 'v1', 'created': 'Thu, 19 Oct 2006 17:22:41 GMT'}, {'version': 'v2', 'created': 'Thu, 18 Jan 2007 17:11:33 GMT'}, {'version': 'v3', 'created': 'Fri, 16 Mar 2007 17:12:40 GMT'}]   \n2236785                                                                 [{'version': 'v1', 'created': 'Thu, 11 Mar 2004 18:47:02 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Jul 2004 16:48:43 GMT'}]   \n\n        update_date  \\\n8087     2007-05-29   \n14065    2007-07-09   \n34765    2008-06-13   \n79827    2008-08-28   \n83506    2008-10-25   \n...             ...   \n2218712  2010-11-05   \n2219165  2012-05-14   \n2226329  2007-05-23   \n2227910  2009-11-13   \n2236785  2007-05-23   \n\n                                                                                                                                                                                  authors_parsed  \n8087     [[Demaine, Erik D., ], [Gomez-Martin, Francisco, ], [Meijer, Henk, ], [Rappaport, David, ], [Taslakian, Perouz, ], [Toussaint, Godfried T., ], [Winograd, Terry, ], [Wood, David R., ]]  \n14065                                                                                                                                                                   [[Zanette, Damian H., ]]  \n34765                                                                                                                         [[Crans, Alissa S., ], [Fiore, Thomas M., ], [Satyendra, Ramon, ]]  \n79827                                                                                                                                                 [[Bisesi, Erica, ], [Michelini, Marisa, ]]  \n83506                                              [[Chakraborty, Soubhik, ], [Solanki, Sandeep Singh, ], [Roy, Sayan, ], [Chauhan, Shivee, ], [Tripathy, Sanjaya Shankar, ], [Mahto, Kartik, ]]  \n...                                                                                                                                                                                          ...  \n2218712                                                                                                                                                                     [[Wilczek, Frank, ]]  \n2219165                                                                                                      [[Cano, Pedro, ], [Celma, Oscar, ], [Koppenberger, Markus, ], [Buldú, Javier M., ]]  \n2226329                                                                                                                                                                       [[Koyrakh, Lev, ]]  \n2227910                                                         [[Bianco, Simone, ], [Ignaccolo, Massimiliano, ], [Rider, Mark S., ], [Ross, Mary J., ], [Winsor, Phil, ], [Grigolini, Paolo, ]]  \n2236785                                                                                                                                                                     [[Segre, Gavriel, ]]  \n\n[1155 rows x 14 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>submitter</th>\n      <th>authors</th>\n      <th>title</th>\n      <th>comments</th>\n      <th>journal-ref</th>\n      <th>doi</th>\n      <th>report-no</th>\n      <th>categories</th>\n      <th>license</th>\n      <th>abstract</th>\n      <th>versions</th>\n      <th>update_date</th>\n      <th>authors_parsed</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8087</th>\n      <td>0705.4085</td>\n      <td>David Wood</td>\n      <td>Erik D. Demaine, Francisco Gomez-Martin, Henk Meijer, David Rappaport,\\n  Perouz Taslakian, Godfried T. Toussaint, Terry Winograd, David R. Wood</td>\n      <td>The Distance Geometry of Music</td>\n      <td>This is the full version of the paper: \"The distance geometry of deep\\n  rhythms and scales.\" 17th Canadian Conference on Computational Geometry (CCCG\\n  '05), University of Windsor, Canada, 2005</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>cs.CG</td>\n      <td>None</td>\n      <td>We demonstrate relationships between the classic Euclidean algorithm and manyother fields of study particularly in the context of music and distancegeometry Specifically we show how the structure of the Euclidean algorithmdefines a family of rhythms which encompass over forty timelinesemphostinatos from traditional world music We prove that theseemphEuclidean rhythms have the mathematical property that their onsetpatterns are distributed as evenly as possible they maximize the sum of theEuclidean distances between all pairs of onsets viewing onsets as points on acircle Indeed Euclidean rhythms are the unique rhythms that maximize thisnotion of emphevenness We also show that essentially all Euclidean rhythmsare emphdeep each distinct distance between onsets occurs with a uniquemultiplicity and these multiplicies form an interval k Finallywe characterize all deep rhythms showing that they form a subclass ofgenerated rhythms which in turn proves a useful property called shelling Allof our results for musical rhythms apply equally well to musical scales Inaddition many of the problems we explore are interesting in their own right asdistance geometry problems on the circle some of the same problems wereexplored by ErdHos in the plane</td>\n      <td>[{'version': 'v1', 'created': 'Mon, 28 May 2007 18:36:19 GMT'}]</td>\n      <td>2007-05-29</td>\n      <td>[[Demaine, Erik D., ], [Gomez-Martin, Francisco, ], [Meijer, Henk, ], [Rappaport, David, ], [Taslakian, Perouz, ], [Toussaint, Godfried T., ], [Winograd, Terry, ], [Wood, David R., ]]</td>\n    </tr>\n    <tr>\n      <th>14065</th>\n      <td>0707.0895</td>\n      <td>Damian H. Zanette</td>\n      <td>Damian H. Zanette</td>\n      <td>Segmentation and Context of Literary and Musical Sequences</td>\n      <td>To appear in Complex Systems</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>cs.CL physics.data-an</td>\n      <td>None</td>\n      <td>We test a segmentation algorithm based on the calculation of theJensenShannon divergence between probability distributions to two symbolicsequences of literary and musical origin The first sequence represents thesuccessive appearance of characters in a theatrical play and the secondrepresents the succession of tones from the twelvetone scale in a keyboardsonata The algorithm divides the sequences into segments of maximalcompositional divergence between them For the play these segments are relatedto changes in the frequency of appearance of different characters and in thegeographical setting of the action For the sonata the segments correspond totonal domains and reveal in detail the characteristic tonal progression of suchkind of musical composition</td>\n      <td>[{'version': 'v1', 'created': 'Fri, 6 Jul 2007 01:45:05 GMT'}]</td>\n      <td>2007-07-09</td>\n      <td>[[Zanette, Damian H., ]]</td>\n    </tr>\n    <tr>\n      <th>34765</th>\n      <td>0711.1873</td>\n      <td>Thomas Fiore</td>\n      <td>Alissa S. Crans, Thomas M. Fiore, and Ramon Satyendra</td>\n      <td>Musical Actions of Dihedral Groups</td>\n      <td>27 pages, 11 figures. To appear in the American Mathematical Monthly.</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>math.GR math.AT</td>\n      <td>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</td>\n      <td>The sequence of pitches which form a musical melody can be transposed orinverted Since the s music theorists have modeled musical transpositionand inversion in terms of an action of the dihedral group of order  Morerecently music theorists have found an intriguing second way that the dihedralgroup of order  acts on the set of major and minor chords We illustrate bothgeometrically and algebraically how these two actions are it dual Bothactions and their duality have been used to analyze works of music as diverseas Hindemith and the Beatles</td>\n      <td>[{'version': 'v1', 'created': 'Mon, 12 Nov 2007 21:52:13 GMT'}, {'version': 'v2', 'created': 'Fri, 13 Jun 2008 17:43:29 GMT'}]</td>\n      <td>2008-06-13</td>\n      <td>[[Crans, Alissa S., ], [Fiore, Thomas M., ], [Satyendra, Ramon, ]]</td>\n    </tr>\n    <tr>\n      <th>79827</th>\n      <td>0808.3695</td>\n      <td>Erica Bisesi</td>\n      <td>Erica Bisesi, and Marisa Michelini</td>\n      <td>Planning Curricular Proposals on Sound and Music with Prospective  SecondarySchool Teachers</td>\n      <td>12 pages with 5 figures. Submitted for publication in Physics\\n  Curriculum Design, Development and Validation - GIREP 2008 book of selected\\n  papers, 2008</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>physics.ed-ph physics.soc-ph</td>\n      <td>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</td>\n      <td>Sound is a preferred context to build foundations on wave phenomena one ofthe most important disciplinary referents in physics It is also one of thebestset frameworks to achieve transversality overcoming scholastic level andactivating emotional aspects which are naturally connected with every day lifeas well as with music and perception Looking at sound and music by atransversal perspective  a borderline approach between science and art isthe adopted statement for a teaching proposal using metacognition as astrategy in scientific education This work analyzes curricular proposals onmusical acoustics planned by prospective secondaryschool teachers in theframework of a Formative Intervention Module answering the expectation ofmaking more effective teaching scientific subjects by improving creativecapabilities as well as leading to build logical and scientificcategorizations able to consciously discipline artistic activity in musicstudents With this aim a particular emphasis is given to those concepts like sound parameters and structural elements of a musical piece which arebest fitted to be addressed on a transversal perspective involvingsimultaneously physics psychophysics and music</td>\n      <td>[{'version': 'v1', 'created': 'Wed, 27 Aug 2008 12:56:16 GMT'}]</td>\n      <td>2008-08-28</td>\n      <td>[[Bisesi, Erica, ], [Michelini, Marisa, ]]</td>\n    </tr>\n    <tr>\n      <th>83506</th>\n      <td>0809.3214</td>\n      <td>Soubhik Chakraborty</td>\n      <td>Soubhik Chakraborty, Sandeep Singh Solanki, Sayan Roy, Shivee Chauhan,\\n  Sanjaya Shankar Tripathy and Kartik Mahto</td>\n      <td>A Statistical Approach to Modeling Indian Classical Music Performance</td>\n      <td>24 pages,11 figures;a replacement paper is being uploaded because the\\n  the nyas swars of Yaman were not correctly given;some typos have been\\n  corrected</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>cs.SD stat.AP</td>\n      <td>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</td>\n      <td>A raga is a melodic structure with fixed notes and a set of rulescharacterizing a certain mood endorsed through performance By a vadi swar ismeant that note which plays the most significant role in expressing the raga Asamvadi swar similarly is the second most significant note However thedetermination of their significance has an element of subjectivity and hence weare motivated to find some truths through an objective analysis The paperproposes a probabilistic method of note detection and demonstrates how therelative frequency relative number of occurrences of the pitch of the moreimportant notes stabilize far more quickly than that of others In addition acount for distinct transitory and similar looking nontransitory fundamentalfrequency movements but possibly embedding distinct emotions between thenotes is also taken depicting the varnalankars or musical ornaments decoratingthe notes and note sequences as rendered by the artist They reflect certainstructural properties of the ragas Several case studies are presented</td>\n      <td>[{'version': 'v1', 'created': 'Thu, 18 Sep 2008 17:38:57 GMT'}, {'version': 'v2', 'created': 'Sat, 25 Oct 2008 14:10:00 GMT'}]</td>\n      <td>2008-10-25</td>\n      <td>[[Chakraborty, Soubhik, ], [Solanki, Sandeep Singh, ], [Roy, Sayan, ], [Chauhan, Shivee, ], [Tripathy, Sanjaya Shankar, ], [Mahto, Kartik, ]]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2218712</th>\n      <td>physics/0511067</td>\n      <td>Frank Wilczek</td>\n      <td>Frank Wilczek</td>\n      <td>The Universe is a Strange Place</td>\n      <td>Public lecture delivered at Lepton-Photon 2005, Uppsala, Sweden, and\\n  in related forms on several other occasions. To be published in the\\n  Proceedings. 14 pages, 6 figures. v2: Two paragraphs that were inadvertently\\n  deleted are restored</td>\n      <td>Int.J.Mod.Phys.A21:2011-2025,2006</td>\n      <td>10.1142/S0217751X06032940</td>\n      <td>MIT-CTP-3701</td>\n      <td>physics.pop-ph hep-ph hep-th</td>\n      <td>None</td>\n      <td>Our understanding of ordinary matter is remarkably accurate and complete butit is based on principles that are very strange and unfamiliar As Illexplain weve come to understand matter to be a Music of the Void in aremarkably literal sense Just as we physicists finalized that wonderfulunderstanding towards the end of the twentieth century astronomers gave usback our humility by informing us that ordinary matter  what we andchemists and biologists and astronomers themselves have been studying allthese centuries constitutes only about  of the mass of the universe as awhole Ill describe some of our promising attempts to rise to this challengeby improving rather than merely complicating our description of the world</td>\n      <td>[{'version': 'v1', 'created': 'Tue, 8 Nov 2005 17:14:12 GMT'}, {'version': 'v2', 'created': 'Fri, 11 Nov 2005 19:33:02 GMT'}]</td>\n      <td>2010-11-05</td>\n      <td>[[Wilczek, Frank, ]]</td>\n    </tr>\n    <tr>\n      <th>2219165</th>\n      <td>physics/0512266</td>\n      <td>Javier Buldu</td>\n      <td>Pedro Cano, Oscar Celma, Markus Koppenberger, Javier M. Buld\\'u</td>\n      <td>The Topology of Music Recommendation Networks</td>\n      <td>15 pages, 3 figures</td>\n      <td>Chaos, 16, 013107 (2006)</td>\n      <td>10.1063/1.2137622</td>\n      <td>None</td>\n      <td>physics.soc-ph</td>\n      <td>None</td>\n      <td>We study the topology of several music recommendation networks which risefrom relationships between artist cooccurrence of songs in playlists orexperts recommendation The analysis uncovers the emergence of complex networkphenomena in this kind of recommendation networks built considering artists asnodes and their resemblance as links We observe structural properties thatprovide some hints on navigation and possible optimizations on the design ofmusic recommendation systems Finally the analysis derived from existing musicknowledge sources provides a deeper understanding of the human music similarityperceptions</td>\n      <td>[{'version': 'v1', 'created': 'Thu, 29 Dec 2005 18:57:12 GMT'}]</td>\n      <td>2012-05-14</td>\n      <td>[[Cano, Pedro, ], [Celma, Oscar, ], [Koppenberger, Markus, ], [Buldú, Javier M., ]]</td>\n    </tr>\n    <tr>\n      <th>2226329</th>\n      <td>q-bio/0310025</td>\n      <td>Lev Koyrakh</td>\n      <td>Lev Koyrakh</td>\n      <td>Pattern ExcitationBased Processing The Music of The Brain</td>\n      <td>15 pages, 7 figures, a reference corrected</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>q-bio.NC cs.NE physics.bio-ph</td>\n      <td>None</td>\n      <td>An approach to information processing based on the excitation of patterns ofactivity by nonlinear active resonators in response to their input patterns isproposed Arguments are presented to show that any computation performed by aconventional Turing machinebased computer called Tmachine in this papercould also be performed by the pattern excitationbased machine which will becalled Pmachine A realization of this processing scheme by neural networks isdiscussed In this realization the role of the resonators is played by neuralpattern excitation networks which are the neural circuits capable of excitingdifferent spatiotemporal patterns of activity in response to different inputsLearning in the neural pattern excitation networks is also considered It isshown that there is a duality between pattern excitation and patternrecognition neural networks which allows to create new pattern excitationmodes corresponding to recognizable input patterns based on Hebbian learningrules Hierarchically organized such networks can produce complex behaviorAnimal behavior human language and thought are treated as examples produced bysuch networks</td>\n      <td>[{'version': 'v1', 'created': 'Mon, 20 Oct 2003 19:36:04 GMT'}, {'version': 'v2', 'created': 'Tue, 21 Oct 2003 19:50:28 GMT'}, {'version': 'v3', 'created': 'Tue, 21 Oct 2003 22:07:29 GMT'}]</td>\n      <td>2007-05-23</td>\n      <td>[[Koyrakh, Lev, ]]</td>\n    </tr>\n    <tr>\n      <th>2227910</th>\n      <td>q-bio/0610037</td>\n      <td>Simone Bianco</td>\n      <td>Simone Bianco, Massimiliano Ignaccolo, Mark S. Rider, Mary J. Ross,\\n  Phil Winsor, and Paolo Grigolini</td>\n      <td>Brain Music and nonPoisson Renewal Processes</td>\n      <td>14 pages, 4 figures. Updated content. Updated figures</td>\n      <td>None</td>\n      <td>10.1103/PhysRevE.75.061911</td>\n      <td>None</td>\n      <td>q-bio.NC</td>\n      <td>None</td>\n      <td>In this paper we show that both music composition and brain function asrevealed by the Electroencephalogram EEG analysis are renewal nonPoissonprocesses living in the nonergodic dominion To reach this importantconclusion we process the data with the minimum spanning tree method so as todetect significant events thereby building a sequence of times which is thetime series to analyze Then we show that in both cases EEG and musiccomposition these significant events are the signature of a nonPoissonrenewal process This conclusion is reached using a techniques of statisticalanalysis recently developed by our group the Aging Experiment AE First wefind that in both cases the distances between two consecutive events aredescribed by nonexponential histograms thereby proving the nonPoisson natureof these processes The corresponding survival probabilities Psit are wellfitted by stretched exponentials Psit propto expgamma talphawith  alpha  The second step rests on the adoption of the AE whichshows that these are renewal processes We show that the renewal stretchedexponential is the emerging tip of an iceberg whose underwater part has slowtails with an inverse power law structure with power index mu    alphaWe find that both EEG and music composition yield mu   On the basis ofthe recently discovered complexity matching effect according to which acomplex system S with muS   responds only to a complex driving signalP with muP  muS we conclude that the results of our analysis mayexplain the influence of music on human brain</td>\n      <td>[{'version': 'v1', 'created': 'Thu, 19 Oct 2006 17:22:41 GMT'}, {'version': 'v2', 'created': 'Thu, 18 Jan 2007 17:11:33 GMT'}, {'version': 'v3', 'created': 'Fri, 16 Mar 2007 17:12:40 GMT'}]</td>\n      <td>2009-11-13</td>\n      <td>[[Bianco, Simone, ], [Ignaccolo, Massimiliano, ], [Rider, Mark S., ], [Ross, Mary J., ], [Winsor, Phil, ], [Grigolini, Paolo, ]]</td>\n    </tr>\n    <tr>\n      <th>2236785</th>\n      <td>quant-ph/0403087</td>\n      <td>Gavriel Segre Dr.</td>\n      <td>Gavriel Segre</td>\n      <td>A remark about the MerminSquires Music Halls inteludium</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>quant-ph</td>\n      <td>None</td>\n      <td>The MerminSquires Music Hall inteludium on the EinsteinPodolskyRosenaffair is analyzed by showing the fallacity of the OneBorelNormalityCriterion and the necessity of replacing it with the more restrictiveAlgorithmicRandomness Criterion</td>\n      <td>[{'version': 'v1', 'created': 'Thu, 11 Mar 2004 18:47:02 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Jul 2004 16:48:43 GMT'}]</td>\n      <td>2007-05-23</td>\n      <td>[[Segre, Gavriel, ]]</td>\n    </tr>\n  </tbody>\n</table>\n<p>1155 rows × 14 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"dfMusicAIyear = pd.DataFrame(dfMusicAI['versions'].apply(lambda x: [version['created'][-17:-13] for version in x if version['version'] == 'v1'][0] if any(version['version'] == 'v1' for version in x) else None))","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:14:04.405965Z","iopub.execute_input":"2023-06-29T23:14:04.406418Z","iopub.status.idle":"2023-06-29T23:14:04.423548Z","shell.execute_reply.started":"2023-06-29T23:14:04.406385Z","shell.execute_reply":"2023-06-29T23:14:04.422236Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"dfMusicAIyear['versions'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:14:04.425561Z","iopub.execute_input":"2023-06-29T23:14:04.425946Z","iopub.status.idle":"2023-06-29T23:14:04.443800Z","shell.execute_reply.started":"2023-06-29T23:14:04.425887Z","shell.execute_reply":"2023-06-29T23:14:04.442954Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"2022    209\n2021    189\n2020    171\n2019    145\n2018    114\n2017     98\n2023     64\n2016     51\n2015     29\n2014     21\n2013     16\n2012     13\n2011      8\n2009      7\n2010      6\n2007      3\n2008      3\n2005      2\n2001      1\n2000      1\n2002      1\n2003      1\n2006      1\n2004      1\nName: versions, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"import plotly.graph_objects as go\n\nyears = dfMusicAIyear['versions'].value_counts().index.tolist()\nno_of_papers = dfMusicAIyear['versions'].value_counts().values.tolist()\n\n# Sort years and no_of_papers together\nsorted_years, sorted_no_of_papers = zip(*sorted(zip(years, no_of_papers)))\n\nfig = go.Figure(data=[go.Bar(x=sorted_years, y=sorted_no_of_papers, marker_color='blue')])\n\nfig.update_layout(\n    title='Number of Music Papers by Year',\n    xaxis_title='Years',\n    yaxis_title='No of Music Papers',\n    xaxis_tickangle=-45,\n    plot_bgcolor='black',\n    paper_bgcolor='black',\n    font=dict(color='white'),\n)\n\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:14:04.445399Z","iopub.execute_input":"2023-06-29T23:14:04.446372Z","iopub.status.idle":"2023-06-29T23:14:04.481952Z","shell.execute_reply.started":"2023-06-29T23:14:04.446337Z","shell.execute_reply":"2023-06-29T23:14:04.480534Z"},"trusted":true},"execution_count":50,"outputs":[{"output_type":"display_data","data":{"text/html":"<div>                            <div id=\"d7401531-28b2-4f74-914b-389fc3e90fb6\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"d7401531-28b2-4f74-914b-389fc3e90fb6\")) {                    Plotly.newPlot(                        \"d7401531-28b2-4f74-914b-389fc3e90fb6\",                        [{\"marker\":{\"color\":\"blue\"},\"x\":[\"2000\",\"2001\",\"2002\",\"2003\",\"2004\",\"2005\",\"2006\",\"2007\",\"2008\",\"2009\",\"2010\",\"2011\",\"2012\",\"2013\",\"2014\",\"2015\",\"2016\",\"2017\",\"2018\",\"2019\",\"2020\",\"2021\",\"2022\",\"2023\"],\"y\":[1,1,1,1,1,2,1,3,3,7,6,8,13,16,21,29,51,98,114,145,171,189,209,64],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"title\":{\"text\":\"Years\"},\"tickangle\":-45},\"font\":{\"color\":\"white\"},\"title\":{\"text\":\"Number of Music Papers by Year\"},\"yaxis\":{\"title\":{\"text\":\"No of Music Papers\"}},\"plot_bgcolor\":\"black\",\"paper_bgcolor\":\"black\"},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('d7401531-28b2-4f74-914b-389fc3e90fb6');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>"},"metadata":{}}]},{"cell_type":"code","source":"df_Music_data = pd.DataFrame({\n    'Authors': dfMusicAI['authors'],\n    'Title': dfMusicAI['title'],\n    'Abstract': dfMusicAI['abstract'],\n    'Year': dfMusicAI['versions'].apply(lambda x: [version['created'][-17:-13] for version in x if version['version'] == 'v1'][0] if any(version['version'] == 'v1' for version in x) else None)\n})","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:14:04.484113Z","iopub.execute_input":"2023-06-29T23:14:04.485183Z","iopub.status.idle":"2023-06-29T23:14:04.501601Z","shell.execute_reply.started":"2023-06-29T23:14:04.485144Z","shell.execute_reply":"2023-06-29T23:14:04.500011Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"df_Music_data","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:14:04.503854Z","iopub.execute_input":"2023-06-29T23:14:04.504591Z","iopub.status.idle":"2023-06-29T23:14:04.524715Z","shell.execute_reply.started":"2023-06-29T23:14:04.504556Z","shell.execute_reply":"2023-06-29T23:14:04.523321Z"},"trusted":true},"execution_count":52,"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"                                                                                                                                                  Authors  \\\n8087     Erik D. Demaine, Francisco Gomez-Martin, Henk Meijer, David Rappaport,\\n  Perouz Taslakian, Godfried T. Toussaint, Terry Winograd, David R. Wood   \n14065                                                                                                                                   Damian H. Zanette   \n34765                                                                                               Alissa S. Crans, Thomas M. Fiore, and Ramon Satyendra   \n79827                                                                                                                  Erica Bisesi, and Marisa Michelini   \n83506                                 Soubhik Chakraborty, Sandeep Singh Solanki, Sayan Roy, Shivee Chauhan,\\n  Sanjaya Shankar Tripathy and Kartik Mahto   \n...                                                                                                                                                   ...   \n2218712                                                                                                                                     Frank Wilczek   \n2219165                                                                                   Pedro Cano, Oscar Celma, Markus Koppenberger, Javier M. Buld\\'u   \n2226329                                                                                                                                       Lev Koyrakh   \n2227910                                           Simone Bianco, Massimiliano Ignaccolo, Mark S. Rider, Mary J. Ross,\\n  Phil Winsor, and Paolo Grigolini   \n2236785                                                                                                                                     Gavriel Segre   \n\n                                                                                               Title  \\\n8087                                                                  The Distance Geometry of Music   \n14065                                     Segmentation and Context of Literary and Musical Sequences   \n34765                                                             Musical Actions of Dihedral Groups   \n79827    Planning Curricular Proposals on Sound and Music with Prospective  SecondarySchool Teachers   \n83506                          A Statistical Approach to Modeling Indian Classical Music Performance   \n...                                                                                              ...   \n2218712                                                              The Universe is a Strange Place   \n2219165                                                The Topology of Music Recommendation Networks   \n2226329                                    Pattern ExcitationBased Processing The Music of The Brain   \n2227910                                                 Brain Music and nonPoisson Renewal Processes   \n2236785                                      A remark about the MerminSquires Music Halls inteludium   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Abstract  \\\n8087                                                                                                                                                                                                                                                                                                   We demonstrate relationships between the classic Euclidean algorithm and manyother fields of study particularly in the context of music and distancegeometry Specifically we show how the structure of the Euclidean algorithmdefines a family of rhythms which encompass over forty timelinesemphostinatos from traditional world music We prove that theseemphEuclidean rhythms have the mathematical property that their onsetpatterns are distributed as evenly as possible they maximize the sum of theEuclidean distances between all pairs of onsets viewing onsets as points on acircle Indeed Euclidean rhythms are the unique rhythms that maximize thisnotion of emphevenness We also show that essentially all Euclidean rhythmsare emphdeep each distinct distance between onsets occurs with a uniquemultiplicity and these multiplicies form an interval k Finallywe characterize all deep rhythms showing that they form a subclass ofgenerated rhythms which in turn proves a useful property called shelling Allof our results for musical rhythms apply equally well to musical scales Inaddition many of the problems we explore are interesting in their own right asdistance geometry problems on the circle some of the same problems wereexplored by ErdHos in the plane   \n14065                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           We test a segmentation algorithm based on the calculation of theJensenShannon divergence between probability distributions to two symbolicsequences of literary and musical origin The first sequence represents thesuccessive appearance of characters in a theatrical play and the secondrepresents the succession of tones from the twelvetone scale in a keyboardsonata The algorithm divides the sequences into segments of maximalcompositional divergence between them For the play these segments are relatedto changes in the frequency of appearance of different characters and in thegeographical setting of the action For the sonata the segments correspond totonal domains and reveal in detail the characteristic tonal progression of suchkind of musical composition   \n34765                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  The sequence of pitches which form a musical melody can be transposed orinverted Since the s music theorists have modeled musical transpositionand inversion in terms of an action of the dihedral group of order  Morerecently music theorists have found an intriguing second way that the dihedralgroup of order  acts on the set of major and minor chords We illustrate bothgeometrically and algebraically how these two actions are it dual Bothactions and their duality have been used to analyze works of music as diverseas Hindemith and the Beatles   \n79827                                                                                                                                                                                                                                                                                                                                                   Sound is a preferred context to build foundations on wave phenomena one ofthe most important disciplinary referents in physics It is also one of thebestset frameworks to achieve transversality overcoming scholastic level andactivating emotional aspects which are naturally connected with every day lifeas well as with music and perception Looking at sound and music by atransversal perspective  a borderline approach between science and art isthe adopted statement for a teaching proposal using metacognition as astrategy in scientific education This work analyzes curricular proposals onmusical acoustics planned by prospective secondaryschool teachers in theframework of a Formative Intervention Module answering the expectation ofmaking more effective teaching scientific subjects by improving creativecapabilities as well as leading to build logical and scientificcategorizations able to consciously discipline artistic activity in musicstudents With this aim a particular emphasis is given to those concepts like sound parameters and structural elements of a musical piece which arebest fitted to be addressed on a transversal perspective involvingsimultaneously physics psychophysics and music   \n83506                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        A raga is a melodic structure with fixed notes and a set of rulescharacterizing a certain mood endorsed through performance By a vadi swar ismeant that note which plays the most significant role in expressing the raga Asamvadi swar similarly is the second most significant note However thedetermination of their significance has an element of subjectivity and hence weare motivated to find some truths through an objective analysis The paperproposes a probabilistic method of note detection and demonstrates how therelative frequency relative number of occurrences of the pitch of the moreimportant notes stabilize far more quickly than that of others In addition acount for distinct transitory and similar looking nontransitory fundamentalfrequency movements but possibly embedding distinct emotions between thenotes is also taken depicting the varnalankars or musical ornaments decoratingthe notes and note sequences as rendered by the artist They reflect certainstructural properties of the ragas Several case studies are presented   \n...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ...   \n2218712                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Our understanding of ordinary matter is remarkably accurate and complete butit is based on principles that are very strange and unfamiliar As Illexplain weve come to understand matter to be a Music of the Void in aremarkably literal sense Just as we physicists finalized that wonderfulunderstanding towards the end of the twentieth century astronomers gave usback our humility by informing us that ordinary matter  what we andchemists and biologists and astronomers themselves have been studying allthese centuries constitutes only about  of the mass of the universe as awhole Ill describe some of our promising attempts to rise to this challengeby improving rather than merely complicating our description of the world   \n2219165                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        We study the topology of several music recommendation networks which risefrom relationships between artist cooccurrence of songs in playlists orexperts recommendation The analysis uncovers the emergence of complex networkphenomena in this kind of recommendation networks built considering artists asnodes and their resemblance as links We observe structural properties thatprovide some hints on navigation and possible optimizations on the design ofmusic recommendation systems Finally the analysis derived from existing musicknowledge sources provides a deeper understanding of the human music similarityperceptions   \n2226329                                                                                                                                                                                                                                                                                                                                                                                                            An approach to information processing based on the excitation of patterns ofactivity by nonlinear active resonators in response to their input patterns isproposed Arguments are presented to show that any computation performed by aconventional Turing machinebased computer called Tmachine in this papercould also be performed by the pattern excitationbased machine which will becalled Pmachine A realization of this processing scheme by neural networks isdiscussed In this realization the role of the resonators is played by neuralpattern excitation networks which are the neural circuits capable of excitingdifferent spatiotemporal patterns of activity in response to different inputsLearning in the neural pattern excitation networks is also considered It isshown that there is a duality between pattern excitation and patternrecognition neural networks which allows to create new pattern excitationmodes corresponding to recognizable input patterns based on Hebbian learningrules Hierarchically organized such networks can produce complex behaviorAnimal behavior human language and thought are treated as examples produced bysuch networks   \n2227910  In this paper we show that both music composition and brain function asrevealed by the Electroencephalogram EEG analysis are renewal nonPoissonprocesses living in the nonergodic dominion To reach this importantconclusion we process the data with the minimum spanning tree method so as todetect significant events thereby building a sequence of times which is thetime series to analyze Then we show that in both cases EEG and musiccomposition these significant events are the signature of a nonPoissonrenewal process This conclusion is reached using a techniques of statisticalanalysis recently developed by our group the Aging Experiment AE First wefind that in both cases the distances between two consecutive events aredescribed by nonexponential histograms thereby proving the nonPoisson natureof these processes The corresponding survival probabilities Psit are wellfitted by stretched exponentials Psit propto expgamma talphawith  alpha  The second step rests on the adoption of the AE whichshows that these are renewal processes We show that the renewal stretchedexponential is the emerging tip of an iceberg whose underwater part has slowtails with an inverse power law structure with power index mu    alphaWe find that both EEG and music composition yield mu   On the basis ofthe recently discovered complexity matching effect according to which acomplex system S with muS   responds only to a complex driving signalP with muP  muS we conclude that the results of our analysis mayexplain the influence of music on human brain   \n2236785                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    The MerminSquires Music Hall inteludium on the EinsteinPodolskyRosenaffair is analyzed by showing the fallacity of the OneBorelNormalityCriterion and the necessity of replacing it with the more restrictiveAlgorithmicRandomness Criterion   \n\n         Year  \n8087     2007  \n14065    2007  \n34765    2007  \n79827    2008  \n83506    2008  \n...       ...  \n2218712  2005  \n2219165  2005  \n2226329  2003  \n2227910  2006  \n2236785  2004  \n\n[1155 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Authors</th>\n      <th>Title</th>\n      <th>Abstract</th>\n      <th>Year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8087</th>\n      <td>Erik D. Demaine, Francisco Gomez-Martin, Henk Meijer, David Rappaport,\\n  Perouz Taslakian, Godfried T. Toussaint, Terry Winograd, David R. Wood</td>\n      <td>The Distance Geometry of Music</td>\n      <td>We demonstrate relationships between the classic Euclidean algorithm and manyother fields of study particularly in the context of music and distancegeometry Specifically we show how the structure of the Euclidean algorithmdefines a family of rhythms which encompass over forty timelinesemphostinatos from traditional world music We prove that theseemphEuclidean rhythms have the mathematical property that their onsetpatterns are distributed as evenly as possible they maximize the sum of theEuclidean distances between all pairs of onsets viewing onsets as points on acircle Indeed Euclidean rhythms are the unique rhythms that maximize thisnotion of emphevenness We also show that essentially all Euclidean rhythmsare emphdeep each distinct distance between onsets occurs with a uniquemultiplicity and these multiplicies form an interval k Finallywe characterize all deep rhythms showing that they form a subclass ofgenerated rhythms which in turn proves a useful property called shelling Allof our results for musical rhythms apply equally well to musical scales Inaddition many of the problems we explore are interesting in their own right asdistance geometry problems on the circle some of the same problems wereexplored by ErdHos in the plane</td>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>14065</th>\n      <td>Damian H. Zanette</td>\n      <td>Segmentation and Context of Literary and Musical Sequences</td>\n      <td>We test a segmentation algorithm based on the calculation of theJensenShannon divergence between probability distributions to two symbolicsequences of literary and musical origin The first sequence represents thesuccessive appearance of characters in a theatrical play and the secondrepresents the succession of tones from the twelvetone scale in a keyboardsonata The algorithm divides the sequences into segments of maximalcompositional divergence between them For the play these segments are relatedto changes in the frequency of appearance of different characters and in thegeographical setting of the action For the sonata the segments correspond totonal domains and reveal in detail the characteristic tonal progression of suchkind of musical composition</td>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>34765</th>\n      <td>Alissa S. Crans, Thomas M. Fiore, and Ramon Satyendra</td>\n      <td>Musical Actions of Dihedral Groups</td>\n      <td>The sequence of pitches which form a musical melody can be transposed orinverted Since the s music theorists have modeled musical transpositionand inversion in terms of an action of the dihedral group of order  Morerecently music theorists have found an intriguing second way that the dihedralgroup of order  acts on the set of major and minor chords We illustrate bothgeometrically and algebraically how these two actions are it dual Bothactions and their duality have been used to analyze works of music as diverseas Hindemith and the Beatles</td>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>79827</th>\n      <td>Erica Bisesi, and Marisa Michelini</td>\n      <td>Planning Curricular Proposals on Sound and Music with Prospective  SecondarySchool Teachers</td>\n      <td>Sound is a preferred context to build foundations on wave phenomena one ofthe most important disciplinary referents in physics It is also one of thebestset frameworks to achieve transversality overcoming scholastic level andactivating emotional aspects which are naturally connected with every day lifeas well as with music and perception Looking at sound and music by atransversal perspective  a borderline approach between science and art isthe adopted statement for a teaching proposal using metacognition as astrategy in scientific education This work analyzes curricular proposals onmusical acoustics planned by prospective secondaryschool teachers in theframework of a Formative Intervention Module answering the expectation ofmaking more effective teaching scientific subjects by improving creativecapabilities as well as leading to build logical and scientificcategorizations able to consciously discipline artistic activity in musicstudents With this aim a particular emphasis is given to those concepts like sound parameters and structural elements of a musical piece which arebest fitted to be addressed on a transversal perspective involvingsimultaneously physics psychophysics and music</td>\n      <td>2008</td>\n    </tr>\n    <tr>\n      <th>83506</th>\n      <td>Soubhik Chakraborty, Sandeep Singh Solanki, Sayan Roy, Shivee Chauhan,\\n  Sanjaya Shankar Tripathy and Kartik Mahto</td>\n      <td>A Statistical Approach to Modeling Indian Classical Music Performance</td>\n      <td>A raga is a melodic structure with fixed notes and a set of rulescharacterizing a certain mood endorsed through performance By a vadi swar ismeant that note which plays the most significant role in expressing the raga Asamvadi swar similarly is the second most significant note However thedetermination of their significance has an element of subjectivity and hence weare motivated to find some truths through an objective analysis The paperproposes a probabilistic method of note detection and demonstrates how therelative frequency relative number of occurrences of the pitch of the moreimportant notes stabilize far more quickly than that of others In addition acount for distinct transitory and similar looking nontransitory fundamentalfrequency movements but possibly embedding distinct emotions between thenotes is also taken depicting the varnalankars or musical ornaments decoratingthe notes and note sequences as rendered by the artist They reflect certainstructural properties of the ragas Several case studies are presented</td>\n      <td>2008</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2218712</th>\n      <td>Frank Wilczek</td>\n      <td>The Universe is a Strange Place</td>\n      <td>Our understanding of ordinary matter is remarkably accurate and complete butit is based on principles that are very strange and unfamiliar As Illexplain weve come to understand matter to be a Music of the Void in aremarkably literal sense Just as we physicists finalized that wonderfulunderstanding towards the end of the twentieth century astronomers gave usback our humility by informing us that ordinary matter  what we andchemists and biologists and astronomers themselves have been studying allthese centuries constitutes only about  of the mass of the universe as awhole Ill describe some of our promising attempts to rise to this challengeby improving rather than merely complicating our description of the world</td>\n      <td>2005</td>\n    </tr>\n    <tr>\n      <th>2219165</th>\n      <td>Pedro Cano, Oscar Celma, Markus Koppenberger, Javier M. Buld\\'u</td>\n      <td>The Topology of Music Recommendation Networks</td>\n      <td>We study the topology of several music recommendation networks which risefrom relationships between artist cooccurrence of songs in playlists orexperts recommendation The analysis uncovers the emergence of complex networkphenomena in this kind of recommendation networks built considering artists asnodes and their resemblance as links We observe structural properties thatprovide some hints on navigation and possible optimizations on the design ofmusic recommendation systems Finally the analysis derived from existing musicknowledge sources provides a deeper understanding of the human music similarityperceptions</td>\n      <td>2005</td>\n    </tr>\n    <tr>\n      <th>2226329</th>\n      <td>Lev Koyrakh</td>\n      <td>Pattern ExcitationBased Processing The Music of The Brain</td>\n      <td>An approach to information processing based on the excitation of patterns ofactivity by nonlinear active resonators in response to their input patterns isproposed Arguments are presented to show that any computation performed by aconventional Turing machinebased computer called Tmachine in this papercould also be performed by the pattern excitationbased machine which will becalled Pmachine A realization of this processing scheme by neural networks isdiscussed In this realization the role of the resonators is played by neuralpattern excitation networks which are the neural circuits capable of excitingdifferent spatiotemporal patterns of activity in response to different inputsLearning in the neural pattern excitation networks is also considered It isshown that there is a duality between pattern excitation and patternrecognition neural networks which allows to create new pattern excitationmodes corresponding to recognizable input patterns based on Hebbian learningrules Hierarchically organized such networks can produce complex behaviorAnimal behavior human language and thought are treated as examples produced bysuch networks</td>\n      <td>2003</td>\n    </tr>\n    <tr>\n      <th>2227910</th>\n      <td>Simone Bianco, Massimiliano Ignaccolo, Mark S. Rider, Mary J. Ross,\\n  Phil Winsor, and Paolo Grigolini</td>\n      <td>Brain Music and nonPoisson Renewal Processes</td>\n      <td>In this paper we show that both music composition and brain function asrevealed by the Electroencephalogram EEG analysis are renewal nonPoissonprocesses living in the nonergodic dominion To reach this importantconclusion we process the data with the minimum spanning tree method so as todetect significant events thereby building a sequence of times which is thetime series to analyze Then we show that in both cases EEG and musiccomposition these significant events are the signature of a nonPoissonrenewal process This conclusion is reached using a techniques of statisticalanalysis recently developed by our group the Aging Experiment AE First wefind that in both cases the distances between two consecutive events aredescribed by nonexponential histograms thereby proving the nonPoisson natureof these processes The corresponding survival probabilities Psit are wellfitted by stretched exponentials Psit propto expgamma talphawith  alpha  The second step rests on the adoption of the AE whichshows that these are renewal processes We show that the renewal stretchedexponential is the emerging tip of an iceberg whose underwater part has slowtails with an inverse power law structure with power index mu    alphaWe find that both EEG and music composition yield mu   On the basis ofthe recently discovered complexity matching effect according to which acomplex system S with muS   responds only to a complex driving signalP with muP  muS we conclude that the results of our analysis mayexplain the influence of music on human brain</td>\n      <td>2006</td>\n    </tr>\n    <tr>\n      <th>2236785</th>\n      <td>Gavriel Segre</td>\n      <td>A remark about the MerminSquires Music Halls inteludium</td>\n      <td>The MerminSquires Music Hall inteludium on the EinsteinPodolskyRosenaffair is analyzed by showing the fallacity of the OneBorelNormalityCriterion and the necessity of replacing it with the more restrictiveAlgorithmicRandomness Criterion</td>\n      <td>2004</td>\n    </tr>\n  </tbody>\n</table>\n<p>1155 rows × 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_Music_data.to_csv('/kaggle/working/Music_Data.csv', index=True, header= True)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:14:04.526514Z","iopub.execute_input":"2023-06-29T23:14:04.527154Z","iopub.status.idle":"2023-06-29T23:14:04.603959Z","shell.execute_reply.started":"2023-06-29T23:14:04.527118Z","shell.execute_reply":"2023-06-29T23:14:04.603007Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"### Filtering Music Composition","metadata":{}},{"cell_type":"code","source":"music_all_keywords1 = [ 'Composition',\n    'Compositional techniques',\n    'Compositional processes',\n    'Structure',\n    'Form',\n    'Melody',\n    'Harmony',\n    'Counterpoint',\n    'Improvisation',\n    'Arrangement',\n    'Notation',\n    'Analysis',\n    'Aesthetics',\n    'Computer-assisted composition',\n    'Algorithmic composition',\n    'Generative',\n    'Creative AI', \n    'Generating',\n    'Generation',\n    'mastering', 'mixing', 'mix','master','remixing', 'composing']","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:14:04.605945Z","iopub.execute_input":"2023-06-29T23:14:04.606830Z","iopub.status.idle":"2023-06-29T23:14:04.615037Z","shell.execute_reply.started":"2023-06-29T23:14:04.606783Z","shell.execute_reply":"2023-06-29T23:14:04.613484Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"dfMusicAICompose = df_Music_data[df_Music_data['Title'].apply(lambda title: any(keyword in title for keyword in music_all_keywords1)) |\n                              df_Music_data['Abstract'].apply(lambda abstract: any(keyword in abstract for keyword in music_all_keywords1))]","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:14:04.617375Z","iopub.execute_input":"2023-06-29T23:14:04.617796Z","iopub.status.idle":"2023-06-29T23:14:04.671082Z","shell.execute_reply.started":"2023-06-29T23:14:04.617764Z","shell.execute_reply":"2023-06-29T23:14:04.669751Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"dfMusicAICompose.shape","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:14:40.658078Z","iopub.execute_input":"2023-06-29T23:14:40.658577Z","iopub.status.idle":"2023-06-29T23:14:40.667556Z","shell.execute_reply.started":"2023-06-29T23:14:40.658544Z","shell.execute_reply":"2023-06-29T23:14:40.666129Z"},"trusted":true},"execution_count":58,"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"(365, 4)"},"metadata":{}}]},{"cell_type":"code","source":"dfMusicAICompose['Year'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:12.110818Z","iopub.execute_input":"2023-06-29T23:15:12.112469Z","iopub.status.idle":"2023-06-29T23:15:12.122271Z","shell.execute_reply.started":"2023-06-29T23:15:12.112423Z","shell.execute_reply":"2023-06-29T23:15:12.121289Z"},"trusted":true},"execution_count":60,"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"2022    74\n2021    69\n2020    57\n2019    35\n2017    32\n2018    32\n2023    28\n2016    16\n2015     6\n2014     4\n2012     3\n2013     3\n2010     2\n2008     1\n2009     1\n2011     1\n2002     1\nName: Year, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df_Music_Compose_data = pd.DataFrame({\n    'Authors': dfMusicAICompose['Authors'],\n    'Title': dfMusicAICompose['Title'],\n    'Abstract': dfMusicAICompose['Abstract'],\n    'Year': dfMusicAICompose['Year']\n})","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:31.696306Z","iopub.execute_input":"2023-06-29T23:15:31.696774Z","iopub.status.idle":"2023-06-29T23:15:31.705491Z","shell.execute_reply.started":"2023-06-29T23:15:31.696736Z","shell.execute_reply":"2023-06-29T23:15:31.704102Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"df_Music_Compose_data.shape","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:31.708042Z","iopub.execute_input":"2023-06-29T23:15:31.709746Z","iopub.status.idle":"2023-06-29T23:15:31.732662Z","shell.execute_reply.started":"2023-06-29T23:15:31.709695Z","shell.execute_reply":"2023-06-29T23:15:31.731363Z"},"trusted":true},"execution_count":65,"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"(365, 4)"},"metadata":{}}]},{"cell_type":"code","source":"df_Music_Compose_data.to_csv('/kaggle/working/Music_Compose_Data.csv', index=True, header= True)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:31.734651Z","iopub.execute_input":"2023-06-29T23:15:31.735485Z","iopub.status.idle":"2023-06-29T23:15:31.777904Z","shell.execute_reply.started":"2023-06-29T23:15:31.735434Z","shell.execute_reply":"2023-06-29T23:15:31.776457Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"df_Music_Compose_data.shape","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:31.781246Z","iopub.execute_input":"2023-06-29T23:15:31.781674Z","iopub.status.idle":"2023-06-29T23:15:31.790317Z","shell.execute_reply.started":"2023-06-29T23:15:31.781644Z","shell.execute_reply":"2023-06-29T23:15:31.789006Z"},"trusted":true},"execution_count":67,"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"(365, 4)"},"metadata":{}}]},{"cell_type":"code","source":"df_Music_Compose_data.Year.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:31.792020Z","iopub.execute_input":"2023-06-29T23:15:31.792411Z","iopub.status.idle":"2023-06-29T23:15:31.811241Z","shell.execute_reply.started":"2023-06-29T23:15:31.792380Z","shell.execute_reply":"2023-06-29T23:15:31.810189Z"},"trusted":true},"execution_count":68,"outputs":[{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"2022    74\n2021    69\n2020    57\n2019    35\n2017    32\n2018    32\n2023    28\n2016    16\n2015     6\n2014     4\n2012     3\n2013     3\n2010     2\n2008     1\n2009     1\n2011     1\n2002     1\nName: Year, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\nyears = [2022, 2021, 2020, 2019, 2018, 2017, 2016, 2015, 2014, 2013, 2012, 2011, 2010, 2023,\n2009, 2008, 2007, 2006, 2005, 2004, 2003, 2002, 2001, 2000, 1999, 1998, 1997, 1996,\n1995, 1994, 1993, 1992, 1991, 1990, 1989, 1988, 1986]\n\nno_of_papers = [185976, 181599, 178275, 155917, 140377, 123781, 113440, 105130, 97590, 92875,\n84374, 76602, 70288, 64429, 64071, 58810, 55749, 50305, 46874, 43713, 39392,\n36105, 33140, 30669, 27700, 24170, 19610, 15872, 13006, 10078, 6729, 3190,\n353, 26, 6, 1, 1]\n\n# colors = np.random.rand(len(years))\n\n# plt.figure(figsize=(10, 6))\n# plt.scatter(years, no_of_papers, c=colors)\n# plt.xlabel('Years')\n# plt.ylabel('No of papers')\n# plt.title('Number of Papers by Year')\n# plt.xticks(rotation=45)\n# plt.tight_layout()\n# plt.show()\n# Generate unique colors for each year\ncolors = np.random.rand(len(years))\n\n# Fit a polynomial curve to the data\ncurve_fit = np.polyfit(years, no_of_papers, 3)  # Adjust the degree of the polynomial if needed\ncurve = np.poly1d(curve_fit)\ncurve_x = np.linspace(min(years), max(years), 100)\ncurve_y = curve(curve_x)\n\nplt.figure(figsize=(10, 6))\nplt.scatter(years, no_of_papers, c=colors)\nplt.plot(curve_x, curve_y, color='black', label='Curve Fit')\nplt.xlabel('Years')\nplt.ylabel('No of papers')\nplt.title('Number of Papers by Year')\nplt.xticks(rotation=45)\nplt.legend()\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:31.813143Z","iopub.execute_input":"2023-06-29T23:15:31.813525Z","iopub.status.idle":"2023-06-29T23:15:32.431514Z","shell.execute_reply.started":"2023-06-29T23:15:31.813493Z","shell.execute_reply":"2023-06-29T23:15:32.429840Z"},"trusted":true},"execution_count":69,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"dfMusic1000 = pd.read_csv('/kaggle/input/AIMusic1000/AIMusic1000.csv', delimiter=',')","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:32.433442Z","iopub.execute_input":"2023-06-29T23:15:32.433845Z","iopub.status.idle":"2023-06-29T23:15:32.492171Z","shell.execute_reply.started":"2023-06-29T23:15:32.433811Z","shell.execute_reply":"2023-06-29T23:15:32.490911Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"dfMusic1000.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:32.493796Z","iopub.execute_input":"2023-06-29T23:15:32.495173Z","iopub.status.idle":"2023-06-29T23:15:32.533098Z","shell.execute_reply.started":"2023-06-29T23:15:32.495126Z","shell.execute_reply":"2023-06-29T23:15:32.531740Z"},"trusted":true},"execution_count":71,"outputs":[{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"   Cites                      Authors  \\\n0      0                    U Kalidas   \n1      6  B Caramiaux, S Fdili Alaoui   \n2      0         E Deruty, M Grachten   \n3     30  A Vizcaíno-Verdú, I Aguaded   \n4      0        JEK Parker, S Dockray   \n\n                                                                                                                                          Title  \\\n0  \" 12\" Music Visualisation: Exploring Generative and Non-Generative techniques within a rhythmic framework in a performative spatial setting.   \n1                                              \" Explorers of Unknown Planets\" Practices and Politics of Artificial Intelligence in Visual Arts   \n2                                                                                        \" Melatonin\": A Case Study on AI-induced Musical Style   \n3                                                                # ThisIsMeChallenge and Music for Empowerment of Marginalized Groups on TikTok   \n4                                                                  'All possible sounds': speech, music, and the emergence of machine listening   \n\n     Year                             Source              Publisher  \\\n0  2023.0                                NaN  openresearch.ocadu.ca   \n1  2022.0  Proceedings of the ACM on Human …             dl.acm.org   \n2  2022.0    arXiv preprint arXiv:2208.08968              arxiv.org   \n3  2022.0            Media and Communication             ssoar.info   \n4  2023.0                      Sound Studies        Taylor &Francis   \n\n                                                          ArticleURL  \\\n0                      https://openresearch.ocadu.ca/id/eprint/4024/   \n1                         https://dl.acm.org/doi/abs/10.1145/3555578   \n2                                   https://arxiv.org/abs/2208.08968   \n3                 https://www.ssoar.info/ssoar/handle/document/77858   \n4  https://www.tandfonline.com/doi/abs/10.1080/20551940.2023.2195057   \n\n                                                                                      CitesURL  \\\n0                                                                                          NaN   \n1   https://scholar.google.com/scholar?cites=6161039021353815985&as_sdt=2005&sciodt=2007&hl=en   \n2                                                                                          NaN   \n3  https://scholar.google.com/scholar?cites=15934592861495898206&as_sdt=2005&sciodt=2007&hl=en   \n4                                                                                          NaN   \n\n   GSRank            QueryDate  ... StartPage EndPage  ECC  CitesPerYear  \\\n0     796  2023-05-31 10:24:35  ...       NaN     NaN    0           0.0   \n1     223  2023-05-31 10:24:35  ...       NaN     NaN    6           6.0   \n2     157  2023-05-31 10:24:35  ...       NaN     NaN    0           0.0   \n3     311  2023-05-31 10:24:35  ...       NaN     NaN   30          30.0   \n4     357  2023-05-31 10:24:35  ...       NaN     NaN    0           0.0   \n\n   CitesPerAuthor  AuthorCount  Age  \\\n0               0            1  1.0   \n1               3            2  1.0   \n2               0            2  1.0   \n3              15            2  1.0   \n4               0            2  1.0   \n\n                                                                                                                                                                                             Abstract  \\\n0            … The two day workshop/ lec-dem involved participants working in groups to create Music on Ableton live Digital Audio Workstation, using AI based tools created by the TheKlong studio …   \n1  … and creatives are interacting with AI and ML in their creative process. In this section, we review previous works looking at AI and ML as creative tools, especially in music and visual arts. …   \n2    … yet for the use of AI tools in music. Nevertheless we believe it is instructive to study how individual music artists use music AI tools, and how these shape the resulting music, as it can …   \n3        … introduces the musical theme of … music venue for social empowerment, we conducted a quantitative content analysis of 100 TikTok posts under the hashtag, and an artificial intelligence …   \n4               … As we will see, from the 1940s to roughly the end of the 1980s, work on artificial intelligence and audio developed along two major streams. There was work on speech recognition/…   \n\n                                                                                                                                                                       FullTextURL  \\\n0                                                                                           https://openresearch.ocadu.ca/id/eprint/4024/1/Kalidas_Unnikrishnan_2023_MDES_DIGF.pdf   \n1                                                                                                                                       https://hal.inria.fr/hal-03762351/document   \n2                                                                                                                                                 https://arxiv.org/pdf/2208.08968   \n3  https://www.ssoar.info/ssoar/bitstream/handle/document/77858/ssoar-mediacomm-2022-1-vizcaino-verdu_et_al-ThisIsMeChallenge_and_Music_for_Empowerment.pdf?sequence=1&isAllowed=y   \n4                                                                                                                                                                              NaN   \n\n                                                                                                                               RelatedURL  \n0                                                                                                                                     NaN  \n1  https://scholar.google.com/scholar?q=related:sU8d61hogFUJ:scholar.google.com/&scioq=music+ai&hl=en&as_sdt=2007&as_ylo=2022&as_yhi=2023  \n2  https://scholar.google.com/scholar?q=related:ZHDCFeHS2mwJ:scholar.google.com/&scioq=music+ai&hl=en&as_sdt=2007&as_ylo=2022&as_yhi=2023  \n3  https://scholar.google.com/scholar?q=related:XjDwTcgLI90J:scholar.google.com/&scioq=music+ai&hl=en&as_sdt=2007&as_ylo=2022&as_yhi=2023  \n4                                                                                                                                     NaN  \n\n[5 rows x 26 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cites</th>\n      <th>Authors</th>\n      <th>Title</th>\n      <th>Year</th>\n      <th>Source</th>\n      <th>Publisher</th>\n      <th>ArticleURL</th>\n      <th>CitesURL</th>\n      <th>GSRank</th>\n      <th>QueryDate</th>\n      <th>...</th>\n      <th>StartPage</th>\n      <th>EndPage</th>\n      <th>ECC</th>\n      <th>CitesPerYear</th>\n      <th>CitesPerAuthor</th>\n      <th>AuthorCount</th>\n      <th>Age</th>\n      <th>Abstract</th>\n      <th>FullTextURL</th>\n      <th>RelatedURL</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>U Kalidas</td>\n      <td>\" 12\" Music Visualisation: Exploring Generative and Non-Generative techniques within a rhythmic framework in a performative spatial setting.</td>\n      <td>2023.0</td>\n      <td>NaN</td>\n      <td>openresearch.ocadu.ca</td>\n      <td>https://openresearch.ocadu.ca/id/eprint/4024/</td>\n      <td>NaN</td>\n      <td>796</td>\n      <td>2023-05-31 10:24:35</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>… The two day workshop/ lec-dem involved participants working in groups to create Music on Ableton live Digital Audio Workstation, using AI based tools created by the TheKlong studio …</td>\n      <td>https://openresearch.ocadu.ca/id/eprint/4024/1/Kalidas_Unnikrishnan_2023_MDES_DIGF.pdf</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6</td>\n      <td>B Caramiaux, S Fdili Alaoui</td>\n      <td>\" Explorers of Unknown Planets\" Practices and Politics of Artificial Intelligence in Visual Arts</td>\n      <td>2022.0</td>\n      <td>Proceedings of the ACM on Human …</td>\n      <td>dl.acm.org</td>\n      <td>https://dl.acm.org/doi/abs/10.1145/3555578</td>\n      <td>https://scholar.google.com/scholar?cites=6161039021353815985&amp;as_sdt=2005&amp;sciodt=2007&amp;hl=en</td>\n      <td>223</td>\n      <td>2023-05-31 10:24:35</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>6</td>\n      <td>6.0</td>\n      <td>3</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>… and creatives are interacting with AI and ML in their creative process. In this section, we review previous works looking at AI and ML as creative tools, especially in music and visual arts. …</td>\n      <td>https://hal.inria.fr/hal-03762351/document</td>\n      <td>https://scholar.google.com/scholar?q=related:sU8d61hogFUJ:scholar.google.com/&amp;scioq=music+ai&amp;hl=en&amp;as_sdt=2007&amp;as_ylo=2022&amp;as_yhi=2023</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>E Deruty, M Grachten</td>\n      <td>\" Melatonin\": A Case Study on AI-induced Musical Style</td>\n      <td>2022.0</td>\n      <td>arXiv preprint arXiv:2208.08968</td>\n      <td>arxiv.org</td>\n      <td>https://arxiv.org/abs/2208.08968</td>\n      <td>NaN</td>\n      <td>157</td>\n      <td>2023-05-31 10:24:35</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>… yet for the use of AI tools in music. Nevertheless we believe it is instructive to study how individual music artists use music AI tools, and how these shape the resulting music, as it can …</td>\n      <td>https://arxiv.org/pdf/2208.08968</td>\n      <td>https://scholar.google.com/scholar?q=related:ZHDCFeHS2mwJ:scholar.google.com/&amp;scioq=music+ai&amp;hl=en&amp;as_sdt=2007&amp;as_ylo=2022&amp;as_yhi=2023</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>30</td>\n      <td>A Vizcaíno-Verdú, I Aguaded</td>\n      <td># ThisIsMeChallenge and Music for Empowerment of Marginalized Groups on TikTok</td>\n      <td>2022.0</td>\n      <td>Media and Communication</td>\n      <td>ssoar.info</td>\n      <td>https://www.ssoar.info/ssoar/handle/document/77858</td>\n      <td>https://scholar.google.com/scholar?cites=15934592861495898206&amp;as_sdt=2005&amp;sciodt=2007&amp;hl=en</td>\n      <td>311</td>\n      <td>2023-05-31 10:24:35</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>30</td>\n      <td>30.0</td>\n      <td>15</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>… introduces the musical theme of … music venue for social empowerment, we conducted a quantitative content analysis of 100 TikTok posts under the hashtag, and an artificial intelligence …</td>\n      <td>https://www.ssoar.info/ssoar/bitstream/handle/document/77858/ssoar-mediacomm-2022-1-vizcaino-verdu_et_al-ThisIsMeChallenge_and_Music_for_Empowerment.pdf?sequence=1&amp;isAllowed=y</td>\n      <td>https://scholar.google.com/scholar?q=related:XjDwTcgLI90J:scholar.google.com/&amp;scioq=music+ai&amp;hl=en&amp;as_sdt=2007&amp;as_ylo=2022&amp;as_yhi=2023</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>JEK Parker, S Dockray</td>\n      <td>'All possible sounds': speech, music, and the emergence of machine listening</td>\n      <td>2023.0</td>\n      <td>Sound Studies</td>\n      <td>Taylor &amp;Francis</td>\n      <td>https://www.tandfonline.com/doi/abs/10.1080/20551940.2023.2195057</td>\n      <td>NaN</td>\n      <td>357</td>\n      <td>2023-05-31 10:24:35</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>… As we will see, from the 1940s to roughly the end of the 1980s, work on artificial intelligence and audio developed along two major streams. There was work on speech recognition/…</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 26 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"dfMusic1000.shape","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:32.537443Z","iopub.execute_input":"2023-06-29T23:15:32.537826Z","iopub.status.idle":"2023-06-29T23:15:32.545408Z","shell.execute_reply.started":"2023-06-29T23:15:32.537797Z","shell.execute_reply":"2023-06-29T23:15:32.544533Z"},"trusted":true},"execution_count":72,"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"(992, 26)"},"metadata":{}}]},{"cell_type":"code","source":"dfMusic1000['Title'] = dfMusic1000['Title'].astype(str)\n\ndfMusic1000['Abstract'] = dfMusic1000['Abstract'].astype(str)\n\ndfMusic1000.dtypes","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:32.546759Z","iopub.execute_input":"2023-06-29T23:15:32.547973Z","iopub.status.idle":"2023-06-29T23:15:32.566141Z","shell.execute_reply.started":"2023-06-29T23:15:32.547939Z","shell.execute_reply":"2023-06-29T23:15:32.564601Z"},"trusted":true},"execution_count":73,"outputs":[{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"Cites               int64\nAuthors            object\nTitle              object\nYear              float64\nSource             object\nPublisher          object\nArticleURL         object\nCitesURL           object\nGSRank              int64\nQueryDate          object\nType               object\nDOI                object\nISSN              float64\nCitationURL       float64\nVolume            float64\nIssue             float64\nStartPage         float64\nEndPage           float64\nECC                 int64\nCitesPerYear      float64\nCitesPerAuthor      int64\nAuthorCount         int64\nAge               float64\nAbstract           object\nFullTextURL        object\nRelatedURL         object\ndtype: object"},"metadata":{}}]},{"cell_type":"code","source":"music_all_keywords1","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:32.568038Z","iopub.execute_input":"2023-06-29T23:15:32.568584Z","iopub.status.idle":"2023-06-29T23:15:32.584467Z","shell.execute_reply.started":"2023-06-29T23:15:32.568551Z","shell.execute_reply":"2023-06-29T23:15:32.583097Z"},"trusted":true},"execution_count":74,"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"['Composition',\n 'Compositional techniques',\n 'Compositional processes',\n 'Structure',\n 'Form',\n 'Melody',\n 'Harmony',\n 'Counterpoint',\n 'Improvisation',\n 'Arrangement',\n 'Notation',\n 'Analysis',\n 'Aesthetics',\n 'Computer-assisted composition',\n 'Algorithmic composition',\n 'Generative',\n 'Creative AI',\n 'Generating',\n 'Generation',\n 'mastering',\n 'mixing',\n 'mix',\n 'master',\n 'remixing',\n 'composing']"},"metadata":{}}]},{"cell_type":"code","source":"dfMusic2022and2023 = dfMusic1000[dfMusic1000['Title'].apply(lambda title: any(keyword in title for keyword in music_all_keywords1)) |\n                              dfMusic1000['Abstract'].apply(lambda abstract: any(keyword in abstract for keyword in music_all_keywords1))]","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:32.586415Z","iopub.execute_input":"2023-06-29T23:15:32.586821Z","iopub.status.idle":"2023-06-29T23:15:32.616734Z","shell.execute_reply.started":"2023-06-29T23:15:32.586790Z","shell.execute_reply":"2023-06-29T23:15:32.614836Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"dfMusic2022and2023","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:32.618923Z","iopub.execute_input":"2023-06-29T23:15:32.619349Z","iopub.status.idle":"2023-06-29T23:15:32.667229Z","shell.execute_reply.started":"2023-06-29T23:15:32.619314Z","shell.execute_reply":"2023-06-29T23:15:32.665546Z"},"trusted":true},"execution_count":76,"outputs":[{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"     Cites                                     Authors  \\\n0        0                                   U Kalidas   \n12      11    C Zhang, C Zhang, S Zheng, Y Qiao, C Li…   \n17       0                                      S Lowe   \n18       0                 H Wang, S Han, G Li, B Zhao   \n20       4         PS Yadav, S Khan, YV Singh, P Garg…   \n..     ...                                         ...   \n949      3  W Zai El Amri, O Tautz, H Ritter, A Melnik   \n965      0                A Ranjan, VNJ Behera, M Reza   \n974      0                            C Limburg Stirum   \n977      0          Y Tie, T Wang, C Jin, X Li, P Yang   \n985      0         V Efimova, V Shalamov, A Filchenkov   \n\n                                                                                                                                                                   Title  \\\n0                           \" 12\" Music Visualisation: Exploring Generative and Non-Generative techniques within a rhythmic framework in a performative spatial setting.   \n12                                                                               A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?   \n17                                                                                                 A Fully-Unsupervised Generative Method for Choreo-Musical Translation   \n18                                                                                              A Hybrid Neural Network for Music Generation Using Frequency Domain Data   \n20                                                                                   A Lightweight Deep Learning-Based Approach for Jazz Music Generation in MIDI Format   \n..                                                                                                                                                                   ...   \n949                                                                                                           Transfer learning with jukebox for music source separation   \n965                                        Using a Bi-Directional Long Short-Term Memory Model with Attention Mechanism Trained on MIDI Data for Generating Unique Music   \n974                                                                                What would you like to hear? Finding features for an artificial intelligence drive DJ   \n977                                                                Wireless Communications and Mobile Computing Multitrack Music Generation Network Based on Music Rules   \n985  … : Generating Covers for Music and Books via Extracting Keywords: This paper presents two methods to generate high resolution uncopyrighted book covers or music …   \n\n       Year                                         Source  \\\n0    2023.0                                            NaN   \n12   2023.0                         arXiv preprint arXiv …   \n17      NaN                                       slowe.io   \n18   2023.0   … Conference on Artificial Intelligence in …   \n20   2022.0                                Computational …   \n..      ...                                            ...   \n949  2022.0                      Artificial Intelligence …   \n965  2022.0  Artificial Intelligence for Data Science in …   \n974  2022.0                                            NaN   \n977  2022.0                  Wireless Communications and …   \n985  2022.0             … on Artificial Intelligence and …   \n\n                 Publisher  \\\n0    openresearch.ocadu.ca   \n12               arxiv.org   \n17                     NaN   \n18                Springer   \n20             hindawi.com   \n..                     ...   \n949               Springer   \n965               Springer   \n974       essay.utwente.nl   \n977            hindawi.com   \n985             dl.acm.org   \n\n                                                                    ArticleURL  \\\n0                                https://openresearch.ocadu.ca/id/eprint/4024/   \n12                                            https://arxiv.org/abs/2303.11717   \n17                                          https://slowe.io/content/cs229.pdf   \n18              https://link.springer.com/chapter/10.1007/978-981-99-1256-8_26   \n20                          https://www.hindawi.com/journals/cin/2022/2140895/   \n..                                                                         ...   \n949             https://link.springer.com/chapter/10.1007/978-3-031-08337-2_35   \n965             https://link.springer.com/chapter/10.1007/978-3-030-92245-0_10   \n974  http://essay.utwente.nl/92465/2/Public_Summary-Limburg-Sitrum-Sam-van.pdf   \n977                        https://www.hindawi.com/journals/wcmc/2022/5845689/   \n985                         https://dl.acm.org/doi/abs/10.1145/3573942.3574088   \n\n                                                                                        CitesURL  \\\n0                                                                                            NaN   \n12   https://scholar.google.com/scholar?cites=10162538139609691591&as_sdt=2005&sciodt=2007&hl=en   \n17                                                                                           NaN   \n18                                                                                           NaN   \n20    https://scholar.google.com/scholar?cites=6733953986624070978&as_sdt=2005&sciodt=2007&hl=en   \n..                                                                                           ...   \n949   https://scholar.google.com/scholar?cites=2578766836719982342&as_sdt=2005&sciodt=2007&hl=en   \n965                                                                                          NaN   \n974                                                                                          NaN   \n977                                                                                          NaN   \n985                                                                                          NaN   \n\n     GSRank            QueryDate  ... StartPage EndPage  ECC  CitesPerYear  \\\n0       796  2023-05-31 10:24:35  ...       NaN     NaN    0           0.0   \n12      340  2023-05-31 10:24:35  ...       NaN     NaN   11          11.0   \n17      762  2023-05-31 10:24:35  ...       NaN     NaN    0           0.0   \n18      816  2023-05-31 10:24:35  ...       NaN     NaN    0           0.0   \n20      292  2023-05-31 10:24:35  ...       NaN     NaN    4           4.0   \n..      ...                  ...  ...       ...     ...  ...           ...   \n949     613  2023-05-31 10:24:35  ...       NaN     NaN    3           3.0   \n965     204  2023-05-31 10:24:35  ...       NaN     NaN    0           0.0   \n974     807  2023-05-31 10:24:35  ...       NaN     NaN    0           0.0   \n977     146  2023-05-31 10:24:35  ...       NaN     NaN    0           0.0   \n985     820  2023-05-31 10:24:35  ...       NaN     NaN    0           0.0   \n\n     CitesPerAuthor  AuthorCount  Age  \\\n0                 0            1  1.0   \n12                2            6  1.0   \n17                0            1  NaN   \n18                0            4  1.0   \n20                1            5  1.0   \n..              ...          ...  ...   \n949               1            4  1.0   \n965               0            3  1.0   \n974               0            1  1.0   \n977               0            5  1.0   \n985               0            3  1.0   \n\n                                                                                                                                                                                                   Abstract  \\\n0                  … The two day workshop/ lec-dem involved participants working in groups to create Music on Ableton live Digital Audio Workstation, using AI based tools created by the TheKlong studio …   \n12          … new music, while some software edit compositions in the style of various composers. Music … it is because of using AI to compose music or to help musicians. A fantastic illustration of an …   \n17           … It is clear that AI systems applied to these tasks need similar … Our algorithm will be able to take as input either music (… that represents choreography and music that should be paired …   \n18                        … adversarial network to generate music in an end-to-end way. The automatic music generation method proposed in this paper explores a new representation of music data, and the …   \n20                 … In paper [15], jazz transformer-based AIcomposed music quantitative measures are discussed. is system measures the jazz transformation using the deep learning model. In paper [16], …   \n..                                                                                                                                                                                                      ...   \n949                     In this work, we demonstrate how a publicly available, pre-trained Jukebox model can be adapted for the problem of audio source separation from a single mixed audio channel. Our …   \n965         … But this cannot similarly be done for music, as music is … music, the learning and generation aspect can make use of a high-level representation of the musical data, such as MIDI (Musical …   \n974  … artificial intelligence (AI) driven DJ, get closer to a product they can launch onto the market. … The user could play music mixed by the AI as a party host, or join a room as a guest. All users …   \n977         … human music was higher than AI (see Table 1). However, across all participants, our AI music … 7.93), indicating that the quality of our AI music creation was quite close to that of human …   \n985           … music album covers on their own using only text annotation, that will considerably simplify and speed up the process of the cover creation and turning a book or music … or musical album …   \n\n                                                                                FullTextURL  \\\n0    https://openresearch.ocadu.ca/id/eprint/4024/1/Kalidas_Unnikrishnan_2023_MDES_DIGF.pdf   \n12                                                         https://arxiv.org/pdf/2303.11717   \n17                                                       https://slowe.io/content/cs229.pdf   \n18                                                                                      NaN   \n20                                       https://www.hindawi.com/journals/cin/2022/2140895/   \n..                                                                                      ...   \n949                                                        https://arxiv.org/pdf/2111.14200   \n965                                                                                     NaN   \n974               http://essay.utwente.nl/92465/2/Public_Summary-Limburg-Sitrum-Sam-van.pdf   \n977                                     https://www.hindawi.com/journals/wcmc/2022/5845689/   \n985                                                                                     NaN   \n\n                                                                                                                                 RelatedURL  \n0                                                                                                                                       NaN  \n12                                                                                                                                      NaN  \n17   https://scholar.google.com/scholar?q=related:gM4lmW8KfdoJ:scholar.google.com/&scioq=music+ai&hl=en&as_sdt=2007&as_ylo=2022&as_yhi=2023  \n18                                                                                                                                      NaN  \n20   https://scholar.google.com/scholar?q=related:QqWNCnnPc10J:scholar.google.com/&scioq=music+ai&hl=en&as_sdt=2007&as_ylo=2022&as_yhi=2023  \n..                                                                                                                                      ...  \n949  https://scholar.google.com/scholar?q=related:BpdAbcaeySMJ:scholar.google.com/&scioq=music+ai&hl=en&as_sdt=2007&as_ylo=2022&as_yhi=2023  \n965  https://scholar.google.com/scholar?q=related:3jP4ulSdZVMJ:scholar.google.com/&scioq=music+ai&hl=en&as_sdt=2007&as_ylo=2022&as_yhi=2023  \n974  https://scholar.google.com/scholar?q=related:Zp7U_zTQ3dYJ:scholar.google.com/&scioq=music+ai&hl=en&as_sdt=2007&as_ylo=2022&as_yhi=2023  \n977  https://scholar.google.com/scholar?q=related:hF3XBL4cf0sJ:scholar.google.com/&scioq=music+ai&hl=en&as_sdt=2007&as_ylo=2022&as_yhi=2023  \n985                                                                                                                                     NaN  \n\n[202 rows x 26 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cites</th>\n      <th>Authors</th>\n      <th>Title</th>\n      <th>Year</th>\n      <th>Source</th>\n      <th>Publisher</th>\n      <th>ArticleURL</th>\n      <th>CitesURL</th>\n      <th>GSRank</th>\n      <th>QueryDate</th>\n      <th>...</th>\n      <th>StartPage</th>\n      <th>EndPage</th>\n      <th>ECC</th>\n      <th>CitesPerYear</th>\n      <th>CitesPerAuthor</th>\n      <th>AuthorCount</th>\n      <th>Age</th>\n      <th>Abstract</th>\n      <th>FullTextURL</th>\n      <th>RelatedURL</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>U Kalidas</td>\n      <td>\" 12\" Music Visualisation: Exploring Generative and Non-Generative techniques within a rhythmic framework in a performative spatial setting.</td>\n      <td>2023.0</td>\n      <td>NaN</td>\n      <td>openresearch.ocadu.ca</td>\n      <td>https://openresearch.ocadu.ca/id/eprint/4024/</td>\n      <td>NaN</td>\n      <td>796</td>\n      <td>2023-05-31 10:24:35</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>… The two day workshop/ lec-dem involved participants working in groups to create Music on Ableton live Digital Audio Workstation, using AI based tools created by the TheKlong studio …</td>\n      <td>https://openresearch.ocadu.ca/id/eprint/4024/1/Kalidas_Unnikrishnan_2023_MDES_DIGF.pdf</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>11</td>\n      <td>C Zhang, C Zhang, S Zheng, Y Qiao, C Li…</td>\n      <td>A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?</td>\n      <td>2023.0</td>\n      <td>arXiv preprint arXiv …</td>\n      <td>arxiv.org</td>\n      <td>https://arxiv.org/abs/2303.11717</td>\n      <td>https://scholar.google.com/scholar?cites=10162538139609691591&amp;as_sdt=2005&amp;sciodt=2007&amp;hl=en</td>\n      <td>340</td>\n      <td>2023-05-31 10:24:35</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>11</td>\n      <td>11.0</td>\n      <td>2</td>\n      <td>6</td>\n      <td>1.0</td>\n      <td>… new music, while some software edit compositions in the style of various composers. Music … it is because of using AI to compose music or to help musicians. A fantastic illustration of an …</td>\n      <td>https://arxiv.org/pdf/2303.11717</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0</td>\n      <td>S Lowe</td>\n      <td>A Fully-Unsupervised Generative Method for Choreo-Musical Translation</td>\n      <td>NaN</td>\n      <td>slowe.io</td>\n      <td>NaN</td>\n      <td>https://slowe.io/content/cs229.pdf</td>\n      <td>NaN</td>\n      <td>762</td>\n      <td>2023-05-31 10:24:35</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>… It is clear that AI systems applied to these tasks need similar … Our algorithm will be able to take as input either music (… that represents choreography and music that should be paired …</td>\n      <td>https://slowe.io/content/cs229.pdf</td>\n      <td>https://scholar.google.com/scholar?q=related:gM4lmW8KfdoJ:scholar.google.com/&amp;scioq=music+ai&amp;hl=en&amp;as_sdt=2007&amp;as_ylo=2022&amp;as_yhi=2023</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0</td>\n      <td>H Wang, S Han, G Li, B Zhao</td>\n      <td>A Hybrid Neural Network for Music Generation Using Frequency Domain Data</td>\n      <td>2023.0</td>\n      <td>… Conference on Artificial Intelligence in …</td>\n      <td>Springer</td>\n      <td>https://link.springer.com/chapter/10.1007/978-981-99-1256-8_26</td>\n      <td>NaN</td>\n      <td>816</td>\n      <td>2023-05-31 10:24:35</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1.0</td>\n      <td>… adversarial network to generate music in an end-to-end way. The automatic music generation method proposed in this paper explores a new representation of music data, and the …</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>4</td>\n      <td>PS Yadav, S Khan, YV Singh, P Garg…</td>\n      <td>A Lightweight Deep Learning-Based Approach for Jazz Music Generation in MIDI Format</td>\n      <td>2022.0</td>\n      <td>Computational …</td>\n      <td>hindawi.com</td>\n      <td>https://www.hindawi.com/journals/cin/2022/2140895/</td>\n      <td>https://scholar.google.com/scholar?cites=6733953986624070978&amp;as_sdt=2005&amp;sciodt=2007&amp;hl=en</td>\n      <td>292</td>\n      <td>2023-05-31 10:24:35</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>4.0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>1.0</td>\n      <td>… In paper [15], jazz transformer-based AIcomposed music quantitative measures are discussed. is system measures the jazz transformation using the deep learning model. In paper [16], …</td>\n      <td>https://www.hindawi.com/journals/cin/2022/2140895/</td>\n      <td>https://scholar.google.com/scholar?q=related:QqWNCnnPc10J:scholar.google.com/&amp;scioq=music+ai&amp;hl=en&amp;as_sdt=2007&amp;as_ylo=2022&amp;as_yhi=2023</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>949</th>\n      <td>3</td>\n      <td>W Zai El Amri, O Tautz, H Ritter, A Melnik</td>\n      <td>Transfer learning with jukebox for music source separation</td>\n      <td>2022.0</td>\n      <td>Artificial Intelligence …</td>\n      <td>Springer</td>\n      <td>https://link.springer.com/chapter/10.1007/978-3-031-08337-2_35</td>\n      <td>https://scholar.google.com/scholar?cites=2578766836719982342&amp;as_sdt=2005&amp;sciodt=2007&amp;hl=en</td>\n      <td>613</td>\n      <td>2023-05-31 10:24:35</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1.0</td>\n      <td>In this work, we demonstrate how a publicly available, pre-trained Jukebox model can be adapted for the problem of audio source separation from a single mixed audio channel. Our …</td>\n      <td>https://arxiv.org/pdf/2111.14200</td>\n      <td>https://scholar.google.com/scholar?q=related:BpdAbcaeySMJ:scholar.google.com/&amp;scioq=music+ai&amp;hl=en&amp;as_sdt=2007&amp;as_ylo=2022&amp;as_yhi=2023</td>\n    </tr>\n    <tr>\n      <th>965</th>\n      <td>0</td>\n      <td>A Ranjan, VNJ Behera, M Reza</td>\n      <td>Using a Bi-Directional Long Short-Term Memory Model with Attention Mechanism Trained on MIDI Data for Generating Unique Music</td>\n      <td>2022.0</td>\n      <td>Artificial Intelligence for Data Science in …</td>\n      <td>Springer</td>\n      <td>https://link.springer.com/chapter/10.1007/978-3-030-92245-0_10</td>\n      <td>NaN</td>\n      <td>204</td>\n      <td>2023-05-31 10:24:35</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1.0</td>\n      <td>… But this cannot similarly be done for music, as music is … music, the learning and generation aspect can make use of a high-level representation of the musical data, such as MIDI (Musical …</td>\n      <td>NaN</td>\n      <td>https://scholar.google.com/scholar?q=related:3jP4ulSdZVMJ:scholar.google.com/&amp;scioq=music+ai&amp;hl=en&amp;as_sdt=2007&amp;as_ylo=2022&amp;as_yhi=2023</td>\n    </tr>\n    <tr>\n      <th>974</th>\n      <td>0</td>\n      <td>C Limburg Stirum</td>\n      <td>What would you like to hear? Finding features for an artificial intelligence drive DJ</td>\n      <td>2022.0</td>\n      <td>NaN</td>\n      <td>essay.utwente.nl</td>\n      <td>http://essay.utwente.nl/92465/2/Public_Summary-Limburg-Sitrum-Sam-van.pdf</td>\n      <td>NaN</td>\n      <td>807</td>\n      <td>2023-05-31 10:24:35</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>… artificial intelligence (AI) driven DJ, get closer to a product they can launch onto the market. … The user could play music mixed by the AI as a party host, or join a room as a guest. All users …</td>\n      <td>http://essay.utwente.nl/92465/2/Public_Summary-Limburg-Sitrum-Sam-van.pdf</td>\n      <td>https://scholar.google.com/scholar?q=related:Zp7U_zTQ3dYJ:scholar.google.com/&amp;scioq=music+ai&amp;hl=en&amp;as_sdt=2007&amp;as_ylo=2022&amp;as_yhi=2023</td>\n    </tr>\n    <tr>\n      <th>977</th>\n      <td>0</td>\n      <td>Y Tie, T Wang, C Jin, X Li, P Yang</td>\n      <td>Wireless Communications and Mobile Computing Multitrack Music Generation Network Based on Music Rules</td>\n      <td>2022.0</td>\n      <td>Wireless Communications and …</td>\n      <td>hindawi.com</td>\n      <td>https://www.hindawi.com/journals/wcmc/2022/5845689/</td>\n      <td>NaN</td>\n      <td>146</td>\n      <td>2023-05-31 10:24:35</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>1.0</td>\n      <td>… human music was higher than AI (see Table 1). However, across all participants, our AI music … 7.93), indicating that the quality of our AI music creation was quite close to that of human …</td>\n      <td>https://www.hindawi.com/journals/wcmc/2022/5845689/</td>\n      <td>https://scholar.google.com/scholar?q=related:hF3XBL4cf0sJ:scholar.google.com/&amp;scioq=music+ai&amp;hl=en&amp;as_sdt=2007&amp;as_ylo=2022&amp;as_yhi=2023</td>\n    </tr>\n    <tr>\n      <th>985</th>\n      <td>0</td>\n      <td>V Efimova, V Shalamov, A Filchenkov</td>\n      <td>… : Generating Covers for Music and Books via Extracting Keywords: This paper presents two methods to generate high resolution uncopyrighted book covers or music …</td>\n      <td>2022.0</td>\n      <td>… on Artificial Intelligence and …</td>\n      <td>dl.acm.org</td>\n      <td>https://dl.acm.org/doi/abs/10.1145/3573942.3574088</td>\n      <td>NaN</td>\n      <td>820</td>\n      <td>2023-05-31 10:24:35</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1.0</td>\n      <td>… music album covers on their own using only text annotation, that will considerably simplify and speed up the process of the cover creation and turning a book or music … or musical album …</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>202 rows × 26 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"dfMusic2022and2023 = dfMusic2022and2023.dropna(subset=['Title', 'Authors'])\ndfMusic2022and2023","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:32.669730Z","iopub.execute_input":"2023-06-29T23:15:32.670253Z","iopub.status.idle":"2023-06-29T23:15:32.716058Z","shell.execute_reply.started":"2023-06-29T23:15:32.670210Z","shell.execute_reply":"2023-06-29T23:15:32.714613Z"},"trusted":true},"execution_count":77,"outputs":[{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"     Cites                                     Authors  \\\n0        0                                   U Kalidas   \n12      11    C Zhang, C Zhang, S Zheng, Y Qiao, C Li…   \n17       0                                      S Lowe   \n18       0                 H Wang, S Han, G Li, B Zhao   \n20       4         PS Yadav, S Khan, YV Singh, P Garg…   \n..     ...                                         ...   \n949      3  W Zai El Amri, O Tautz, H Ritter, A Melnik   \n965      0                A Ranjan, VNJ Behera, M Reza   \n974      0                            C Limburg Stirum   \n977      0          Y Tie, T Wang, C Jin, X Li, P Yang   \n985      0         V Efimova, V Shalamov, A Filchenkov   \n\n                                                                                                                                                                   Title  \\\n0                           \" 12\" Music Visualisation: Exploring Generative and Non-Generative techniques within a rhythmic framework in a performative spatial setting.   \n12                                                                               A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?   \n17                                                                                                 A Fully-Unsupervised Generative Method for Choreo-Musical Translation   \n18                                                                                              A Hybrid Neural Network for Music Generation Using Frequency Domain Data   \n20                                                                                   A Lightweight Deep Learning-Based Approach for Jazz Music Generation in MIDI Format   \n..                                                                                                                                                                   ...   \n949                                                                                                           Transfer learning with jukebox for music source separation   \n965                                        Using a Bi-Directional Long Short-Term Memory Model with Attention Mechanism Trained on MIDI Data for Generating Unique Music   \n974                                                                                What would you like to hear? Finding features for an artificial intelligence drive DJ   \n977                                                                Wireless Communications and Mobile Computing Multitrack Music Generation Network Based on Music Rules   \n985  … : Generating Covers for Music and Books via Extracting Keywords: This paper presents two methods to generate high resolution uncopyrighted book covers or music …   \n\n       Year                                         Source  \\\n0    2023.0                                            NaN   \n12   2023.0                         arXiv preprint arXiv …   \n17      NaN                                       slowe.io   \n18   2023.0   … Conference on Artificial Intelligence in …   \n20   2022.0                                Computational …   \n..      ...                                            ...   \n949  2022.0                      Artificial Intelligence …   \n965  2022.0  Artificial Intelligence for Data Science in …   \n974  2022.0                                            NaN   \n977  2022.0                  Wireless Communications and …   \n985  2022.0             … on Artificial Intelligence and …   \n\n                 Publisher  \\\n0    openresearch.ocadu.ca   \n12               arxiv.org   \n17                     NaN   \n18                Springer   \n20             hindawi.com   \n..                     ...   \n949               Springer   \n965               Springer   \n974       essay.utwente.nl   \n977            hindawi.com   \n985             dl.acm.org   \n\n                                                                    ArticleURL  \\\n0                                https://openresearch.ocadu.ca/id/eprint/4024/   \n12                                            https://arxiv.org/abs/2303.11717   \n17                                          https://slowe.io/content/cs229.pdf   \n18              https://link.springer.com/chapter/10.1007/978-981-99-1256-8_26   \n20                          https://www.hindawi.com/journals/cin/2022/2140895/   \n..                                                                         ...   \n949             https://link.springer.com/chapter/10.1007/978-3-031-08337-2_35   \n965             https://link.springer.com/chapter/10.1007/978-3-030-92245-0_10   \n974  http://essay.utwente.nl/92465/2/Public_Summary-Limburg-Sitrum-Sam-van.pdf   \n977                        https://www.hindawi.com/journals/wcmc/2022/5845689/   \n985                         https://dl.acm.org/doi/abs/10.1145/3573942.3574088   \n\n                                                                                        CitesURL  \\\n0                                                                                            NaN   \n12   https://scholar.google.com/scholar?cites=10162538139609691591&as_sdt=2005&sciodt=2007&hl=en   \n17                                                                                           NaN   \n18                                                                                           NaN   \n20    https://scholar.google.com/scholar?cites=6733953986624070978&as_sdt=2005&sciodt=2007&hl=en   \n..                                                                                           ...   \n949   https://scholar.google.com/scholar?cites=2578766836719982342&as_sdt=2005&sciodt=2007&hl=en   \n965                                                                                          NaN   \n974                                                                                          NaN   \n977                                                                                          NaN   \n985                                                                                          NaN   \n\n     GSRank            QueryDate  ... StartPage EndPage  ECC  CitesPerYear  \\\n0       796  2023-05-31 10:24:35  ...       NaN     NaN    0           0.0   \n12      340  2023-05-31 10:24:35  ...       NaN     NaN   11          11.0   \n17      762  2023-05-31 10:24:35  ...       NaN     NaN    0           0.0   \n18      816  2023-05-31 10:24:35  ...       NaN     NaN    0           0.0   \n20      292  2023-05-31 10:24:35  ...       NaN     NaN    4           4.0   \n..      ...                  ...  ...       ...     ...  ...           ...   \n949     613  2023-05-31 10:24:35  ...       NaN     NaN    3           3.0   \n965     204  2023-05-31 10:24:35  ...       NaN     NaN    0           0.0   \n974     807  2023-05-31 10:24:35  ...       NaN     NaN    0           0.0   \n977     146  2023-05-31 10:24:35  ...       NaN     NaN    0           0.0   \n985     820  2023-05-31 10:24:35  ...       NaN     NaN    0           0.0   \n\n     CitesPerAuthor  AuthorCount  Age  \\\n0                 0            1  1.0   \n12                2            6  1.0   \n17                0            1  NaN   \n18                0            4  1.0   \n20                1            5  1.0   \n..              ...          ...  ...   \n949               1            4  1.0   \n965               0            3  1.0   \n974               0            1  1.0   \n977               0            5  1.0   \n985               0            3  1.0   \n\n                                                                                                                                                                                                   Abstract  \\\n0                  … The two day workshop/ lec-dem involved participants working in groups to create Music on Ableton live Digital Audio Workstation, using AI based tools created by the TheKlong studio …   \n12          … new music, while some software edit compositions in the style of various composers. Music … it is because of using AI to compose music or to help musicians. A fantastic illustration of an …   \n17           … It is clear that AI systems applied to these tasks need similar … Our algorithm will be able to take as input either music (… that represents choreography and music that should be paired …   \n18                        … adversarial network to generate music in an end-to-end way. The automatic music generation method proposed in this paper explores a new representation of music data, and the …   \n20                 … In paper [15], jazz transformer-based AIcomposed music quantitative measures are discussed. is system measures the jazz transformation using the deep learning model. In paper [16], …   \n..                                                                                                                                                                                                      ...   \n949                     In this work, we demonstrate how a publicly available, pre-trained Jukebox model can be adapted for the problem of audio source separation from a single mixed audio channel. Our …   \n965         … But this cannot similarly be done for music, as music is … music, the learning and generation aspect can make use of a high-level representation of the musical data, such as MIDI (Musical …   \n974  … artificial intelligence (AI) driven DJ, get closer to a product they can launch onto the market. … The user could play music mixed by the AI as a party host, or join a room as a guest. All users …   \n977         … human music was higher than AI (see Table 1). However, across all participants, our AI music … 7.93), indicating that the quality of our AI music creation was quite close to that of human …   \n985           … music album covers on their own using only text annotation, that will considerably simplify and speed up the process of the cover creation and turning a book or music … or musical album …   \n\n                                                                                FullTextURL  \\\n0    https://openresearch.ocadu.ca/id/eprint/4024/1/Kalidas_Unnikrishnan_2023_MDES_DIGF.pdf   \n12                                                         https://arxiv.org/pdf/2303.11717   \n17                                                       https://slowe.io/content/cs229.pdf   \n18                                                                                      NaN   \n20                                       https://www.hindawi.com/journals/cin/2022/2140895/   \n..                                                                                      ...   \n949                                                        https://arxiv.org/pdf/2111.14200   \n965                                                                                     NaN   \n974               http://essay.utwente.nl/92465/2/Public_Summary-Limburg-Sitrum-Sam-van.pdf   \n977                                     https://www.hindawi.com/journals/wcmc/2022/5845689/   \n985                                                                                     NaN   \n\n                                                                                                                                 RelatedURL  \n0                                                                                                                                       NaN  \n12                                                                                                                                      NaN  \n17   https://scholar.google.com/scholar?q=related:gM4lmW8KfdoJ:scholar.google.com/&scioq=music+ai&hl=en&as_sdt=2007&as_ylo=2022&as_yhi=2023  \n18                                                                                                                                      NaN  \n20   https://scholar.google.com/scholar?q=related:QqWNCnnPc10J:scholar.google.com/&scioq=music+ai&hl=en&as_sdt=2007&as_ylo=2022&as_yhi=2023  \n..                                                                                                                                      ...  \n949  https://scholar.google.com/scholar?q=related:BpdAbcaeySMJ:scholar.google.com/&scioq=music+ai&hl=en&as_sdt=2007&as_ylo=2022&as_yhi=2023  \n965  https://scholar.google.com/scholar?q=related:3jP4ulSdZVMJ:scholar.google.com/&scioq=music+ai&hl=en&as_sdt=2007&as_ylo=2022&as_yhi=2023  \n974  https://scholar.google.com/scholar?q=related:Zp7U_zTQ3dYJ:scholar.google.com/&scioq=music+ai&hl=en&as_sdt=2007&as_ylo=2022&as_yhi=2023  \n977  https://scholar.google.com/scholar?q=related:hF3XBL4cf0sJ:scholar.google.com/&scioq=music+ai&hl=en&as_sdt=2007&as_ylo=2022&as_yhi=2023  \n985                                                                                                                                     NaN  \n\n[202 rows x 26 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cites</th>\n      <th>Authors</th>\n      <th>Title</th>\n      <th>Year</th>\n      <th>Source</th>\n      <th>Publisher</th>\n      <th>ArticleURL</th>\n      <th>CitesURL</th>\n      <th>GSRank</th>\n      <th>QueryDate</th>\n      <th>...</th>\n      <th>StartPage</th>\n      <th>EndPage</th>\n      <th>ECC</th>\n      <th>CitesPerYear</th>\n      <th>CitesPerAuthor</th>\n      <th>AuthorCount</th>\n      <th>Age</th>\n      <th>Abstract</th>\n      <th>FullTextURL</th>\n      <th>RelatedURL</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>U Kalidas</td>\n      <td>\" 12\" Music Visualisation: Exploring Generative and Non-Generative techniques within a rhythmic framework in a performative spatial setting.</td>\n      <td>2023.0</td>\n      <td>NaN</td>\n      <td>openresearch.ocadu.ca</td>\n      <td>https://openresearch.ocadu.ca/id/eprint/4024/</td>\n      <td>NaN</td>\n      <td>796</td>\n      <td>2023-05-31 10:24:35</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>… The two day workshop/ lec-dem involved participants working in groups to create Music on Ableton live Digital Audio Workstation, using AI based tools created by the TheKlong studio …</td>\n      <td>https://openresearch.ocadu.ca/id/eprint/4024/1/Kalidas_Unnikrishnan_2023_MDES_DIGF.pdf</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>11</td>\n      <td>C Zhang, C Zhang, S Zheng, Y Qiao, C Li…</td>\n      <td>A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?</td>\n      <td>2023.0</td>\n      <td>arXiv preprint arXiv …</td>\n      <td>arxiv.org</td>\n      <td>https://arxiv.org/abs/2303.11717</td>\n      <td>https://scholar.google.com/scholar?cites=10162538139609691591&amp;as_sdt=2005&amp;sciodt=2007&amp;hl=en</td>\n      <td>340</td>\n      <td>2023-05-31 10:24:35</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>11</td>\n      <td>11.0</td>\n      <td>2</td>\n      <td>6</td>\n      <td>1.0</td>\n      <td>… new music, while some software edit compositions in the style of various composers. Music … it is because of using AI to compose music or to help musicians. A fantastic illustration of an …</td>\n      <td>https://arxiv.org/pdf/2303.11717</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0</td>\n      <td>S Lowe</td>\n      <td>A Fully-Unsupervised Generative Method for Choreo-Musical Translation</td>\n      <td>NaN</td>\n      <td>slowe.io</td>\n      <td>NaN</td>\n      <td>https://slowe.io/content/cs229.pdf</td>\n      <td>NaN</td>\n      <td>762</td>\n      <td>2023-05-31 10:24:35</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>… It is clear that AI systems applied to these tasks need similar … Our algorithm will be able to take as input either music (… that represents choreography and music that should be paired …</td>\n      <td>https://slowe.io/content/cs229.pdf</td>\n      <td>https://scholar.google.com/scholar?q=related:gM4lmW8KfdoJ:scholar.google.com/&amp;scioq=music+ai&amp;hl=en&amp;as_sdt=2007&amp;as_ylo=2022&amp;as_yhi=2023</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0</td>\n      <td>H Wang, S Han, G Li, B Zhao</td>\n      <td>A Hybrid Neural Network for Music Generation Using Frequency Domain Data</td>\n      <td>2023.0</td>\n      <td>… Conference on Artificial Intelligence in …</td>\n      <td>Springer</td>\n      <td>https://link.springer.com/chapter/10.1007/978-981-99-1256-8_26</td>\n      <td>NaN</td>\n      <td>816</td>\n      <td>2023-05-31 10:24:35</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1.0</td>\n      <td>… adversarial network to generate music in an end-to-end way. The automatic music generation method proposed in this paper explores a new representation of music data, and the …</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>4</td>\n      <td>PS Yadav, S Khan, YV Singh, P Garg…</td>\n      <td>A Lightweight Deep Learning-Based Approach for Jazz Music Generation in MIDI Format</td>\n      <td>2022.0</td>\n      <td>Computational …</td>\n      <td>hindawi.com</td>\n      <td>https://www.hindawi.com/journals/cin/2022/2140895/</td>\n      <td>https://scholar.google.com/scholar?cites=6733953986624070978&amp;as_sdt=2005&amp;sciodt=2007&amp;hl=en</td>\n      <td>292</td>\n      <td>2023-05-31 10:24:35</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>4.0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>1.0</td>\n      <td>… In paper [15], jazz transformer-based AIcomposed music quantitative measures are discussed. is system measures the jazz transformation using the deep learning model. In paper [16], …</td>\n      <td>https://www.hindawi.com/journals/cin/2022/2140895/</td>\n      <td>https://scholar.google.com/scholar?q=related:QqWNCnnPc10J:scholar.google.com/&amp;scioq=music+ai&amp;hl=en&amp;as_sdt=2007&amp;as_ylo=2022&amp;as_yhi=2023</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>949</th>\n      <td>3</td>\n      <td>W Zai El Amri, O Tautz, H Ritter, A Melnik</td>\n      <td>Transfer learning with jukebox for music source separation</td>\n      <td>2022.0</td>\n      <td>Artificial Intelligence …</td>\n      <td>Springer</td>\n      <td>https://link.springer.com/chapter/10.1007/978-3-031-08337-2_35</td>\n      <td>https://scholar.google.com/scholar?cites=2578766836719982342&amp;as_sdt=2005&amp;sciodt=2007&amp;hl=en</td>\n      <td>613</td>\n      <td>2023-05-31 10:24:35</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1.0</td>\n      <td>In this work, we demonstrate how a publicly available, pre-trained Jukebox model can be adapted for the problem of audio source separation from a single mixed audio channel. Our …</td>\n      <td>https://arxiv.org/pdf/2111.14200</td>\n      <td>https://scholar.google.com/scholar?q=related:BpdAbcaeySMJ:scholar.google.com/&amp;scioq=music+ai&amp;hl=en&amp;as_sdt=2007&amp;as_ylo=2022&amp;as_yhi=2023</td>\n    </tr>\n    <tr>\n      <th>965</th>\n      <td>0</td>\n      <td>A Ranjan, VNJ Behera, M Reza</td>\n      <td>Using a Bi-Directional Long Short-Term Memory Model with Attention Mechanism Trained on MIDI Data for Generating Unique Music</td>\n      <td>2022.0</td>\n      <td>Artificial Intelligence for Data Science in …</td>\n      <td>Springer</td>\n      <td>https://link.springer.com/chapter/10.1007/978-3-030-92245-0_10</td>\n      <td>NaN</td>\n      <td>204</td>\n      <td>2023-05-31 10:24:35</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1.0</td>\n      <td>… But this cannot similarly be done for music, as music is … music, the learning and generation aspect can make use of a high-level representation of the musical data, such as MIDI (Musical …</td>\n      <td>NaN</td>\n      <td>https://scholar.google.com/scholar?q=related:3jP4ulSdZVMJ:scholar.google.com/&amp;scioq=music+ai&amp;hl=en&amp;as_sdt=2007&amp;as_ylo=2022&amp;as_yhi=2023</td>\n    </tr>\n    <tr>\n      <th>974</th>\n      <td>0</td>\n      <td>C Limburg Stirum</td>\n      <td>What would you like to hear? Finding features for an artificial intelligence drive DJ</td>\n      <td>2022.0</td>\n      <td>NaN</td>\n      <td>essay.utwente.nl</td>\n      <td>http://essay.utwente.nl/92465/2/Public_Summary-Limburg-Sitrum-Sam-van.pdf</td>\n      <td>NaN</td>\n      <td>807</td>\n      <td>2023-05-31 10:24:35</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>… artificial intelligence (AI) driven DJ, get closer to a product they can launch onto the market. … The user could play music mixed by the AI as a party host, or join a room as a guest. All users …</td>\n      <td>http://essay.utwente.nl/92465/2/Public_Summary-Limburg-Sitrum-Sam-van.pdf</td>\n      <td>https://scholar.google.com/scholar?q=related:Zp7U_zTQ3dYJ:scholar.google.com/&amp;scioq=music+ai&amp;hl=en&amp;as_sdt=2007&amp;as_ylo=2022&amp;as_yhi=2023</td>\n    </tr>\n    <tr>\n      <th>977</th>\n      <td>0</td>\n      <td>Y Tie, T Wang, C Jin, X Li, P Yang</td>\n      <td>Wireless Communications and Mobile Computing Multitrack Music Generation Network Based on Music Rules</td>\n      <td>2022.0</td>\n      <td>Wireless Communications and …</td>\n      <td>hindawi.com</td>\n      <td>https://www.hindawi.com/journals/wcmc/2022/5845689/</td>\n      <td>NaN</td>\n      <td>146</td>\n      <td>2023-05-31 10:24:35</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>1.0</td>\n      <td>… human music was higher than AI (see Table 1). However, across all participants, our AI music … 7.93), indicating that the quality of our AI music creation was quite close to that of human …</td>\n      <td>https://www.hindawi.com/journals/wcmc/2022/5845689/</td>\n      <td>https://scholar.google.com/scholar?q=related:hF3XBL4cf0sJ:scholar.google.com/&amp;scioq=music+ai&amp;hl=en&amp;as_sdt=2007&amp;as_ylo=2022&amp;as_yhi=2023</td>\n    </tr>\n    <tr>\n      <th>985</th>\n      <td>0</td>\n      <td>V Efimova, V Shalamov, A Filchenkov</td>\n      <td>… : Generating Covers for Music and Books via Extracting Keywords: This paper presents two methods to generate high resolution uncopyrighted book covers or music …</td>\n      <td>2022.0</td>\n      <td>… on Artificial Intelligence and …</td>\n      <td>dl.acm.org</td>\n      <td>https://dl.acm.org/doi/abs/10.1145/3573942.3574088</td>\n      <td>NaN</td>\n      <td>820</td>\n      <td>2023-05-31 10:24:35</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1.0</td>\n      <td>… music album covers on their own using only text annotation, that will considerably simplify and speed up the process of the cover creation and turning a book or music … or musical album …</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>202 rows × 26 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"dfMusic2022and2023.columns","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:32.718138Z","iopub.execute_input":"2023-06-29T23:15:32.718550Z","iopub.status.idle":"2023-06-29T23:15:32.725949Z","shell.execute_reply.started":"2023-06-29T23:15:32.718521Z","shell.execute_reply":"2023-06-29T23:15:32.724971Z"},"trusted":true},"execution_count":78,"outputs":[{"execution_count":78,"output_type":"execute_result","data":{"text/plain":"Index(['Cites', 'Authors', 'Title', 'Year', 'Source', 'Publisher',\n       'ArticleURL', 'CitesURL', 'GSRank', 'QueryDate', 'Type', 'DOI', 'ISSN',\n       'CitationURL', 'Volume', 'Issue', 'StartPage', 'EndPage', 'ECC',\n       'CitesPerYear', 'CitesPerAuthor', 'AuthorCount', 'Age', 'Abstract',\n       'FullTextURL', 'RelatedURL'],\n      dtype='object')"},"metadata":{}}]},{"cell_type":"code","source":"dfMusic2022and2023 = pd.DataFrame({\n    'Authors': dfMusic2022and2023['Authors'],\n    'Title': dfMusic2022and2023['Title'],\n    'Abstract': dfMusic2022and2023['Abstract'],\n    'Year': dfMusic2022and2023['Year']\n})","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:32.727465Z","iopub.execute_input":"2023-06-29T23:15:32.728211Z","iopub.status.idle":"2023-06-29T23:15:32.741796Z","shell.execute_reply.started":"2023-06-29T23:15:32.728178Z","shell.execute_reply":"2023-06-29T23:15:32.740468Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"dfMusic2022and2023.shape","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:32.746244Z","iopub.execute_input":"2023-06-29T23:15:32.746654Z","iopub.status.idle":"2023-06-29T23:15:32.758604Z","shell.execute_reply.started":"2023-06-29T23:15:32.746615Z","shell.execute_reply":"2023-06-29T23:15:32.756819Z"},"trusted":true},"execution_count":80,"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"(202, 4)"},"metadata":{}}]},{"cell_type":"code","source":"dfMusic2022and2023.dtypes","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:32.760492Z","iopub.execute_input":"2023-06-29T23:15:32.760964Z","iopub.status.idle":"2023-06-29T23:15:32.776568Z","shell.execute_reply.started":"2023-06-29T23:15:32.760923Z","shell.execute_reply":"2023-06-29T23:15:32.775276Z"},"trusted":true},"execution_count":81,"outputs":[{"execution_count":81,"output_type":"execute_result","data":{"text/plain":"Authors      object\nTitle        object\nAbstract     object\nYear        float64\ndtype: object"},"metadata":{}}]},{"cell_type":"code","source":"dfMusic2022and2023.Year.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:32.778341Z","iopub.execute_input":"2023-06-29T23:15:32.778766Z","iopub.status.idle":"2023-06-29T23:15:32.795593Z","shell.execute_reply.started":"2023-06-29T23:15:32.778735Z","shell.execute_reply":"2023-06-29T23:15:32.794131Z"},"trusted":true},"execution_count":82,"outputs":[{"execution_count":82,"output_type":"execute_result","data":{"text/plain":"2022.0    145\n2023.0     45\nName: Year, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# dfMusic2022and2023 = dfMusic2022and2023['Year'].astype(int) if None\ndfMusic2022and2023['Year'] = dfMusic2022and2023['Year'].fillna(0).astype(int)\n\ndfMusic2022and2023.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:32.797549Z","iopub.execute_input":"2023-06-29T23:15:32.797909Z","iopub.status.idle":"2023-06-29T23:15:32.818655Z","shell.execute_reply.started":"2023-06-29T23:15:32.797879Z","shell.execute_reply":"2023-06-29T23:15:32.817253Z"},"trusted":true},"execution_count":83,"outputs":[{"execution_count":83,"output_type":"execute_result","data":{"text/plain":"                                     Authors  \\\n0                                  U Kalidas   \n12  C Zhang, C Zhang, S Zheng, Y Qiao, C Li…   \n17                                    S Lowe   \n18               H Wang, S Han, G Li, B Zhao   \n20       PS Yadav, S Khan, YV Singh, P Garg…   \n\n                                                                                                                                           Title  \\\n0   \" 12\" Music Visualisation: Exploring Generative and Non-Generative techniques within a rhythmic framework in a performative spatial setting.   \n12                                                       A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?   \n17                                                                         A Fully-Unsupervised Generative Method for Choreo-Musical Translation   \n18                                                                      A Hybrid Neural Network for Music Generation Using Frequency Domain Data   \n20                                                           A Lightweight Deep Learning-Based Approach for Jazz Music Generation in MIDI Format   \n\n                                                                                                                                                                                           Abstract  \\\n0          … The two day workshop/ lec-dem involved participants working in groups to create Music on Ableton live Digital Audio Workstation, using AI based tools created by the TheKlong studio …   \n12  … new music, while some software edit compositions in the style of various composers. Music … it is because of using AI to compose music or to help musicians. A fantastic illustration of an …   \n17   … It is clear that AI systems applied to these tasks need similar … Our algorithm will be able to take as input either music (… that represents choreography and music that should be paired …   \n18                … adversarial network to generate music in an end-to-end way. The automatic music generation method proposed in this paper explores a new representation of music data, and the …   \n20         … In paper [15], jazz transformer-based AIcomposed music quantitative measures are discussed. is system measures the jazz transformation using the deep learning model. In paper [16], …   \n\n    Year  \n0   2023  \n12  2023  \n17     0  \n18  2023  \n20  2022  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Authors</th>\n      <th>Title</th>\n      <th>Abstract</th>\n      <th>Year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>U Kalidas</td>\n      <td>\" 12\" Music Visualisation: Exploring Generative and Non-Generative techniques within a rhythmic framework in a performative spatial setting.</td>\n      <td>… The two day workshop/ lec-dem involved participants working in groups to create Music on Ableton live Digital Audio Workstation, using AI based tools created by the TheKlong studio …</td>\n      <td>2023</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>C Zhang, C Zhang, S Zheng, Y Qiao, C Li…</td>\n      <td>A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?</td>\n      <td>… new music, while some software edit compositions in the style of various composers. Music … it is because of using AI to compose music or to help musicians. A fantastic illustration of an …</td>\n      <td>2023</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>S Lowe</td>\n      <td>A Fully-Unsupervised Generative Method for Choreo-Musical Translation</td>\n      <td>… It is clear that AI systems applied to these tasks need similar … Our algorithm will be able to take as input either music (… that represents choreography and music that should be paired …</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>H Wang, S Han, G Li, B Zhao</td>\n      <td>A Hybrid Neural Network for Music Generation Using Frequency Domain Data</td>\n      <td>… adversarial network to generate music in an end-to-end way. The automatic music generation method proposed in this paper explores a new representation of music data, and the …</td>\n      <td>2023</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>PS Yadav, S Khan, YV Singh, P Garg…</td>\n      <td>A Lightweight Deep Learning-Based Approach for Jazz Music Generation in MIDI Format</td>\n      <td>… In paper [15], jazz transformer-based AIcomposed music quantitative measures are discussed. is system measures the jazz transformation using the deep learning model. In paper [16], …</td>\n      <td>2022</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"dfMusic2022and2023.dtypes","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:32.820340Z","iopub.execute_input":"2023-06-29T23:15:32.820809Z","iopub.status.idle":"2023-06-29T23:15:32.835427Z","shell.execute_reply.started":"2023-06-29T23:15:32.820756Z","shell.execute_reply":"2023-06-29T23:15:32.834095Z"},"trusted":true},"execution_count":84,"outputs":[{"execution_count":84,"output_type":"execute_result","data":{"text/plain":"Authors     object\nTitle       object\nAbstract    object\nYear         int64\ndtype: object"},"metadata":{}}]},{"cell_type":"code","source":"dfMusic2022and2023.shape","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:32.837287Z","iopub.execute_input":"2023-06-29T23:15:32.838078Z","iopub.status.idle":"2023-06-29T23:15:32.853374Z","shell.execute_reply.started":"2023-06-29T23:15:32.838042Z","shell.execute_reply":"2023-06-29T23:15:32.852091Z"},"trusted":true},"execution_count":85,"outputs":[{"execution_count":85,"output_type":"execute_result","data":{"text/plain":"(202, 4)"},"metadata":{}}]},{"cell_type":"code","source":"#Music1000\ndfONE = dfMusic2022and2023.reset_index(drop=True)\n\ndfONE.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:32.855296Z","iopub.execute_input":"2023-06-29T23:15:32.855717Z","iopub.status.idle":"2023-06-29T23:15:32.879756Z","shell.execute_reply.started":"2023-06-29T23:15:32.855679Z","shell.execute_reply":"2023-06-29T23:15:32.878523Z"},"trusted":true},"execution_count":86,"outputs":[{"execution_count":86,"output_type":"execute_result","data":{"text/plain":"                                    Authors  \\\n0                                 U Kalidas   \n1  C Zhang, C Zhang, S Zheng, Y Qiao, C Li…   \n2                                    S Lowe   \n3               H Wang, S Han, G Li, B Zhao   \n4       PS Yadav, S Khan, YV Singh, P Garg…   \n\n                                                                                                                                          Title  \\\n0  \" 12\" Music Visualisation: Exploring Generative and Non-Generative techniques within a rhythmic framework in a performative spatial setting.   \n1                                                       A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?   \n2                                                                         A Fully-Unsupervised Generative Method for Choreo-Musical Translation   \n3                                                                      A Hybrid Neural Network for Music Generation Using Frequency Domain Data   \n4                                                           A Lightweight Deep Learning-Based Approach for Jazz Music Generation in MIDI Format   \n\n                                                                                                                                                                                          Abstract  \\\n0         … The two day workshop/ lec-dem involved participants working in groups to create Music on Ableton live Digital Audio Workstation, using AI based tools created by the TheKlong studio …   \n1  … new music, while some software edit compositions in the style of various composers. Music … it is because of using AI to compose music or to help musicians. A fantastic illustration of an …   \n2   … It is clear that AI systems applied to these tasks need similar … Our algorithm will be able to take as input either music (… that represents choreography and music that should be paired …   \n3                … adversarial network to generate music in an end-to-end way. The automatic music generation method proposed in this paper explores a new representation of music data, and the …   \n4         … In paper [15], jazz transformer-based AIcomposed music quantitative measures are discussed. is system measures the jazz transformation using the deep learning model. In paper [16], …   \n\n   Year  \n0  2023  \n1  2023  \n2     0  \n3  2023  \n4  2022  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Authors</th>\n      <th>Title</th>\n      <th>Abstract</th>\n      <th>Year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>U Kalidas</td>\n      <td>\" 12\" Music Visualisation: Exploring Generative and Non-Generative techniques within a rhythmic framework in a performative spatial setting.</td>\n      <td>… The two day workshop/ lec-dem involved participants working in groups to create Music on Ableton live Digital Audio Workstation, using AI based tools created by the TheKlong studio …</td>\n      <td>2023</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>C Zhang, C Zhang, S Zheng, Y Qiao, C Li…</td>\n      <td>A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?</td>\n      <td>… new music, while some software edit compositions in the style of various composers. Music … it is because of using AI to compose music or to help musicians. A fantastic illustration of an …</td>\n      <td>2023</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>S Lowe</td>\n      <td>A Fully-Unsupervised Generative Method for Choreo-Musical Translation</td>\n      <td>… It is clear that AI systems applied to these tasks need similar … Our algorithm will be able to take as input either music (… that represents choreography and music that should be paired …</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>H Wang, S Han, G Li, B Zhao</td>\n      <td>A Hybrid Neural Network for Music Generation Using Frequency Domain Data</td>\n      <td>… adversarial network to generate music in an end-to-end way. The automatic music generation method proposed in this paper explores a new representation of music data, and the …</td>\n      <td>2023</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>PS Yadav, S Khan, YV Singh, P Garg…</td>\n      <td>A Lightweight Deep Learning-Based Approach for Jazz Music Generation in MIDI Format</td>\n      <td>… In paper [15], jazz transformer-based AIcomposed music quantitative measures are discussed. is system measures the jazz transformation using the deep learning model. In paper [16], …</td>\n      <td>2022</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#XRIV dataset\ndfTWO = df_Music_Compose_data.reset_index(drop=True)\ndfTWO.head() ","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:32.890202Z","iopub.execute_input":"2023-06-29T23:15:32.891592Z","iopub.status.idle":"2023-06-29T23:15:32.907619Z","shell.execute_reply.started":"2023-06-29T23:15:32.891548Z","shell.execute_reply":"2023-06-29T23:15:32.906413Z"},"trusted":true},"execution_count":87,"outputs":[{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"                                                                                       Authors  \\\n0                                                           Erica Bisesi, and Marisa Michelini   \n1  Fabien Gallot, Owen Lagadec, Myriam Desainte-Catherine (LaBRI),\\n  Sylvain Marchand (LaBRI)   \n2                                                                    J.R. Dawin, D. Volchenkov   \n3                                     Georg Boenn, Martin Brain, Marina De Vos and John ffitch   \n4                             Avishek Ghosh, Joydeep Banerjee, Sk. S. Hassan, P. Pal Choudhury   \n\n                                                                                         Title  \\\n0  Planning Curricular Proposals on Sound and Music with Prospective  SecondarySchool Teachers   \n1                                        iKlax a New Musical Audio Format for Active Listening   \n2                                                  Markov Chain Analysis of Musical Dice Games   \n3                                     Automatic Music Composition using Answer Set Programming   \n4                           Fractal String Generation and Its Application in Music Composition   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Abstract  \\\n0                                                                                                                                                                                                                                                                                     Sound is a preferred context to build foundations on wave phenomena one ofthe most important disciplinary referents in physics It is also one of thebestset frameworks to achieve transversality overcoming scholastic level andactivating emotional aspects which are naturally connected with every day lifeas well as with music and perception Looking at sound and music by atransversal perspective  a borderline approach between science and art isthe adopted statement for a teaching proposal using metacognition as astrategy in scientific education This work analyzes curricular proposals onmusical acoustics planned by prospective secondaryschool teachers in theframework of a Formative Intervention Module answering the expectation ofmaking more effective teaching scientific subjects by improving creativecapabilities as well as leading to build logical and scientificcategorizations able to consciously discipline artistic activity in musicstudents With this aim a particular emphasis is given to those concepts like sound parameters and structural elements of a musical piece which arebest fitted to be addressed on a transversal perspective involvingsimultaneously physics psychophysics and music   \n1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        In this paper we are presenting a new model for interactive music Unlikemost interactive systems our model is based on file organization but does notrequire digital audio treatments This model includes a definition of aconstraints system and its solver The products of this project are intendedfor the general public inexperienced users as well as professional musiciansand will be distributed commercially We are here presenting three products ofthis project The difficulty of this project is to design a technology andsoftware products for interactive music which must be easy to use by thegeneral public and by professional composers   \n2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      We have studied entropy redundancy complexity and first passage times tonotes for  pieces of  composers The successful understanding of tonalmusic calls for an experienced listener as entropy dominates over redundancyin musical messages First passage times to notes resolve tonality and featurea composer We also discuss the possible distances in space of musical dicegames and introduced the geodesic distance based on the Riemann structureassociated to the probability vectors rows of the transition matrices   \n3  Music composition used to be a pen and paper activity These these days musicis often composed with the aid of computer software even to the point wherethe computer compose parts of the score autonomously The composition of moststyles of music is governed by rules We show that by approaching theautomation analysis and verification of composition as a knowledgerepresentation task and formalising these rules in a suitable logical languagepowerful and expressive intelligent composition tools can be easily built Thisapplication paper describes the use of answer set programming to construct anautomated system named ANTON that can compose melodic harmonic and rhythmicmusic diagnose errors in human compositions and serve as a computeraidedcomposition tool The combination of harmonic rhythmic and melodic compositionin a single framework makes ANTON unique in the growing area of algorithmiccomposition With near realtime composition ANTON reaches the point where itcan not only be used as a component in an interactive composition tool but alsohas the potential for live performances and concerts or automatically generatedbackground music in a variety of applications With the use of a fullydeclarative language and an offtheshelf reasoning engine ANTON providesthe human composer a tool which is significantly simpler more compact and moreversatile than other existing systems This paper has been accepted forpublication in Theory and Practice of Logic Programming TPLP   \n4                                                                                                                                                                                                                                                                                                                                                                                     Music is a string of some of the notes out of  notes Sa Komalre ReKomalga Ga Ma Karima Pa Komaldha Dha Komalni Ni and theirharmonics Each note corresponds to a particular frequency When such stringsare encoded to form discrete sequences different frequencies present in themusic corresponds to different amplitude levels value of the discretesequence Initially a class of discrete sequences has been generated usinglogistic map All these discrete sequences have at most ndifferent amplitudelevels value depending on the particular raga Without loss of generalitywe have chosen two discrete sequences of two types of Indian raga viz Bhairabiand Bhupali having same number of amplitude levels to obtainsearch closerelatives from the class The relative  closeness can be assured throughcorrelation coefficientThe search is unbiased random and nonadaptive Theobtained string is that which maximally resembles the given two sequences Thesame can be thought of as a music composition of the given two strings It isto be noted that all these string are fractal string which can be persuaded byfractal dimension   \n\n   Year  \n0  2008  \n1  2009  \n2  2010  \n3  2010  \n4  2011  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Authors</th>\n      <th>Title</th>\n      <th>Abstract</th>\n      <th>Year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Erica Bisesi, and Marisa Michelini</td>\n      <td>Planning Curricular Proposals on Sound and Music with Prospective  SecondarySchool Teachers</td>\n      <td>Sound is a preferred context to build foundations on wave phenomena one ofthe most important disciplinary referents in physics It is also one of thebestset frameworks to achieve transversality overcoming scholastic level andactivating emotional aspects which are naturally connected with every day lifeas well as with music and perception Looking at sound and music by atransversal perspective  a borderline approach between science and art isthe adopted statement for a teaching proposal using metacognition as astrategy in scientific education This work analyzes curricular proposals onmusical acoustics planned by prospective secondaryschool teachers in theframework of a Formative Intervention Module answering the expectation ofmaking more effective teaching scientific subjects by improving creativecapabilities as well as leading to build logical and scientificcategorizations able to consciously discipline artistic activity in musicstudents With this aim a particular emphasis is given to those concepts like sound parameters and structural elements of a musical piece which arebest fitted to be addressed on a transversal perspective involvingsimultaneously physics psychophysics and music</td>\n      <td>2008</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Fabien Gallot, Owen Lagadec, Myriam Desainte-Catherine (LaBRI),\\n  Sylvain Marchand (LaBRI)</td>\n      <td>iKlax a New Musical Audio Format for Active Listening</td>\n      <td>In this paper we are presenting a new model for interactive music Unlikemost interactive systems our model is based on file organization but does notrequire digital audio treatments This model includes a definition of aconstraints system and its solver The products of this project are intendedfor the general public inexperienced users as well as professional musiciansand will be distributed commercially We are here presenting three products ofthis project The difficulty of this project is to design a technology andsoftware products for interactive music which must be easy to use by thegeneral public and by professional composers</td>\n      <td>2009</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>J.R. Dawin, D. Volchenkov</td>\n      <td>Markov Chain Analysis of Musical Dice Games</td>\n      <td>We have studied entropy redundancy complexity and first passage times tonotes for  pieces of  composers The successful understanding of tonalmusic calls for an experienced listener as entropy dominates over redundancyin musical messages First passage times to notes resolve tonality and featurea composer We also discuss the possible distances in space of musical dicegames and introduced the geodesic distance based on the Riemann structureassociated to the probability vectors rows of the transition matrices</td>\n      <td>2010</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Georg Boenn, Martin Brain, Marina De Vos and John ffitch</td>\n      <td>Automatic Music Composition using Answer Set Programming</td>\n      <td>Music composition used to be a pen and paper activity These these days musicis often composed with the aid of computer software even to the point wherethe computer compose parts of the score autonomously The composition of moststyles of music is governed by rules We show that by approaching theautomation analysis and verification of composition as a knowledgerepresentation task and formalising these rules in a suitable logical languagepowerful and expressive intelligent composition tools can be easily built Thisapplication paper describes the use of answer set programming to construct anautomated system named ANTON that can compose melodic harmonic and rhythmicmusic diagnose errors in human compositions and serve as a computeraidedcomposition tool The combination of harmonic rhythmic and melodic compositionin a single framework makes ANTON unique in the growing area of algorithmiccomposition With near realtime composition ANTON reaches the point where itcan not only be used as a component in an interactive composition tool but alsohas the potential for live performances and concerts or automatically generatedbackground music in a variety of applications With the use of a fullydeclarative language and an offtheshelf reasoning engine ANTON providesthe human composer a tool which is significantly simpler more compact and moreversatile than other existing systems This paper has been accepted forpublication in Theory and Practice of Logic Programming TPLP</td>\n      <td>2010</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Avishek Ghosh, Joydeep Banerjee, Sk. S. Hassan, P. Pal Choudhury</td>\n      <td>Fractal String Generation and Its Application in Music Composition</td>\n      <td>Music is a string of some of the notes out of  notes Sa Komalre ReKomalga Ga Ma Karima Pa Komaldha Dha Komalni Ni and theirharmonics Each note corresponds to a particular frequency When such stringsare encoded to form discrete sequences different frequencies present in themusic corresponds to different amplitude levels value of the discretesequence Initially a class of discrete sequences has been generated usinglogistic map All these discrete sequences have at most ndifferent amplitudelevels value depending on the particular raga Without loss of generalitywe have chosen two discrete sequences of two types of Indian raga viz Bhairabiand Bhupali having same number of amplitude levels to obtainsearch closerelatives from the class The relative  closeness can be assured throughcorrelation coefficientThe search is unbiased random and nonadaptive Theobtained string is that which maximally resembles the given two sequences Thesame can be thought of as a music composition of the given two strings It isto be noted that all these string are fractal string which can be persuaded byfractal dimension</td>\n      <td>2011</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"dfTWO['Year'] = dfTWO['Year'].fillna(0).astype(int)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:32.909571Z","iopub.execute_input":"2023-06-29T23:15:32.910345Z","iopub.status.idle":"2023-06-29T23:15:32.924375Z","shell.execute_reply.started":"2023-06-29T23:15:32.910298Z","shell.execute_reply":"2023-06-29T23:15:32.922926Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"dfTWO.dtypes","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:32.927031Z","iopub.execute_input":"2023-06-29T23:15:32.927581Z","iopub.status.idle":"2023-06-29T23:15:32.944782Z","shell.execute_reply.started":"2023-06-29T23:15:32.927536Z","shell.execute_reply":"2023-06-29T23:15:32.943348Z"},"trusted":true},"execution_count":89,"outputs":[{"execution_count":89,"output_type":"execute_result","data":{"text/plain":"Authors     object\nTitle       object\nAbstract    object\nYear         int64\ndtype: object"},"metadata":{}}]},{"cell_type":"code","source":"dfONE['Dataset'] = 'Music1000'\ndfTWO['Dataset'] = 'ArXiV'","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:32.946496Z","iopub.execute_input":"2023-06-29T23:15:32.946860Z","iopub.status.idle":"2023-06-29T23:15:32.960355Z","shell.execute_reply.started":"2023-06-29T23:15:32.946830Z","shell.execute_reply":"2023-06-29T23:15:32.959256Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"dfTWO['Year'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:32.962187Z","iopub.execute_input":"2023-06-29T23:15:32.963656Z","iopub.status.idle":"2023-06-29T23:15:32.979323Z","shell.execute_reply.started":"2023-06-29T23:15:32.963612Z","shell.execute_reply":"2023-06-29T23:15:32.978083Z"},"trusted":true},"execution_count":91,"outputs":[{"execution_count":91,"output_type":"execute_result","data":{"text/plain":"2022    74\n2021    69\n2020    57\n2019    35\n2017    32\n2018    32\n2023    28\n2016    16\n2015     6\n2014     4\n2012     3\n2013     3\n2010     2\n2008     1\n2009     1\n2011     1\n2002     1\nName: Year, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"dfONE.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:32.981427Z","iopub.execute_input":"2023-06-29T23:15:32.982934Z","iopub.status.idle":"2023-06-29T23:15:33.002231Z","shell.execute_reply.started":"2023-06-29T23:15:32.982887Z","shell.execute_reply":"2023-06-29T23:15:33.000518Z"},"trusted":true},"execution_count":92,"outputs":[{"execution_count":92,"output_type":"execute_result","data":{"text/plain":"                                    Authors  \\\n0                                 U Kalidas   \n1  C Zhang, C Zhang, S Zheng, Y Qiao, C Li…   \n2                                    S Lowe   \n3               H Wang, S Han, G Li, B Zhao   \n4       PS Yadav, S Khan, YV Singh, P Garg…   \n\n                                                                                                                                          Title  \\\n0  \" 12\" Music Visualisation: Exploring Generative and Non-Generative techniques within a rhythmic framework in a performative spatial setting.   \n1                                                       A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?   \n2                                                                         A Fully-Unsupervised Generative Method for Choreo-Musical Translation   \n3                                                                      A Hybrid Neural Network for Music Generation Using Frequency Domain Data   \n4                                                           A Lightweight Deep Learning-Based Approach for Jazz Music Generation in MIDI Format   \n\n                                                                                                                                                                                          Abstract  \\\n0         … The two day workshop/ lec-dem involved participants working in groups to create Music on Ableton live Digital Audio Workstation, using AI based tools created by the TheKlong studio …   \n1  … new music, while some software edit compositions in the style of various composers. Music … it is because of using AI to compose music or to help musicians. A fantastic illustration of an …   \n2   … It is clear that AI systems applied to these tasks need similar … Our algorithm will be able to take as input either music (… that represents choreography and music that should be paired …   \n3                … adversarial network to generate music in an end-to-end way. The automatic music generation method proposed in this paper explores a new representation of music data, and the …   \n4         … In paper [15], jazz transformer-based AIcomposed music quantitative measures are discussed. is system measures the jazz transformation using the deep learning model. In paper [16], …   \n\n   Year    Dataset  \n0  2023  Music1000  \n1  2023  Music1000  \n2     0  Music1000  \n3  2023  Music1000  \n4  2022  Music1000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Authors</th>\n      <th>Title</th>\n      <th>Abstract</th>\n      <th>Year</th>\n      <th>Dataset</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>U Kalidas</td>\n      <td>\" 12\" Music Visualisation: Exploring Generative and Non-Generative techniques within a rhythmic framework in a performative spatial setting.</td>\n      <td>… The two day workshop/ lec-dem involved participants working in groups to create Music on Ableton live Digital Audio Workstation, using AI based tools created by the TheKlong studio …</td>\n      <td>2023</td>\n      <td>Music1000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>C Zhang, C Zhang, S Zheng, Y Qiao, C Li…</td>\n      <td>A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?</td>\n      <td>… new music, while some software edit compositions in the style of various composers. Music … it is because of using AI to compose music or to help musicians. A fantastic illustration of an …</td>\n      <td>2023</td>\n      <td>Music1000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>S Lowe</td>\n      <td>A Fully-Unsupervised Generative Method for Choreo-Musical Translation</td>\n      <td>… It is clear that AI systems applied to these tasks need similar … Our algorithm will be able to take as input either music (… that represents choreography and music that should be paired …</td>\n      <td>0</td>\n      <td>Music1000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>H Wang, S Han, G Li, B Zhao</td>\n      <td>A Hybrid Neural Network for Music Generation Using Frequency Domain Data</td>\n      <td>… adversarial network to generate music in an end-to-end way. The automatic music generation method proposed in this paper explores a new representation of music data, and the …</td>\n      <td>2023</td>\n      <td>Music1000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>PS Yadav, S Khan, YV Singh, P Garg…</td>\n      <td>A Lightweight Deep Learning-Based Approach for Jazz Music Generation in MIDI Format</td>\n      <td>… In paper [15], jazz transformer-based AIcomposed music quantitative measures are discussed. is system measures the jazz transformation using the deep learning model. In paper [16], …</td>\n      <td>2022</td>\n      <td>Music1000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"dfTWO.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:33.004111Z","iopub.execute_input":"2023-06-29T23:15:33.004506Z","iopub.status.idle":"2023-06-29T23:15:33.019854Z","shell.execute_reply.started":"2023-06-29T23:15:33.004474Z","shell.execute_reply":"2023-06-29T23:15:33.018904Z"},"trusted":true},"execution_count":93,"outputs":[{"execution_count":93,"output_type":"execute_result","data":{"text/plain":"                                                                                       Authors  \\\n0                                                           Erica Bisesi, and Marisa Michelini   \n1  Fabien Gallot, Owen Lagadec, Myriam Desainte-Catherine (LaBRI),\\n  Sylvain Marchand (LaBRI)   \n2                                                                    J.R. Dawin, D. Volchenkov   \n3                                     Georg Boenn, Martin Brain, Marina De Vos and John ffitch   \n4                             Avishek Ghosh, Joydeep Banerjee, Sk. S. Hassan, P. Pal Choudhury   \n\n                                                                                         Title  \\\n0  Planning Curricular Proposals on Sound and Music with Prospective  SecondarySchool Teachers   \n1                                        iKlax a New Musical Audio Format for Active Listening   \n2                                                  Markov Chain Analysis of Musical Dice Games   \n3                                     Automatic Music Composition using Answer Set Programming   \n4                           Fractal String Generation and Its Application in Music Composition   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Abstract  \\\n0                                                                                                                                                                                                                                                                                     Sound is a preferred context to build foundations on wave phenomena one ofthe most important disciplinary referents in physics It is also one of thebestset frameworks to achieve transversality overcoming scholastic level andactivating emotional aspects which are naturally connected with every day lifeas well as with music and perception Looking at sound and music by atransversal perspective  a borderline approach between science and art isthe adopted statement for a teaching proposal using metacognition as astrategy in scientific education This work analyzes curricular proposals onmusical acoustics planned by prospective secondaryschool teachers in theframework of a Formative Intervention Module answering the expectation ofmaking more effective teaching scientific subjects by improving creativecapabilities as well as leading to build logical and scientificcategorizations able to consciously discipline artistic activity in musicstudents With this aim a particular emphasis is given to those concepts like sound parameters and structural elements of a musical piece which arebest fitted to be addressed on a transversal perspective involvingsimultaneously physics psychophysics and music   \n1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        In this paper we are presenting a new model for interactive music Unlikemost interactive systems our model is based on file organization but does notrequire digital audio treatments This model includes a definition of aconstraints system and its solver The products of this project are intendedfor the general public inexperienced users as well as professional musiciansand will be distributed commercially We are here presenting three products ofthis project The difficulty of this project is to design a technology andsoftware products for interactive music which must be easy to use by thegeneral public and by professional composers   \n2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      We have studied entropy redundancy complexity and first passage times tonotes for  pieces of  composers The successful understanding of tonalmusic calls for an experienced listener as entropy dominates over redundancyin musical messages First passage times to notes resolve tonality and featurea composer We also discuss the possible distances in space of musical dicegames and introduced the geodesic distance based on the Riemann structureassociated to the probability vectors rows of the transition matrices   \n3  Music composition used to be a pen and paper activity These these days musicis often composed with the aid of computer software even to the point wherethe computer compose parts of the score autonomously The composition of moststyles of music is governed by rules We show that by approaching theautomation analysis and verification of composition as a knowledgerepresentation task and formalising these rules in a suitable logical languagepowerful and expressive intelligent composition tools can be easily built Thisapplication paper describes the use of answer set programming to construct anautomated system named ANTON that can compose melodic harmonic and rhythmicmusic diagnose errors in human compositions and serve as a computeraidedcomposition tool The combination of harmonic rhythmic and melodic compositionin a single framework makes ANTON unique in the growing area of algorithmiccomposition With near realtime composition ANTON reaches the point where itcan not only be used as a component in an interactive composition tool but alsohas the potential for live performances and concerts or automatically generatedbackground music in a variety of applications With the use of a fullydeclarative language and an offtheshelf reasoning engine ANTON providesthe human composer a tool which is significantly simpler more compact and moreversatile than other existing systems This paper has been accepted forpublication in Theory and Practice of Logic Programming TPLP   \n4                                                                                                                                                                                                                                                                                                                                                                                     Music is a string of some of the notes out of  notes Sa Komalre ReKomalga Ga Ma Karima Pa Komaldha Dha Komalni Ni and theirharmonics Each note corresponds to a particular frequency When such stringsare encoded to form discrete sequences different frequencies present in themusic corresponds to different amplitude levels value of the discretesequence Initially a class of discrete sequences has been generated usinglogistic map All these discrete sequences have at most ndifferent amplitudelevels value depending on the particular raga Without loss of generalitywe have chosen two discrete sequences of two types of Indian raga viz Bhairabiand Bhupali having same number of amplitude levels to obtainsearch closerelatives from the class The relative  closeness can be assured throughcorrelation coefficientThe search is unbiased random and nonadaptive Theobtained string is that which maximally resembles the given two sequences Thesame can be thought of as a music composition of the given two strings It isto be noted that all these string are fractal string which can be persuaded byfractal dimension   \n\n   Year Dataset  \n0  2008   ArXiV  \n1  2009   ArXiV  \n2  2010   ArXiV  \n3  2010   ArXiV  \n4  2011   ArXiV  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Authors</th>\n      <th>Title</th>\n      <th>Abstract</th>\n      <th>Year</th>\n      <th>Dataset</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Erica Bisesi, and Marisa Michelini</td>\n      <td>Planning Curricular Proposals on Sound and Music with Prospective  SecondarySchool Teachers</td>\n      <td>Sound is a preferred context to build foundations on wave phenomena one ofthe most important disciplinary referents in physics It is also one of thebestset frameworks to achieve transversality overcoming scholastic level andactivating emotional aspects which are naturally connected with every day lifeas well as with music and perception Looking at sound and music by atransversal perspective  a borderline approach between science and art isthe adopted statement for a teaching proposal using metacognition as astrategy in scientific education This work analyzes curricular proposals onmusical acoustics planned by prospective secondaryschool teachers in theframework of a Formative Intervention Module answering the expectation ofmaking more effective teaching scientific subjects by improving creativecapabilities as well as leading to build logical and scientificcategorizations able to consciously discipline artistic activity in musicstudents With this aim a particular emphasis is given to those concepts like sound parameters and structural elements of a musical piece which arebest fitted to be addressed on a transversal perspective involvingsimultaneously physics psychophysics and music</td>\n      <td>2008</td>\n      <td>ArXiV</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Fabien Gallot, Owen Lagadec, Myriam Desainte-Catherine (LaBRI),\\n  Sylvain Marchand (LaBRI)</td>\n      <td>iKlax a New Musical Audio Format for Active Listening</td>\n      <td>In this paper we are presenting a new model for interactive music Unlikemost interactive systems our model is based on file organization but does notrequire digital audio treatments This model includes a definition of aconstraints system and its solver The products of this project are intendedfor the general public inexperienced users as well as professional musiciansand will be distributed commercially We are here presenting three products ofthis project The difficulty of this project is to design a technology andsoftware products for interactive music which must be easy to use by thegeneral public and by professional composers</td>\n      <td>2009</td>\n      <td>ArXiV</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>J.R. Dawin, D. Volchenkov</td>\n      <td>Markov Chain Analysis of Musical Dice Games</td>\n      <td>We have studied entropy redundancy complexity and first passage times tonotes for  pieces of  composers The successful understanding of tonalmusic calls for an experienced listener as entropy dominates over redundancyin musical messages First passage times to notes resolve tonality and featurea composer We also discuss the possible distances in space of musical dicegames and introduced the geodesic distance based on the Riemann structureassociated to the probability vectors rows of the transition matrices</td>\n      <td>2010</td>\n      <td>ArXiV</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Georg Boenn, Martin Brain, Marina De Vos and John ffitch</td>\n      <td>Automatic Music Composition using Answer Set Programming</td>\n      <td>Music composition used to be a pen and paper activity These these days musicis often composed with the aid of computer software even to the point wherethe computer compose parts of the score autonomously The composition of moststyles of music is governed by rules We show that by approaching theautomation analysis and verification of composition as a knowledgerepresentation task and formalising these rules in a suitable logical languagepowerful and expressive intelligent composition tools can be easily built Thisapplication paper describes the use of answer set programming to construct anautomated system named ANTON that can compose melodic harmonic and rhythmicmusic diagnose errors in human compositions and serve as a computeraidedcomposition tool The combination of harmonic rhythmic and melodic compositionin a single framework makes ANTON unique in the growing area of algorithmiccomposition With near realtime composition ANTON reaches the point where itcan not only be used as a component in an interactive composition tool but alsohas the potential for live performances and concerts or automatically generatedbackground music in a variety of applications With the use of a fullydeclarative language and an offtheshelf reasoning engine ANTON providesthe human composer a tool which is significantly simpler more compact and moreversatile than other existing systems This paper has been accepted forpublication in Theory and Practice of Logic Programming TPLP</td>\n      <td>2010</td>\n      <td>ArXiV</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Avishek Ghosh, Joydeep Banerjee, Sk. S. Hassan, P. Pal Choudhury</td>\n      <td>Fractal String Generation and Its Application in Music Composition</td>\n      <td>Music is a string of some of the notes out of  notes Sa Komalre ReKomalga Ga Ma Karima Pa Komaldha Dha Komalni Ni and theirharmonics Each note corresponds to a particular frequency When such stringsare encoded to form discrete sequences different frequencies present in themusic corresponds to different amplitude levels value of the discretesequence Initially a class of discrete sequences has been generated usinglogistic map All these discrete sequences have at most ndifferent amplitudelevels value depending on the particular raga Without loss of generalitywe have chosen two discrete sequences of two types of Indian raga viz Bhairabiand Bhupali having same number of amplitude levels to obtainsearch closerelatives from the class The relative  closeness can be assured throughcorrelation coefficientThe search is unbiased random and nonadaptive Theobtained string is that which maximally resembles the given two sequences Thesame can be thought of as a music composition of the given two strings It isto be noted that all these string are fractal string which can be persuaded byfractal dimension</td>\n      <td>2011</td>\n      <td>ArXiV</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"dfFinal = pd.concat([dfONE, dfTWO]).drop_duplicates(subset='Title')","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:33.022124Z","iopub.execute_input":"2023-06-29T23:15:33.023621Z","iopub.status.idle":"2023-06-29T23:15:33.034498Z","shell.execute_reply.started":"2023-06-29T23:15:33.023572Z","shell.execute_reply":"2023-06-29T23:15:33.032998Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"dfFinal","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:33.037447Z","iopub.execute_input":"2023-06-29T23:15:33.038342Z","iopub.status.idle":"2023-06-29T23:15:33.061792Z","shell.execute_reply.started":"2023-06-29T23:15:33.038305Z","shell.execute_reply":"2023-06-29T23:15:33.060553Z"},"trusted":true},"execution_count":95,"outputs":[{"execution_count":95,"output_type":"execute_result","data":{"text/plain":"                                                                                                                                                                           Authors  \\\n0                                                                                                                                                                        U Kalidas   \n1                                                                                                                                         C Zhang, C Zhang, S Zheng, Y Qiao, C Li…   \n2                                                                                                                                                                           S Lowe   \n3                                                                                                                                                      H Wang, S Han, G Li, B Zhao   \n4                                                                                                                                              PS Yadav, S Khan, YV Singh, P Garg…   \n..                                                                                                                                                                             ...   \n360                                                                                                                                                Adarsh Kumar and Pedro Sarmento   \n361                                                               Haolin Zhuang, Shun Lei, Long Xiao, Weiqin Li, Liyang Chen, Sicheng\\n  Yang, Zhiyong Wu, Shiyin Kang, Helen Meng   \n362  Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang,\\n  Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, Yi Ren, Zhou\\n  Zhao, Shinji Watanabe   \n363                                                                                                                                                                       Xinyu Li   \n364                                                                                                     Claudia V. Goldman, Dan Gang, Jeffrey S. Rosenschein and Daniel\\n  Lehmann   \n\n                                                                                                                                            Title  \\\n0    \" 12\" Music Visualisation: Exploring Generative and Non-Generative techniques within a rhythmic framework in a performative spatial setting.   \n1                                                         A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?   \n2                                                                           A Fully-Unsupervised Generative Method for Choreo-Musical Translation   \n3                                                                        A Hybrid Neural Network for Music Generation Using Frequency Domain Data   \n4                                                             A Lightweight Deep Learning-Based Approach for Jazz Music Generation in MIDI Format   \n..                                                                                                                                            ...   \n360                                                  From Words to Music A Study of Subword Tokenization Techniques in  Symbolic Music Generation   \n361                                             GTNBailando Genre Consistent LongTerm D Dance Generation based on  Pretrained Genre Token Network   \n362                                                                    AudioGPT Understanding and Generating Speech Music Sound and Talking  Head   \n363                                             LooPy A ResearchFriendly Mix Framework for Music Information Retrieval  on Electronic Dance Music   \n364                                                             NetNeg A ConnectionistAgent Integrated System for Representing Musical  Knowledge   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Abstract  \\\n0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                … The two day workshop/ lec-dem involved participants working in groups to create Music on Ableton live Digital Audio Workstation, using AI based tools created by the TheKlong studio …   \n1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         … new music, while some software edit compositions in the style of various composers. Music … it is because of using AI to compose music or to help musicians. A fantastic illustration of an …   \n2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          … It is clear that AI systems applied to these tasks need similar … Our algorithm will be able to take as input either music (… that represents choreography and music that should be paired …   \n3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       … adversarial network to generate music in an end-to-end way. The automatic music generation method proposed in this paper explores a new representation of music data, and the …   \n4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                … In paper [15], jazz transformer-based AIcomposed music quantitative measures are discussed. is system measures the jazz transformation using the deep learning model. In paper [16], …   \n..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    ...   \n360  Subword tokenization has been widely successful in textbased naturallanguage processing NLP tasks with Transformerbased models As Transformermodels become increasingly popular in symbolic musicrelated studies it isimperative to investigate the efficacy of subword tokenization in the symbolicmusic domain In this paper we explore subword tokenization techniques suchas bytepair encoding BPE in symbolic music generation and its impact on theoverall structure of generated songs Our experiments are based on three typesof MIDI datasets single trackmelody only multitrack with a singleinstrument and multitrack and multiinstrument We apply subword tokenizationon postmusical tokenization schemes and find that it enables the generation oflonger songs at the same time and improves the overall structure of thegenerated music in terms of objective metrics like structure indicator SIPitch Class Entropy etc We also compare two subword tokenization methods BPEand Unigram and observe that both methods lead to consistent improvements Ourstudy suggests that subword tokenization is a promising technique for symbolicmusic generation and may have broader implications for music compositionparticularly in cases involving complex data such as multitrack songs   \n361                                                                                                                                                                                                                                                                                                                                                                                                           Musicdriven D dance generation has become an intensive research topic inrecent years with great potential for realworld applications Most existingmethods lack the consideration of genre which results in genre inconsistencyin the generated dance movements In addition the correlation between thedance genre and the music has not been investigated To address these issueswe propose a genreconsistent dance generation framework GTNBailando Firstwe propose the Genre Token Network GTN which infers the genre from music toenhance the genre consistency of longterm dance generation Second to improvethe generalization capability of the model the strategy of pretraining andfinetuning is adoptedExperimental results on the AIST dataset show that theproposed dance generation framework outperforms stateoftheart methods interms of motion quality and genre consistency   \n362                                                                                                                    Large language models LLMs have exhibited remarkable capabilities across avariety of domains and tasks challenging our understanding of learning andcognition Despite the recent success current LLMs are not capable ofprocessing complex audio information or conducting spoken conversations likeSiri or Alexa In this work we propose a multimodal AI system namedAudioGPT which complements LLMs ie ChatGPT with  foundation models toprocess complex audio information and solve numerous understanding andgeneration tasks and  the inputoutput interface ASR TTS to supportspoken dialogue With an increasing demand to evaluate multimodal LLMs ofhuman intention understanding and cooperation with foundation models weoutline the principles and processes and test AudioGPT in terms of consistencycapability and robustness Experimental results demonstrate the capabilitiesof AudioGPT in solving AI tasks with speech music sound and talking headunderstanding and generation in multiround dialogues which empower humans tocreate rich and diverse audio content with unprecedented ease Our system ispublicly available at urlhttpsgithubcomAIGCAudioAudioGPT   \n363                                                                                                                                         Music information retrieval MIR has gone through an explosive developmentwith the advancement of deep learning in recent years However music genreslike electronic dance music EDM has always been relatively less investigatedcompared to others Considering its wide range of applications we present aPython package for automated EDM audio generation as an infrastructure for MIRfor EDM songs to mitigate the difficulty of acquiring labelled data It is aconvenient tool that could be easily concatenated to the end of many symbolicmusic generation pipelines Inside this package we provide a framework tobuild professionallevel templates that could render a wellproduced track fromspecified melody and chords or produce massive tracks given only a specifickey by our probabilistic symbolic melody generator Experiments show that ourmixes could achieve the same quality of the original reference songs producedby worldfamous artists with respect to both subjective and objectivecriteria Our code is accessible in this repositoryhttpsgithubcomGariscatloopy and the official site of the project is alsoonline httpsloopyedmcom   \n364                                                                                                                             The system presented here shows the feasibility of modeling the knowledgeinvolved in a complex musical activity by integrating subsymbolic and symbolicprocesses This research focuses on the question of whether there is anyadvantage in integrating a neural network together with a distributedartificial intelligence approach within the music domain The primary purposeof our work is to design a model that describes the different aspects a usermight be interested in considering when involved in a musical activity Theapproach we suggest in this work enables the musician to encode his knowledgeintuitions and aesthetic taste into different modules The system capturesthese aspects by computing and applying three distinct functions rules fuzzyconcepts and learning  As a case study we began experimenting with first species twopartcounterpoint melodies We have developed a hybrid system composed of aconnectionist module and an agentbased module to combine the subsymbolic andsymbolic levels to achieve this task The technique presented here to representmusical knowledge constitutes a new approach for composing polyphonic music   \n\n     Year    Dataset  \n0    2023  Music1000  \n1    2023  Music1000  \n2       0  Music1000  \n3    2023  Music1000  \n4    2022  Music1000  \n..    ...        ...  \n360  2023      ArXiV  \n361  2023      ArXiV  \n362  2023      ArXiV  \n363  2023      ArXiV  \n364  2002      ArXiV  \n\n[560 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Authors</th>\n      <th>Title</th>\n      <th>Abstract</th>\n      <th>Year</th>\n      <th>Dataset</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>U Kalidas</td>\n      <td>\" 12\" Music Visualisation: Exploring Generative and Non-Generative techniques within a rhythmic framework in a performative spatial setting.</td>\n      <td>… The two day workshop/ lec-dem involved participants working in groups to create Music on Ableton live Digital Audio Workstation, using AI based tools created by the TheKlong studio …</td>\n      <td>2023</td>\n      <td>Music1000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>C Zhang, C Zhang, S Zheng, Y Qiao, C Li…</td>\n      <td>A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?</td>\n      <td>… new music, while some software edit compositions in the style of various composers. Music … it is because of using AI to compose music or to help musicians. A fantastic illustration of an …</td>\n      <td>2023</td>\n      <td>Music1000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>S Lowe</td>\n      <td>A Fully-Unsupervised Generative Method for Choreo-Musical Translation</td>\n      <td>… It is clear that AI systems applied to these tasks need similar … Our algorithm will be able to take as input either music (… that represents choreography and music that should be paired …</td>\n      <td>0</td>\n      <td>Music1000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>H Wang, S Han, G Li, B Zhao</td>\n      <td>A Hybrid Neural Network for Music Generation Using Frequency Domain Data</td>\n      <td>… adversarial network to generate music in an end-to-end way. The automatic music generation method proposed in this paper explores a new representation of music data, and the …</td>\n      <td>2023</td>\n      <td>Music1000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>PS Yadav, S Khan, YV Singh, P Garg…</td>\n      <td>A Lightweight Deep Learning-Based Approach for Jazz Music Generation in MIDI Format</td>\n      <td>… In paper [15], jazz transformer-based AIcomposed music quantitative measures are discussed. is system measures the jazz transformation using the deep learning model. In paper [16], …</td>\n      <td>2022</td>\n      <td>Music1000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>360</th>\n      <td>Adarsh Kumar and Pedro Sarmento</td>\n      <td>From Words to Music A Study of Subword Tokenization Techniques in  Symbolic Music Generation</td>\n      <td>Subword tokenization has been widely successful in textbased naturallanguage processing NLP tasks with Transformerbased models As Transformermodels become increasingly popular in symbolic musicrelated studies it isimperative to investigate the efficacy of subword tokenization in the symbolicmusic domain In this paper we explore subword tokenization techniques suchas bytepair encoding BPE in symbolic music generation and its impact on theoverall structure of generated songs Our experiments are based on three typesof MIDI datasets single trackmelody only multitrack with a singleinstrument and multitrack and multiinstrument We apply subword tokenizationon postmusical tokenization schemes and find that it enables the generation oflonger songs at the same time and improves the overall structure of thegenerated music in terms of objective metrics like structure indicator SIPitch Class Entropy etc We also compare two subword tokenization methods BPEand Unigram and observe that both methods lead to consistent improvements Ourstudy suggests that subword tokenization is a promising technique for symbolicmusic generation and may have broader implications for music compositionparticularly in cases involving complex data such as multitrack songs</td>\n      <td>2023</td>\n      <td>ArXiV</td>\n    </tr>\n    <tr>\n      <th>361</th>\n      <td>Haolin Zhuang, Shun Lei, Long Xiao, Weiqin Li, Liyang Chen, Sicheng\\n  Yang, Zhiyong Wu, Shiyin Kang, Helen Meng</td>\n      <td>GTNBailando Genre Consistent LongTerm D Dance Generation based on  Pretrained Genre Token Network</td>\n      <td>Musicdriven D dance generation has become an intensive research topic inrecent years with great potential for realworld applications Most existingmethods lack the consideration of genre which results in genre inconsistencyin the generated dance movements In addition the correlation between thedance genre and the music has not been investigated To address these issueswe propose a genreconsistent dance generation framework GTNBailando Firstwe propose the Genre Token Network GTN which infers the genre from music toenhance the genre consistency of longterm dance generation Second to improvethe generalization capability of the model the strategy of pretraining andfinetuning is adoptedExperimental results on the AIST dataset show that theproposed dance generation framework outperforms stateoftheart methods interms of motion quality and genre consistency</td>\n      <td>2023</td>\n      <td>ArXiV</td>\n    </tr>\n    <tr>\n      <th>362</th>\n      <td>Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang,\\n  Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, Yi Ren, Zhou\\n  Zhao, Shinji Watanabe</td>\n      <td>AudioGPT Understanding and Generating Speech Music Sound and Talking  Head</td>\n      <td>Large language models LLMs have exhibited remarkable capabilities across avariety of domains and tasks challenging our understanding of learning andcognition Despite the recent success current LLMs are not capable ofprocessing complex audio information or conducting spoken conversations likeSiri or Alexa In this work we propose a multimodal AI system namedAudioGPT which complements LLMs ie ChatGPT with  foundation models toprocess complex audio information and solve numerous understanding andgeneration tasks and  the inputoutput interface ASR TTS to supportspoken dialogue With an increasing demand to evaluate multimodal LLMs ofhuman intention understanding and cooperation with foundation models weoutline the principles and processes and test AudioGPT in terms of consistencycapability and robustness Experimental results demonstrate the capabilitiesof AudioGPT in solving AI tasks with speech music sound and talking headunderstanding and generation in multiround dialogues which empower humans tocreate rich and diverse audio content with unprecedented ease Our system ispublicly available at urlhttpsgithubcomAIGCAudioAudioGPT</td>\n      <td>2023</td>\n      <td>ArXiV</td>\n    </tr>\n    <tr>\n      <th>363</th>\n      <td>Xinyu Li</td>\n      <td>LooPy A ResearchFriendly Mix Framework for Music Information Retrieval  on Electronic Dance Music</td>\n      <td>Music information retrieval MIR has gone through an explosive developmentwith the advancement of deep learning in recent years However music genreslike electronic dance music EDM has always been relatively less investigatedcompared to others Considering its wide range of applications we present aPython package for automated EDM audio generation as an infrastructure for MIRfor EDM songs to mitigate the difficulty of acquiring labelled data It is aconvenient tool that could be easily concatenated to the end of many symbolicmusic generation pipelines Inside this package we provide a framework tobuild professionallevel templates that could render a wellproduced track fromspecified melody and chords or produce massive tracks given only a specifickey by our probabilistic symbolic melody generator Experiments show that ourmixes could achieve the same quality of the original reference songs producedby worldfamous artists with respect to both subjective and objectivecriteria Our code is accessible in this repositoryhttpsgithubcomGariscatloopy and the official site of the project is alsoonline httpsloopyedmcom</td>\n      <td>2023</td>\n      <td>ArXiV</td>\n    </tr>\n    <tr>\n      <th>364</th>\n      <td>Claudia V. Goldman, Dan Gang, Jeffrey S. Rosenschein and Daniel\\n  Lehmann</td>\n      <td>NetNeg A ConnectionistAgent Integrated System for Representing Musical  Knowledge</td>\n      <td>The system presented here shows the feasibility of modeling the knowledgeinvolved in a complex musical activity by integrating subsymbolic and symbolicprocesses This research focuses on the question of whether there is anyadvantage in integrating a neural network together with a distributedartificial intelligence approach within the music domain The primary purposeof our work is to design a model that describes the different aspects a usermight be interested in considering when involved in a musical activity Theapproach we suggest in this work enables the musician to encode his knowledgeintuitions and aesthetic taste into different modules The system capturesthese aspects by computing and applying three distinct functions rules fuzzyconcepts and learning  As a case study we began experimenting with first species twopartcounterpoint melodies We have developed a hybrid system composed of aconnectionist module and an agentbased module to combine the subsymbolic andsymbolic levels to achieve this task The technique presented here to representmusical knowledge constitutes a new approach for composing polyphonic music</td>\n      <td>2002</td>\n      <td>ArXiV</td>\n    </tr>\n  </tbody>\n</table>\n<p>560 rows × 5 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"dfFinal.shape","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:33.063471Z","iopub.execute_input":"2023-06-29T23:15:33.064846Z","iopub.status.idle":"2023-06-29T23:15:33.073140Z","shell.execute_reply.started":"2023-06-29T23:15:33.064802Z","shell.execute_reply":"2023-06-29T23:15:33.071777Z"},"trusted":true},"execution_count":96,"outputs":[{"execution_count":96,"output_type":"execute_result","data":{"text/plain":"(560, 5)"},"metadata":{}}]},{"cell_type":"code","source":"pip install langdetect","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:33.074572Z","iopub.execute_input":"2023-06-29T23:15:33.074928Z","iopub.status.idle":"2023-06-29T23:15:52.539605Z","shell.execute_reply.started":"2023-06-29T23:15:33.074899Z","shell.execute_reply":"2023-06-29T23:15:52.537960Z"},"trusted":true},"execution_count":97,"outputs":[{"name":"stdout","text":"Collecting langdetect\n  Downloading langdetect-1.0.9.tar.gz (981 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from langdetect) (1.16.0)\nBuilding wheels for collected packages: langdetect\n  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993241 sha256=6529e9ff8ed7ad565124a77117c61a1c6d33659641de3b1a8d63b8349c5aa0b4\n  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\nSuccessfully built langdetect\nInstalling collected packages: langdetect\nSuccessfully installed langdetect-1.0.9\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from langdetect import detect\n\n# Function to detect the language of a given text\ndef detect_language(text):\n    try:\n        return detect(text)\n    except:\n        return None\n\n# Filter papers that are only in English language\ndfFinalEnglishPapers = dfFinal[dfFinal['Abstract'].apply(lambda x: detect_language(x) == 'en')]\ndfFinalEnglishPapers","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:52.543440Z","iopub.execute_input":"2023-06-29T23:15:52.543883Z","iopub.status.idle":"2023-06-29T23:15:59.714232Z","shell.execute_reply.started":"2023-06-29T23:15:52.543847Z","shell.execute_reply":"2023-06-29T23:15:59.712822Z"},"trusted":true},"execution_count":98,"outputs":[{"execution_count":98,"output_type":"execute_result","data":{"text/plain":"                                                                                                                                                                           Authors  \\\n0                                                                                                                                                                        U Kalidas   \n1                                                                                                                                         C Zhang, C Zhang, S Zheng, Y Qiao, C Li…   \n2                                                                                                                                                                           S Lowe   \n3                                                                                                                                                      H Wang, S Han, G Li, B Zhao   \n4                                                                                                                                              PS Yadav, S Khan, YV Singh, P Garg…   \n..                                                                                                                                                                             ...   \n360                                                                                                                                                Adarsh Kumar and Pedro Sarmento   \n361                                                               Haolin Zhuang, Shun Lei, Long Xiao, Weiqin Li, Liyang Chen, Sicheng\\n  Yang, Zhiyong Wu, Shiyin Kang, Helen Meng   \n362  Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang,\\n  Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, Yi Ren, Zhou\\n  Zhao, Shinji Watanabe   \n363                                                                                                                                                                       Xinyu Li   \n364                                                                                                     Claudia V. Goldman, Dan Gang, Jeffrey S. Rosenschein and Daniel\\n  Lehmann   \n\n                                                                                                                                            Title  \\\n0    \" 12\" Music Visualisation: Exploring Generative and Non-Generative techniques within a rhythmic framework in a performative spatial setting.   \n1                                                         A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?   \n2                                                                           A Fully-Unsupervised Generative Method for Choreo-Musical Translation   \n3                                                                        A Hybrid Neural Network for Music Generation Using Frequency Domain Data   \n4                                                             A Lightweight Deep Learning-Based Approach for Jazz Music Generation in MIDI Format   \n..                                                                                                                                            ...   \n360                                                  From Words to Music A Study of Subword Tokenization Techniques in  Symbolic Music Generation   \n361                                             GTNBailando Genre Consistent LongTerm D Dance Generation based on  Pretrained Genre Token Network   \n362                                                                    AudioGPT Understanding and Generating Speech Music Sound and Talking  Head   \n363                                             LooPy A ResearchFriendly Mix Framework for Music Information Retrieval  on Electronic Dance Music   \n364                                                             NetNeg A ConnectionistAgent Integrated System for Representing Musical  Knowledge   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Abstract  \\\n0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                … The two day workshop/ lec-dem involved participants working in groups to create Music on Ableton live Digital Audio Workstation, using AI based tools created by the TheKlong studio …   \n1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         … new music, while some software edit compositions in the style of various composers. Music … it is because of using AI to compose music or to help musicians. A fantastic illustration of an …   \n2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          … It is clear that AI systems applied to these tasks need similar … Our algorithm will be able to take as input either music (… that represents choreography and music that should be paired …   \n3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       … adversarial network to generate music in an end-to-end way. The automatic music generation method proposed in this paper explores a new representation of music data, and the …   \n4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                … In paper [15], jazz transformer-based AIcomposed music quantitative measures are discussed. is system measures the jazz transformation using the deep learning model. In paper [16], …   \n..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    ...   \n360  Subword tokenization has been widely successful in textbased naturallanguage processing NLP tasks with Transformerbased models As Transformermodels become increasingly popular in symbolic musicrelated studies it isimperative to investigate the efficacy of subword tokenization in the symbolicmusic domain In this paper we explore subword tokenization techniques suchas bytepair encoding BPE in symbolic music generation and its impact on theoverall structure of generated songs Our experiments are based on three typesof MIDI datasets single trackmelody only multitrack with a singleinstrument and multitrack and multiinstrument We apply subword tokenizationon postmusical tokenization schemes and find that it enables the generation oflonger songs at the same time and improves the overall structure of thegenerated music in terms of objective metrics like structure indicator SIPitch Class Entropy etc We also compare two subword tokenization methods BPEand Unigram and observe that both methods lead to consistent improvements Ourstudy suggests that subword tokenization is a promising technique for symbolicmusic generation and may have broader implications for music compositionparticularly in cases involving complex data such as multitrack songs   \n361                                                                                                                                                                                                                                                                                                                                                                                                           Musicdriven D dance generation has become an intensive research topic inrecent years with great potential for realworld applications Most existingmethods lack the consideration of genre which results in genre inconsistencyin the generated dance movements In addition the correlation between thedance genre and the music has not been investigated To address these issueswe propose a genreconsistent dance generation framework GTNBailando Firstwe propose the Genre Token Network GTN which infers the genre from music toenhance the genre consistency of longterm dance generation Second to improvethe generalization capability of the model the strategy of pretraining andfinetuning is adoptedExperimental results on the AIST dataset show that theproposed dance generation framework outperforms stateoftheart methods interms of motion quality and genre consistency   \n362                                                                                                                    Large language models LLMs have exhibited remarkable capabilities across avariety of domains and tasks challenging our understanding of learning andcognition Despite the recent success current LLMs are not capable ofprocessing complex audio information or conducting spoken conversations likeSiri or Alexa In this work we propose a multimodal AI system namedAudioGPT which complements LLMs ie ChatGPT with  foundation models toprocess complex audio information and solve numerous understanding andgeneration tasks and  the inputoutput interface ASR TTS to supportspoken dialogue With an increasing demand to evaluate multimodal LLMs ofhuman intention understanding and cooperation with foundation models weoutline the principles and processes and test AudioGPT in terms of consistencycapability and robustness Experimental results demonstrate the capabilitiesof AudioGPT in solving AI tasks with speech music sound and talking headunderstanding and generation in multiround dialogues which empower humans tocreate rich and diverse audio content with unprecedented ease Our system ispublicly available at urlhttpsgithubcomAIGCAudioAudioGPT   \n363                                                                                                                                         Music information retrieval MIR has gone through an explosive developmentwith the advancement of deep learning in recent years However music genreslike electronic dance music EDM has always been relatively less investigatedcompared to others Considering its wide range of applications we present aPython package for automated EDM audio generation as an infrastructure for MIRfor EDM songs to mitigate the difficulty of acquiring labelled data It is aconvenient tool that could be easily concatenated to the end of many symbolicmusic generation pipelines Inside this package we provide a framework tobuild professionallevel templates that could render a wellproduced track fromspecified melody and chords or produce massive tracks given only a specifickey by our probabilistic symbolic melody generator Experiments show that ourmixes could achieve the same quality of the original reference songs producedby worldfamous artists with respect to both subjective and objectivecriteria Our code is accessible in this repositoryhttpsgithubcomGariscatloopy and the official site of the project is alsoonline httpsloopyedmcom   \n364                                                                                                                             The system presented here shows the feasibility of modeling the knowledgeinvolved in a complex musical activity by integrating subsymbolic and symbolicprocesses This research focuses on the question of whether there is anyadvantage in integrating a neural network together with a distributedartificial intelligence approach within the music domain The primary purposeof our work is to design a model that describes the different aspects a usermight be interested in considering when involved in a musical activity Theapproach we suggest in this work enables the musician to encode his knowledgeintuitions and aesthetic taste into different modules The system capturesthese aspects by computing and applying three distinct functions rules fuzzyconcepts and learning  As a case study we began experimenting with first species twopartcounterpoint melodies We have developed a hybrid system composed of aconnectionist module and an agentbased module to combine the subsymbolic andsymbolic levels to achieve this task The technique presented here to representmusical knowledge constitutes a new approach for composing polyphonic music   \n\n     Year    Dataset  \n0    2023  Music1000  \n1    2023  Music1000  \n2       0  Music1000  \n3    2023  Music1000  \n4    2022  Music1000  \n..    ...        ...  \n360  2023      ArXiV  \n361  2023      ArXiV  \n362  2023      ArXiV  \n363  2023      ArXiV  \n364  2002      ArXiV  \n\n[557 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Authors</th>\n      <th>Title</th>\n      <th>Abstract</th>\n      <th>Year</th>\n      <th>Dataset</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>U Kalidas</td>\n      <td>\" 12\" Music Visualisation: Exploring Generative and Non-Generative techniques within a rhythmic framework in a performative spatial setting.</td>\n      <td>… The two day workshop/ lec-dem involved participants working in groups to create Music on Ableton live Digital Audio Workstation, using AI based tools created by the TheKlong studio …</td>\n      <td>2023</td>\n      <td>Music1000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>C Zhang, C Zhang, S Zheng, Y Qiao, C Li…</td>\n      <td>A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?</td>\n      <td>… new music, while some software edit compositions in the style of various composers. Music … it is because of using AI to compose music or to help musicians. A fantastic illustration of an …</td>\n      <td>2023</td>\n      <td>Music1000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>S Lowe</td>\n      <td>A Fully-Unsupervised Generative Method for Choreo-Musical Translation</td>\n      <td>… It is clear that AI systems applied to these tasks need similar … Our algorithm will be able to take as input either music (… that represents choreography and music that should be paired …</td>\n      <td>0</td>\n      <td>Music1000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>H Wang, S Han, G Li, B Zhao</td>\n      <td>A Hybrid Neural Network for Music Generation Using Frequency Domain Data</td>\n      <td>… adversarial network to generate music in an end-to-end way. The automatic music generation method proposed in this paper explores a new representation of music data, and the …</td>\n      <td>2023</td>\n      <td>Music1000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>PS Yadav, S Khan, YV Singh, P Garg…</td>\n      <td>A Lightweight Deep Learning-Based Approach for Jazz Music Generation in MIDI Format</td>\n      <td>… In paper [15], jazz transformer-based AIcomposed music quantitative measures are discussed. is system measures the jazz transformation using the deep learning model. In paper [16], …</td>\n      <td>2022</td>\n      <td>Music1000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>360</th>\n      <td>Adarsh Kumar and Pedro Sarmento</td>\n      <td>From Words to Music A Study of Subword Tokenization Techniques in  Symbolic Music Generation</td>\n      <td>Subword tokenization has been widely successful in textbased naturallanguage processing NLP tasks with Transformerbased models As Transformermodels become increasingly popular in symbolic musicrelated studies it isimperative to investigate the efficacy of subword tokenization in the symbolicmusic domain In this paper we explore subword tokenization techniques suchas bytepair encoding BPE in symbolic music generation and its impact on theoverall structure of generated songs Our experiments are based on three typesof MIDI datasets single trackmelody only multitrack with a singleinstrument and multitrack and multiinstrument We apply subword tokenizationon postmusical tokenization schemes and find that it enables the generation oflonger songs at the same time and improves the overall structure of thegenerated music in terms of objective metrics like structure indicator SIPitch Class Entropy etc We also compare two subword tokenization methods BPEand Unigram and observe that both methods lead to consistent improvements Ourstudy suggests that subword tokenization is a promising technique for symbolicmusic generation and may have broader implications for music compositionparticularly in cases involving complex data such as multitrack songs</td>\n      <td>2023</td>\n      <td>ArXiV</td>\n    </tr>\n    <tr>\n      <th>361</th>\n      <td>Haolin Zhuang, Shun Lei, Long Xiao, Weiqin Li, Liyang Chen, Sicheng\\n  Yang, Zhiyong Wu, Shiyin Kang, Helen Meng</td>\n      <td>GTNBailando Genre Consistent LongTerm D Dance Generation based on  Pretrained Genre Token Network</td>\n      <td>Musicdriven D dance generation has become an intensive research topic inrecent years with great potential for realworld applications Most existingmethods lack the consideration of genre which results in genre inconsistencyin the generated dance movements In addition the correlation between thedance genre and the music has not been investigated To address these issueswe propose a genreconsistent dance generation framework GTNBailando Firstwe propose the Genre Token Network GTN which infers the genre from music toenhance the genre consistency of longterm dance generation Second to improvethe generalization capability of the model the strategy of pretraining andfinetuning is adoptedExperimental results on the AIST dataset show that theproposed dance generation framework outperforms stateoftheart methods interms of motion quality and genre consistency</td>\n      <td>2023</td>\n      <td>ArXiV</td>\n    </tr>\n    <tr>\n      <th>362</th>\n      <td>Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang,\\n  Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, Yi Ren, Zhou\\n  Zhao, Shinji Watanabe</td>\n      <td>AudioGPT Understanding and Generating Speech Music Sound and Talking  Head</td>\n      <td>Large language models LLMs have exhibited remarkable capabilities across avariety of domains and tasks challenging our understanding of learning andcognition Despite the recent success current LLMs are not capable ofprocessing complex audio information or conducting spoken conversations likeSiri or Alexa In this work we propose a multimodal AI system namedAudioGPT which complements LLMs ie ChatGPT with  foundation models toprocess complex audio information and solve numerous understanding andgeneration tasks and  the inputoutput interface ASR TTS to supportspoken dialogue With an increasing demand to evaluate multimodal LLMs ofhuman intention understanding and cooperation with foundation models weoutline the principles and processes and test AudioGPT in terms of consistencycapability and robustness Experimental results demonstrate the capabilitiesof AudioGPT in solving AI tasks with speech music sound and talking headunderstanding and generation in multiround dialogues which empower humans tocreate rich and diverse audio content with unprecedented ease Our system ispublicly available at urlhttpsgithubcomAIGCAudioAudioGPT</td>\n      <td>2023</td>\n      <td>ArXiV</td>\n    </tr>\n    <tr>\n      <th>363</th>\n      <td>Xinyu Li</td>\n      <td>LooPy A ResearchFriendly Mix Framework for Music Information Retrieval  on Electronic Dance Music</td>\n      <td>Music information retrieval MIR has gone through an explosive developmentwith the advancement of deep learning in recent years However music genreslike electronic dance music EDM has always been relatively less investigatedcompared to others Considering its wide range of applications we present aPython package for automated EDM audio generation as an infrastructure for MIRfor EDM songs to mitigate the difficulty of acquiring labelled data It is aconvenient tool that could be easily concatenated to the end of many symbolicmusic generation pipelines Inside this package we provide a framework tobuild professionallevel templates that could render a wellproduced track fromspecified melody and chords or produce massive tracks given only a specifickey by our probabilistic symbolic melody generator Experiments show that ourmixes could achieve the same quality of the original reference songs producedby worldfamous artists with respect to both subjective and objectivecriteria Our code is accessible in this repositoryhttpsgithubcomGariscatloopy and the official site of the project is alsoonline httpsloopyedmcom</td>\n      <td>2023</td>\n      <td>ArXiV</td>\n    </tr>\n    <tr>\n      <th>364</th>\n      <td>Claudia V. Goldman, Dan Gang, Jeffrey S. Rosenschein and Daniel\\n  Lehmann</td>\n      <td>NetNeg A ConnectionistAgent Integrated System for Representing Musical  Knowledge</td>\n      <td>The system presented here shows the feasibility of modeling the knowledgeinvolved in a complex musical activity by integrating subsymbolic and symbolicprocesses This research focuses on the question of whether there is anyadvantage in integrating a neural network together with a distributedartificial intelligence approach within the music domain The primary purposeof our work is to design a model that describes the different aspects a usermight be interested in considering when involved in a musical activity Theapproach we suggest in this work enables the musician to encode his knowledgeintuitions and aesthetic taste into different modules The system capturesthese aspects by computing and applying three distinct functions rules fuzzyconcepts and learning  As a case study we began experimenting with first species twopartcounterpoint melodies We have developed a hybrid system composed of aconnectionist module and an agentbased module to combine the subsymbolic andsymbolic levels to achieve this task The technique presented here to representmusical knowledge constitutes a new approach for composing polyphonic music</td>\n      <td>2002</td>\n      <td>ArXiV</td>\n    </tr>\n  </tbody>\n</table>\n<p>557 rows × 5 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# import pandas as pd\n\n# # Create separate dataframes for papers mentioning Bert and GPT\n# dfBertPapers = dfFinalEnglishPapers[dfFinalEnglishPapers['Title'].str.contains(r'\\bBert\\b', case=False) | dfFinalEnglishPapers['Abstract'].str.contains(r'\\bBert\\b', case=False)]\n# dfGptPapers = dfFinalEnglishPapers[dfFinalEnglishPapers['Title'].str.contains(r'\\bGPT\\b', case=False) | dfFinalEnglishPapers['Abstract'].str.contains(r'\\bGPT\\b', case=False)]\n# print(dfBertPapers.shape)\n# print(dfGptPapers.shape)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:59.715788Z","iopub.execute_input":"2023-06-29T23:15:59.716192Z","iopub.status.idle":"2023-06-29T23:15:59.723315Z","shell.execute_reply.started":"2023-06-29T23:15:59.716161Z","shell.execute_reply":"2023-06-29T23:15:59.721705Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"# dfGptPapers","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:59.725109Z","iopub.execute_input":"2023-06-29T23:15:59.726129Z","iopub.status.idle":"2023-06-29T23:15:59.741782Z","shell.execute_reply.started":"2023-06-29T23:15:59.726090Z","shell.execute_reply":"2023-06-29T23:15:59.740393Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"# dfBertPapers","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:59.743735Z","iopub.execute_input":"2023-06-29T23:15:59.744179Z","iopub.status.idle":"2023-06-29T23:15:59.756963Z","shell.execute_reply.started":"2023-06-29T23:15:59.744134Z","shell.execute_reply":"2023-06-29T23:15:59.755646Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"# import re\n# # Create a new column 'PrecedingWord' to store the word preceding \"learning\" in titles\n# dfFinalEnglishPapers['PrecedingWord'] = dfFinalEnglishPapers['Title'].str.extract(r'(\\S+)\\s+learning', flags=re.IGNORECASE)\n\n# # Get the top 10 words preceding \"learning\" and their counts\n# top_words = dfFinalEnglishPapers['PrecedingWord'].value_counts().head(10)\n\n# # Print the top 10 words and their counts\n# for word, count in top_words.items():\n#     print(word, count)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:59.758767Z","iopub.execute_input":"2023-06-29T23:15:59.759543Z","iopub.status.idle":"2023-06-29T23:15:59.770895Z","shell.execute_reply.started":"2023-06-29T23:15:59.759508Z","shell.execute_reply":"2023-06-29T23:15:59.769770Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"dfFinalEnglishPapers = dfFinalEnglishPapers.sort_values('Year', ascending=True)\ndfFinalEnglishPapers_2022_2023 = dfFinalEnglishPapers[dfFinalEnglishPapers['Year'] >= 2022]\ndfFinalEnglishPapersB42021 = dfFinalEnglishPapers[dfFinalEnglishPapers['Year'] < 2022]\ndfFinalEnglishPapers_2022_2023.reset_index(drop=True, inplace=True)\ndfFinalEnglishPapersB42021.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:59.772645Z","iopub.execute_input":"2023-06-29T23:15:59.773045Z","iopub.status.idle":"2023-06-29T23:15:59.792267Z","shell.execute_reply.started":"2023-06-29T23:15:59.773014Z","shell.execute_reply":"2023-06-29T23:15:59.790936Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"print(dfFinalEnglishPapers_2022_2023.shape)\nprint(dfFinalEnglishPapersB42021.shape)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:59.794182Z","iopub.execute_input":"2023-06-29T23:15:59.795211Z","iopub.status.idle":"2023-06-29T23:15:59.805528Z","shell.execute_reply.started":"2023-06-29T23:15:59.795171Z","shell.execute_reply":"2023-06-29T23:15:59.803970Z"},"trusted":true},"execution_count":104,"outputs":[{"name":"stdout","text":"(282, 5)\n(275, 5)\n","output_type":"stream"}]},{"cell_type":"code","source":"dfFinalEnglishPapers_2022_2023.to_csv('/kaggle/working/Music_2022_2023_list_final2.csv', index=True, header= True)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:59.807779Z","iopub.execute_input":"2023-06-29T23:15:59.808416Z","iopub.status.idle":"2023-06-29T23:15:59.831865Z","shell.execute_reply.started":"2023-06-29T23:15:59.808368Z","shell.execute_reply":"2023-06-29T23:15:59.830400Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"dfFinalEnglishPapersB42021.to_csv('/kaggle/working/Music_B42021_list_final2.csv', index=True, header= True)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:59.834118Z","iopub.execute_input":"2023-06-29T23:15:59.835060Z","iopub.status.idle":"2023-06-29T23:15:59.860053Z","shell.execute_reply.started":"2023-06-29T23:15:59.835004Z","shell.execute_reply":"2023-06-29T23:15:59.858510Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"dfFinalEnglishPapers_2022_2023 = pd.read_csv('/kaggle/input/Input/Music_2022_2023_final.csv')","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:59.861801Z","iopub.execute_input":"2023-06-29T23:15:59.862576Z","iopub.status.idle":"2023-06-29T23:15:59.891582Z","shell.execute_reply.started":"2023-06-29T23:15:59.862531Z","shell.execute_reply":"2023-06-29T23:15:59.889956Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"dfFinalEnglishPapers_2022_2023","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:59.893368Z","iopub.execute_input":"2023-06-29T23:15:59.894201Z","iopub.status.idle":"2023-06-29T23:15:59.914841Z","shell.execute_reply.started":"2023-06-29T23:15:59.894155Z","shell.execute_reply":"2023-06-29T23:15:59.913569Z"},"trusted":true},"execution_count":108,"outputs":[{"execution_count":108,"output_type":"execute_result","data":{"text/plain":"     Unnamed: 0  \\\n0             0   \n1             1   \n2             2   \n3             3   \n4             4   \n..          ...   \n277         277   \n278         278   \n279         279   \n280         280   \n281         281   \n\n                                                                                 Authors  \\\n0                                               Carlos Hernandez-Olivan, Jose R. Beltran   \n1                       Zhihuan Kuang, Shi Zong, Jianbing Zhang, Jiajun Chen, Hongfu Liu   \n2                                                                     Yi Luo, Jianwei Yu   \n3    Yusong Wu, Josh Gardner, Ethan Manilow, Ian Simon, Curtis Hawthorne,\\n  Jesse Engel   \n4                                       Junyan Jiang, Daniel Chin, Yixiao Zhang, Gus Xia   \n..                                                                                   ...   \n277                                                                  A Kumar, P Sarmento   \n278                                                     D von Rütte, L Biggio, Y Kilcher   \n279                                                                S Colton, S Cardinale   \n280                                                          K Tan, H Liu, S Huang, C Li   \n281                                                                            U Kalidas   \n\n                                                                                                                                            Title  \\\n0                                                             musicaiz A Python Library for Symbolic Music Generation Analysis and  Visualization   \n1                                                                     MusictoText Synaesthesia Generating Descriptive Text from Music  Recordings   \n2                                                                                                      Music Source Separation with Bandsplit RNN   \n3                                                          The Chamber Ensemble Generator Limitless HighQuality MIR Data via  Generative Modeling   \n4                                                                                        Learning Hierarchical Metrical Structure Beyond Measures   \n..                                                                                                                                            ...   \n277                                                  From Words to Music: A Study of Subword Tokenization Techniques in Symbolic Music Generation   \n278                                                                       FIGARO: Controllable Music Generation using Learned and Expert Features   \n279                                                              Extending Generative Neo-Riemannian Theory for Event-Based Soundtrack Production   \n280                                            Efficacy of Music Intervention for Dental Anxiety Disorders: A Systematic Review and Meta-Analysis   \n281  \" 12\" Music Visualisation: Exploring Generative and Non-Generative techniques within a rhythmic framework in a performative spatial setting.   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Abstract  \\\n0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     In this article we present musicaiz an objectoriented library foranalyzing generating and evaluating symbolic music The submodules of thepackage allow the user to create symbolic music data from scratch buildalgorithms to analyze symbolic music encode MIDI data as tokens to train deeplearning sequence models modify existing music data and evaluate musicgeneration systems The evaluation submodule builds on previous work toobjectively measure music generation systems and to be able to reproduce theresults of music generation models The library is publicly available onlineWe encourage the community to contribute and provide feedback   \n1    In this paper we consider a novel research problem musictotextsynaesthesia Different from the classical music tagging problem thatclassifies a music recording into predefined categories the musictotextsynaesthesia aims to generate descriptive texts from music recordings forfurther understanding Although this is a new and interesting application tothe machine learning community to our best knowledge the existingmusicrelated datasets do not contain the semantic descriptions on musicrecordings and cannot serve the musictotext synaesthesia task In light ofthis we collect a new dataset that contains  aligned pairs of classicalmusic recordings and text descriptions Based on this we build a computationalmodel to generate sentences that can describe the content of the musicrecording To tackle the highly nondiscriminative classical music we design agroup topologypreservation loss in our computational model which considersmore samples as a group reference and preserves the relative topology amongdifferent samples Extensive experimental results qualitatively andquantitatively demonstrate the effectiveness of our proposed model over fiveheuristics or pretrained competitive methods and their variants on ourcollected dataset   \n2                   The performance of music source separation MSS models has been greatlyimproved in recent years thanks to the development of novel neural networkarchitectures and training pipelines However recent model designs for MSSwere mainly motivated by other audio processing tasks or other research fieldswhile the intrinsic characteristics and patterns of the music signals were notfully discovered In this paper we propose bandsplit RNN BSRNN afrequencydomain model that explictly splits the spectrogram of the mixtureinto subbands and perform interleaved bandlevel and sequencelevel modelingThe choices of the bandwidths of the subbands can be determined by a prioriknowledge or expert knowledge on the characteristics of the target source inorder to optimize the performance on a certain type of target musicalinstrument To better make use of unlabeled data we also describe asemisupervised model finetuning pipeline that can further improve theperformance of the model Experiment results show that BSRNN trained only onMUSDBHQ dataset significantly outperforms several topranking models inMusic Demixing MDX Challenge  and the semisupervised finetuning stagefurther improves the performance on all four instrument tracks   \n3                                                                                                                                                                                     Data is the lifeblood of modern machine learning systems including for thosein Music Information Retrieval MIR However MIR has long been mired by smalldatasets and unreliable labels In this work we propose to break thisbottleneck using generative modeling By pipelining a generative model of notesCoconet trained on Bach Chorales with a structured synthesis model of chamberensembles MIDIDDSP trained on URMP we demonstrate a system capable ofproducing unlimited amounts of realistic chorale music with rich annotationsincluding mixes stems MIDI notelevel performance attributes staccatovibrato etc and even finegrained synthesis parameters pitch amplitudeetc We call this system the Chamber Ensemble Generator CEG and use it togenerate a large dataset of chorales from four different chamber ensemblesCocoChorales We demonstrate that data generated using our approach improvesstateoftheart models for music transcription and source separation and werelease both the system and the dataset as an opensource foundation for futurework in the MIR community   \n4                                                                                                                                                                                                                                     Music contains hierarchical structures beyond beats and measures Whilehierarchical structure annotations are helpful for music information retrievaland computer musicology such annotations are scarce in current digital musicdatabases In this paper we explore a datadriven approach to automaticallyextract hierarchical metrical structures from scores We propose a new modelwith a Temporal Convolutional NetworkConditional Random Field TCNCRFarchitecture Given a symbolic music score our model takes in an arbitrarynumber of voices in a beatquantized form and predicts a level hierarchicalmetrical structure from downbeatlevel to sectionlevel We also annotate adataset using RWCPOP MIDI files to facilitate training and evaluation We showby experiments that the proposed method performs better than the rulebasedapproach under different orchestration settings We also perform some simplemusicological analysis on the model predictions All demos datasets andpretrained models are publicly available on Github   \n..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ...   \n277                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          of the music becomes complex such as in the case of polyphonic music or multi-track music. A  The jazz transformer on the front line: Exploring the shortcomings of ai-composed music    \n278                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   to music generation. We aim to extend this kind of control to other domains, in this case to music  In this regard, we believe that our contribution is a step toward human-AI collaboration    \n279                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            of music, but we provide a generative formalism, which can be employed to make music, as  and making the music between events suitably low-key, so the event music is not obfuscated   \n280                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         in S-AI between the preoperative experimental and control patients (I 2 = 0%, WMD (95% CI): 0.82(−0.73, 2.37), p = 0.30); postoperatively, music therapy significantly reduced S-AI in    \n281                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           The two day workshop/ lec-dem involved participants working in groups to create Music on Ableton live Digital Audio Workstation, using AI based tools created by the TheKlong studio    \n\n     Year          Dataset  \n0    2022            ArXiV  \n1    2022            ArXiV  \n2    2022            ArXiV  \n3    2022            ArXiV  \n4    2022            ArXiV  \n..    ...              ...  \n277  2023  Music_2022_2023  \n278  2023  Music_2022_2023  \n279  2023  Music_2022_2023  \n280  2023  Music_2022_2023  \n281  2023  Music_2022_2023  \n\n[282 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Authors</th>\n      <th>Title</th>\n      <th>Abstract</th>\n      <th>Year</th>\n      <th>Dataset</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Carlos Hernandez-Olivan, Jose R. Beltran</td>\n      <td>musicaiz A Python Library for Symbolic Music Generation Analysis and  Visualization</td>\n      <td>In this article we present musicaiz an objectoriented library foranalyzing generating and evaluating symbolic music The submodules of thepackage allow the user to create symbolic music data from scratch buildalgorithms to analyze symbolic music encode MIDI data as tokens to train deeplearning sequence models modify existing music data and evaluate musicgeneration systems The evaluation submodule builds on previous work toobjectively measure music generation systems and to be able to reproduce theresults of music generation models The library is publicly available onlineWe encourage the community to contribute and provide feedback</td>\n      <td>2022</td>\n      <td>ArXiV</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Zhihuan Kuang, Shi Zong, Jianbing Zhang, Jiajun Chen, Hongfu Liu</td>\n      <td>MusictoText Synaesthesia Generating Descriptive Text from Music  Recordings</td>\n      <td>In this paper we consider a novel research problem musictotextsynaesthesia Different from the classical music tagging problem thatclassifies a music recording into predefined categories the musictotextsynaesthesia aims to generate descriptive texts from music recordings forfurther understanding Although this is a new and interesting application tothe machine learning community to our best knowledge the existingmusicrelated datasets do not contain the semantic descriptions on musicrecordings and cannot serve the musictotext synaesthesia task In light ofthis we collect a new dataset that contains  aligned pairs of classicalmusic recordings and text descriptions Based on this we build a computationalmodel to generate sentences that can describe the content of the musicrecording To tackle the highly nondiscriminative classical music we design agroup topologypreservation loss in our computational model which considersmore samples as a group reference and preserves the relative topology amongdifferent samples Extensive experimental results qualitatively andquantitatively demonstrate the effectiveness of our proposed model over fiveheuristics or pretrained competitive methods and their variants on ourcollected dataset</td>\n      <td>2022</td>\n      <td>ArXiV</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Yi Luo, Jianwei Yu</td>\n      <td>Music Source Separation with Bandsplit RNN</td>\n      <td>The performance of music source separation MSS models has been greatlyimproved in recent years thanks to the development of novel neural networkarchitectures and training pipelines However recent model designs for MSSwere mainly motivated by other audio processing tasks or other research fieldswhile the intrinsic characteristics and patterns of the music signals were notfully discovered In this paper we propose bandsplit RNN BSRNN afrequencydomain model that explictly splits the spectrogram of the mixtureinto subbands and perform interleaved bandlevel and sequencelevel modelingThe choices of the bandwidths of the subbands can be determined by a prioriknowledge or expert knowledge on the characteristics of the target source inorder to optimize the performance on a certain type of target musicalinstrument To better make use of unlabeled data we also describe asemisupervised model finetuning pipeline that can further improve theperformance of the model Experiment results show that BSRNN trained only onMUSDBHQ dataset significantly outperforms several topranking models inMusic Demixing MDX Challenge  and the semisupervised finetuning stagefurther improves the performance on all four instrument tracks</td>\n      <td>2022</td>\n      <td>ArXiV</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Yusong Wu, Josh Gardner, Ethan Manilow, Ian Simon, Curtis Hawthorne,\\n  Jesse Engel</td>\n      <td>The Chamber Ensemble Generator Limitless HighQuality MIR Data via  Generative Modeling</td>\n      <td>Data is the lifeblood of modern machine learning systems including for thosein Music Information Retrieval MIR However MIR has long been mired by smalldatasets and unreliable labels In this work we propose to break thisbottleneck using generative modeling By pipelining a generative model of notesCoconet trained on Bach Chorales with a structured synthesis model of chamberensembles MIDIDDSP trained on URMP we demonstrate a system capable ofproducing unlimited amounts of realistic chorale music with rich annotationsincluding mixes stems MIDI notelevel performance attributes staccatovibrato etc and even finegrained synthesis parameters pitch amplitudeetc We call this system the Chamber Ensemble Generator CEG and use it togenerate a large dataset of chorales from four different chamber ensemblesCocoChorales We demonstrate that data generated using our approach improvesstateoftheart models for music transcription and source separation and werelease both the system and the dataset as an opensource foundation for futurework in the MIR community</td>\n      <td>2022</td>\n      <td>ArXiV</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Junyan Jiang, Daniel Chin, Yixiao Zhang, Gus Xia</td>\n      <td>Learning Hierarchical Metrical Structure Beyond Measures</td>\n      <td>Music contains hierarchical structures beyond beats and measures Whilehierarchical structure annotations are helpful for music information retrievaland computer musicology such annotations are scarce in current digital musicdatabases In this paper we explore a datadriven approach to automaticallyextract hierarchical metrical structures from scores We propose a new modelwith a Temporal Convolutional NetworkConditional Random Field TCNCRFarchitecture Given a symbolic music score our model takes in an arbitrarynumber of voices in a beatquantized form and predicts a level hierarchicalmetrical structure from downbeatlevel to sectionlevel We also annotate adataset using RWCPOP MIDI files to facilitate training and evaluation We showby experiments that the proposed method performs better than the rulebasedapproach under different orchestration settings We also perform some simplemusicological analysis on the model predictions All demos datasets andpretrained models are publicly available on Github</td>\n      <td>2022</td>\n      <td>ArXiV</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>277</th>\n      <td>277</td>\n      <td>A Kumar, P Sarmento</td>\n      <td>From Words to Music: A Study of Subword Tokenization Techniques in Symbolic Music Generation</td>\n      <td>of the music becomes complex such as in the case of polyphonic music or multi-track music. A  The jazz transformer on the front line: Exploring the shortcomings of ai-composed music</td>\n      <td>2023</td>\n      <td>Music_2022_2023</td>\n    </tr>\n    <tr>\n      <th>278</th>\n      <td>278</td>\n      <td>D von Rütte, L Biggio, Y Kilcher</td>\n      <td>FIGARO: Controllable Music Generation using Learned and Expert Features</td>\n      <td>to music generation. We aim to extend this kind of control to other domains, in this case to music  In this regard, we believe that our contribution is a step toward human-AI collaboration</td>\n      <td>2023</td>\n      <td>Music_2022_2023</td>\n    </tr>\n    <tr>\n      <th>279</th>\n      <td>279</td>\n      <td>S Colton, S Cardinale</td>\n      <td>Extending Generative Neo-Riemannian Theory for Event-Based Soundtrack Production</td>\n      <td>of music, but we provide a generative formalism, which can be employed to make music, as  and making the music between events suitably low-key, so the event music is not obfuscated</td>\n      <td>2023</td>\n      <td>Music_2022_2023</td>\n    </tr>\n    <tr>\n      <th>280</th>\n      <td>280</td>\n      <td>K Tan, H Liu, S Huang, C Li</td>\n      <td>Efficacy of Music Intervention for Dental Anxiety Disorders: A Systematic Review and Meta-Analysis</td>\n      <td>in S-AI between the preoperative experimental and control patients (I 2 = 0%, WMD (95% CI): 0.82(−0.73, 2.37), p = 0.30); postoperatively, music therapy significantly reduced S-AI in</td>\n      <td>2023</td>\n      <td>Music_2022_2023</td>\n    </tr>\n    <tr>\n      <th>281</th>\n      <td>281</td>\n      <td>U Kalidas</td>\n      <td>\" 12\" Music Visualisation: Exploring Generative and Non-Generative techniques within a rhythmic framework in a performative spatial setting.</td>\n      <td>The two day workshop/ lec-dem involved participants working in groups to create Music on Ableton live Digital Audio Workstation, using AI based tools created by the TheKlong studio</td>\n      <td>2023</td>\n      <td>Music_2022_2023</td>\n    </tr>\n  </tbody>\n</table>\n<p>282 rows × 6 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"dfFinalEnglishPapersB42021 = pd.read_csv('/kaggle/input/Input/Music_B42021_final.csv')","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:59.917513Z","iopub.execute_input":"2023-06-29T23:15:59.918029Z","iopub.status.idle":"2023-06-29T23:15:59.953287Z","shell.execute_reply.started":"2023-06-29T23:15:59.917949Z","shell.execute_reply":"2023-06-29T23:15:59.951857Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"dfFinalEnglishPapersB42021","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:59.955104Z","iopub.execute_input":"2023-06-29T23:15:59.955495Z","iopub.status.idle":"2023-06-29T23:15:59.976918Z","shell.execute_reply.started":"2023-06-29T23:15:59.955465Z","shell.execute_reply":"2023-06-29T23:15:59.975093Z"},"trusted":true},"execution_count":110,"outputs":[{"execution_count":110,"output_type":"execute_result","data":{"text/plain":"     Unnamed: 0  \\\n0             0   \n1             1   \n2             2   \n3             3   \n4             4   \n..          ...   \n270         270   \n271         271   \n272         272   \n273         273   \n274         274   \n\n                                                                                     Authors  \\\n0                                       E Idrobo-Ávilaa, H Loaiza-Correaa, F Muñoz-Bolañosb…   \n1                                                     SJ Niharika, N Nityashree, RS Rachana…   \n2                                                                                     S Lowe   \n3                                                                      W Sun, R Sundarasekar   \n4                                                                                     L Wang   \n..                                                                                       ...   \n270                                                             Gunjan Aggarwal, Devi Parikh   \n271                             Hugo Flores Garcia, Aldo Aguilar, Ethan Manilow, Bryan Pardo   \n272  Mila Soares de Oliveira de Souza and Pedro Nuno de Souza Moura and\\n  Jean-Pierre Briot   \n273                                                                Ning Zhang and Junchi Yan   \n274                                Martin Strauss, Jouni Paulus, Matteo Torcoli, Bernd Edler   \n\n                                                                                                                                Title  \\\n0                                                      Heart Response to Harmonic Music Interval Stimuli Via Deep Learning Structures   \n1                                                                    Music Generation using Deep Learning and Implementation on Piano   \n2                                                               A Fully-Unsupervised Generative Method for Choreo-Musical Translation   \n3    Research on pattern recognition of different music types in the context of AI with the help of multimedia information processing   \n4                                 The Mathematical Analysis Model of Educational System in Music Courses in Colleges and Universities   \n..                                                                                                                                ...   \n270                                                                                 DanceMusic Automatic Dancedriven Music Generation   \n271                                                    Leveraging Hierarchical Structures for FewShot Musical Instrument  Recognition   \n272                                                                Music Tempo Estimation via Neural Networks  A Comparative Analysis   \n273                                                  Melody Structure Transfer Network Generating Music with Separable  SelfAttention   \n274                          A Handson Comparison of DNNs for Dialog Separation Using Transfer  Learning from Music Source Separation   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Abstract  \\\n0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       … Artificial intelligence techniques have recently been … on the heart of harmonic musical intervals and colored noise. … it could be incorporated into future music perception research. …   \n1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        … make AI creative. By learning to generate music through deep learning models, we can make AI … In this paper, we present the use of the LSTM model architecture to generate music. The …   \n2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    … It is clear that AI systems applied to these tasks need similar … Our algorithm will be able to take as input either music (… that represents choreography and music that should be paired …   \n3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    … to use artificial intelligence for algorithmic music by incorporating AI techniques as the primary compositional generator. Artificial intelligence models used in composing music include …   \n4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           … An experiment testing expectancy violation theory and AI music. new media &society.,2021; … An experiment testing expectancy violation theory and AI music. new media &society 2021 …   \n..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ...   \n270                                                                                                                                                                                                                                                                                                                                                                                                                           Dance and music typically go hand in hand The complexities in dance musicand their synchronisation make them fascinating to study from a computationalcreativity perspective While several works have looked at generating dance fora given music automatically generating music for a given dance remainsunderexplored This capability could have several creative expression andentertainment applications We present some early explorations in thisdirection We present a searchbased offline approach that generates musicafter processing the entire dance video and an online approach that uses a deepneural network to generate music onthefly as the video proceeds We comparethese approaches to a strong heuristic baseline via human studies and presentour findings We have integrated our online approach in a live demo A video ofthe demo can be found herehttpssitesgooglecomviewdancemusiclivedemo   \n271                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Deep learning work on musical instrument recognition has generally focused oninstrument classes for which we have abundant data In this work we exploithierarchical relationships between instruments in a fewshot learning setup toenable classification of a wider set of musical instruments given a fewexamples at inference We apply a hierarchical loss function to the training ofprototypical networks combined with a method to aggregate prototypeshierarchically mirroring the structure of a predefined musical instrumenthierarchy These extensions require no changes to the network architecture andnew levels can be easily added or removed Compared to a nonhierarchicalfewshot baseline our method leads to a significant increase in classificationaccuracy and significant decrease mistake severity on instrument classes unseenin training   \n272                                                                                                                                                                                                        This paper presents a comparative analysis on two artificial neural networkswith different architectures for the task of tempo estimation For thispurpose it also proposes the modeling training and evaluation of a BRNNBidirectional Recurrent Neural Network model capable of estimating tempo inbpm beats per minutes of musical pieces without using external auxiliarymodules An extensive database  pieces in total was curated to conducta quantitative and qualitative analysis over the experiment Percussiononlytracks were also included in the dataset The performance of the BRNN iscompared to that of stateoftheart models For further comparison astateoftheart CNN was also retrained with the same datasets used for theBRNN training Evaluation results for each model and datasets are presentedand discussed as well as observations and ideas for future research Tempoestimation was more accurate for the percussion only dataset suggesting thatthe estimation can be more accurate for percussiononly tracks althoughfurther experiments with more of such datasets should be made to gatherstronger evidence   \n273                                                                                                                                                                                                                                                                                                              Symbolic music generation has attracted increasing attention while mostmethods focus on generating short piece mostly less than  bars and up to bars Generating long music calls for effective expression of the coherentmusic structure Despite their success on long sequences selfattentionarchitectures still have challenge in dealing with longterm music as itrequires additional care on the subtle music structure In this paper wepropose to transfer the structure of training samples for new music generationand develop a novel separable selfattention based model which enable thelearning and transferring of the structure embedding We show that our transfermodel can generate music sequences up to  bars with interpretablestructures which bears similar structures and composition techniques with thetemplate music from training set Extensive experiments show its ability ofgenerating music with target structure and well diversity The generated sets of music is uploaded as supplemental material   \n274  This paper describes a handson comparison on using stateoftheart musicsource separation deep neural networks DNNs before and after taskspecificfinetuning for separating speech content from nonspeech content in broadcastaudio ie dialog separation The music separation models are selected asthey share the number of channels  and sampling rate  kHz or higherwith the considered broadcast content and vocals separation in music isconsidered as a parallel for dialog separation in the target applicationdomain These similarities are assumed to enable transfer learning between thetasks Three models pretrained on music OpenUnmix Spleeter andConvTasNet are considered in the experiments and finetuned with realbroadcast data The performance of the models is evaluated before and afterfinetuning with computational evaluation metrics SISIRi SISDRi fmodelas well as with a listening test simulating an application where the nonspeechsignal is partially attenuated eg for better speech intelligibility Theevaluations include two reference systems specifically developed for dialogseparation The results indicate that pretrained music source separationmodels can be used for dialog separation to some degree and that they benefitfrom the finetuning reaching a performance close to taskspecific solutions   \n\n     Year          Dataset  \n0       0  Music_2022_2023  \n1       0  Music_2022_2023  \n2       0  Music_2022_2023  \n3       0  Music_2022_2023  \n4       0  Music_2022_2023  \n..    ...              ...  \n270  2021            ArXiV  \n271  2021            ArXiV  \n272  2021            ArXiV  \n273  2021            ArXiV  \n274  2021            ArXiV  \n\n[275 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Authors</th>\n      <th>Title</th>\n      <th>Abstract</th>\n      <th>Year</th>\n      <th>Dataset</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>E Idrobo-Ávilaa, H Loaiza-Correaa, F Muñoz-Bolañosb…</td>\n      <td>Heart Response to Harmonic Music Interval Stimuli Via Deep Learning Structures</td>\n      <td>… Artificial intelligence techniques have recently been … on the heart of harmonic musical intervals and colored noise. … it could be incorporated into future music perception research. …</td>\n      <td>0</td>\n      <td>Music_2022_2023</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>SJ Niharika, N Nityashree, RS Rachana…</td>\n      <td>Music Generation using Deep Learning and Implementation on Piano</td>\n      <td>… make AI creative. By learning to generate music through deep learning models, we can make AI … In this paper, we present the use of the LSTM model architecture to generate music. The …</td>\n      <td>0</td>\n      <td>Music_2022_2023</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>S Lowe</td>\n      <td>A Fully-Unsupervised Generative Method for Choreo-Musical Translation</td>\n      <td>… It is clear that AI systems applied to these tasks need similar … Our algorithm will be able to take as input either music (… that represents choreography and music that should be paired …</td>\n      <td>0</td>\n      <td>Music_2022_2023</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>W Sun, R Sundarasekar</td>\n      <td>Research on pattern recognition of different music types in the context of AI with the help of multimedia information processing</td>\n      <td>… to use artificial intelligence for algorithmic music by incorporating AI techniques as the primary compositional generator. Artificial intelligence models used in composing music include …</td>\n      <td>0</td>\n      <td>Music_2022_2023</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>L Wang</td>\n      <td>The Mathematical Analysis Model of Educational System in Music Courses in Colleges and Universities</td>\n      <td>… An experiment testing expectancy violation theory and AI music. new media &amp;society.,2021; … An experiment testing expectancy violation theory and AI music. new media &amp;society 2021 …</td>\n      <td>0</td>\n      <td>Music_2022_2023</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>270</th>\n      <td>270</td>\n      <td>Gunjan Aggarwal, Devi Parikh</td>\n      <td>DanceMusic Automatic Dancedriven Music Generation</td>\n      <td>Dance and music typically go hand in hand The complexities in dance musicand their synchronisation make them fascinating to study from a computationalcreativity perspective While several works have looked at generating dance fora given music automatically generating music for a given dance remainsunderexplored This capability could have several creative expression andentertainment applications We present some early explorations in thisdirection We present a searchbased offline approach that generates musicafter processing the entire dance video and an online approach that uses a deepneural network to generate music onthefly as the video proceeds We comparethese approaches to a strong heuristic baseline via human studies and presentour findings We have integrated our online approach in a live demo A video ofthe demo can be found herehttpssitesgooglecomviewdancemusiclivedemo</td>\n      <td>2021</td>\n      <td>ArXiV</td>\n    </tr>\n    <tr>\n      <th>271</th>\n      <td>271</td>\n      <td>Hugo Flores Garcia, Aldo Aguilar, Ethan Manilow, Bryan Pardo</td>\n      <td>Leveraging Hierarchical Structures for FewShot Musical Instrument  Recognition</td>\n      <td>Deep learning work on musical instrument recognition has generally focused oninstrument classes for which we have abundant data In this work we exploithierarchical relationships between instruments in a fewshot learning setup toenable classification of a wider set of musical instruments given a fewexamples at inference We apply a hierarchical loss function to the training ofprototypical networks combined with a method to aggregate prototypeshierarchically mirroring the structure of a predefined musical instrumenthierarchy These extensions require no changes to the network architecture andnew levels can be easily added or removed Compared to a nonhierarchicalfewshot baseline our method leads to a significant increase in classificationaccuracy and significant decrease mistake severity on instrument classes unseenin training</td>\n      <td>2021</td>\n      <td>ArXiV</td>\n    </tr>\n    <tr>\n      <th>272</th>\n      <td>272</td>\n      <td>Mila Soares de Oliveira de Souza and Pedro Nuno de Souza Moura and\\n  Jean-Pierre Briot</td>\n      <td>Music Tempo Estimation via Neural Networks  A Comparative Analysis</td>\n      <td>This paper presents a comparative analysis on two artificial neural networkswith different architectures for the task of tempo estimation For thispurpose it also proposes the modeling training and evaluation of a BRNNBidirectional Recurrent Neural Network model capable of estimating tempo inbpm beats per minutes of musical pieces without using external auxiliarymodules An extensive database  pieces in total was curated to conducta quantitative and qualitative analysis over the experiment Percussiononlytracks were also included in the dataset The performance of the BRNN iscompared to that of stateoftheart models For further comparison astateoftheart CNN was also retrained with the same datasets used for theBRNN training Evaluation results for each model and datasets are presentedand discussed as well as observations and ideas for future research Tempoestimation was more accurate for the percussion only dataset suggesting thatthe estimation can be more accurate for percussiononly tracks althoughfurther experiments with more of such datasets should be made to gatherstronger evidence</td>\n      <td>2021</td>\n      <td>ArXiV</td>\n    </tr>\n    <tr>\n      <th>273</th>\n      <td>273</td>\n      <td>Ning Zhang and Junchi Yan</td>\n      <td>Melody Structure Transfer Network Generating Music with Separable  SelfAttention</td>\n      <td>Symbolic music generation has attracted increasing attention while mostmethods focus on generating short piece mostly less than  bars and up to bars Generating long music calls for effective expression of the coherentmusic structure Despite their success on long sequences selfattentionarchitectures still have challenge in dealing with longterm music as itrequires additional care on the subtle music structure In this paper wepropose to transfer the structure of training samples for new music generationand develop a novel separable selfattention based model which enable thelearning and transferring of the structure embedding We show that our transfermodel can generate music sequences up to  bars with interpretablestructures which bears similar structures and composition techniques with thetemplate music from training set Extensive experiments show its ability ofgenerating music with target structure and well diversity The generated sets of music is uploaded as supplemental material</td>\n      <td>2021</td>\n      <td>ArXiV</td>\n    </tr>\n    <tr>\n      <th>274</th>\n      <td>274</td>\n      <td>Martin Strauss, Jouni Paulus, Matteo Torcoli, Bernd Edler</td>\n      <td>A Handson Comparison of DNNs for Dialog Separation Using Transfer  Learning from Music Source Separation</td>\n      <td>This paper describes a handson comparison on using stateoftheart musicsource separation deep neural networks DNNs before and after taskspecificfinetuning for separating speech content from nonspeech content in broadcastaudio ie dialog separation The music separation models are selected asthey share the number of channels  and sampling rate  kHz or higherwith the considered broadcast content and vocals separation in music isconsidered as a parallel for dialog separation in the target applicationdomain These similarities are assumed to enable transfer learning between thetasks Three models pretrained on music OpenUnmix Spleeter andConvTasNet are considered in the experiments and finetuned with realbroadcast data The performance of the models is evaluated before and afterfinetuning with computational evaluation metrics SISIRi SISDRi fmodelas well as with a listening test simulating an application where the nonspeechsignal is partially attenuated eg for better speech intelligibility Theevaluations include two reference systems specifically developed for dialogseparation The results indicate that pretrained music source separationmodels can be used for dialog separation to some degree and that they benefitfrom the finetuning reaching a performance close to taskspecific solutions</td>\n      <td>2021</td>\n      <td>ArXiV</td>\n    </tr>\n  </tbody>\n</table>\n<p>275 rows × 6 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"music_Transformer_keywords = [\"Transformer\", \"generative\",\"generation\", \"generate\", \"generation\"]","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:59.979567Z","iopub.execute_input":"2023-06-29T23:15:59.980134Z","iopub.status.idle":"2023-06-29T23:15:59.987625Z","shell.execute_reply.started":"2023-06-29T23:15:59.980089Z","shell.execute_reply":"2023-06-29T23:15:59.986352Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"markdown","source":"dfFinalEnglishPapers_2022_2023","metadata":{}},{"cell_type":"code","source":"df_transformers_2022_2023 = dfFinalEnglishPapers_2022_2023[ dfFinalEnglishPapers_2022_2023['Title'].apply(lambda abstract: any(keyword in abstract for keyword in music_Transformer_keywords))|\n    dfFinalEnglishPapers_2022_2023['Abstract'].apply(lambda abstract: any(keyword in abstract for keyword in music_Transformer_keywords))]","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:15:59.989656Z","iopub.execute_input":"2023-06-29T23:15:59.990035Z","iopub.status.idle":"2023-06-29T23:16:00.014807Z","shell.execute_reply.started":"2023-06-29T23:15:59.990005Z","shell.execute_reply":"2023-06-29T23:16:00.012358Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"df_transformers_2022_2023.shape","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:16:00.017254Z","iopub.execute_input":"2023-06-29T23:16:00.017727Z","iopub.status.idle":"2023-06-29T23:16:00.026740Z","shell.execute_reply.started":"2023-06-29T23:16:00.017695Z","shell.execute_reply":"2023-06-29T23:16:00.025602Z"},"trusted":true},"execution_count":113,"outputs":[{"execution_count":113,"output_type":"execute_result","data":{"text/plain":"(124, 6)"},"metadata":{}}]},{"cell_type":"code","source":"dfTransformers_B42021 = dfFinalEnglishPapersB42021[ dfFinalEnglishPapersB42021['Title'].apply(lambda abstract: any(keyword in abstract for keyword in music_Transformer_keywords))|\n    dfFinalEnglishPapersB42021['Abstract'].apply(lambda abstract: any(keyword in abstract for keyword in music_Transformer_keywords))]","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:16:00.028023Z","iopub.execute_input":"2023-06-29T23:16:00.028406Z","iopub.status.idle":"2023-06-29T23:16:00.044305Z","shell.execute_reply.started":"2023-06-29T23:16:00.028374Z","shell.execute_reply":"2023-06-29T23:16:00.043081Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"dfTransformers_B42021.shape","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:16:47.886839Z","iopub.execute_input":"2023-06-29T23:16:47.887364Z","iopub.status.idle":"2023-06-29T23:16:47.897717Z","shell.execute_reply.started":"2023-06-29T23:16:47.887328Z","shell.execute_reply":"2023-06-29T23:16:47.896211Z"},"trusted":true},"execution_count":117,"outputs":[{"execution_count":117,"output_type":"execute_result","data":{"text/plain":"(147, 6)"},"metadata":{}}]},{"cell_type":"code","source":"others_keywords = [\"LSTM\", \"GAN\", \"RNN\", \"GRU\", \"VAE\", \"AutoEncoder\"] # Add more models as needed\n# dfFinalEnglishPapersB42021","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:16:47.902724Z","iopub.execute_input":"2023-06-29T23:16:47.903838Z","iopub.status.idle":"2023-06-29T23:16:47.916202Z","shell.execute_reply.started":"2023-06-29T23:16:47.903797Z","shell.execute_reply":"2023-06-29T23:16:47.915146Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"#LSTM in 2022_2023\ndf_others_2022_2023 = dfFinalEnglishPapers_2022_2023[ dfFinalEnglishPapers_2022_2023['Title'].apply(lambda abstract: any(keyword in abstract for keyword in others_keywords))|\n    dfFinalEnglishPapers_2022_2023['Abstract'].apply(lambda abstract: any(keyword in abstract for keyword in others_keywords))]","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:16:47.922473Z","iopub.execute_input":"2023-06-29T23:16:47.923476Z","iopub.status.idle":"2023-06-29T23:16:47.936252Z","shell.execute_reply.started":"2023-06-29T23:16:47.923413Z","shell.execute_reply":"2023-06-29T23:16:47.935262Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":"df_others_2022_2023.shape","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:16:47.938685Z","iopub.execute_input":"2023-06-29T23:16:47.940090Z","iopub.status.idle":"2023-06-29T23:16:47.950312Z","shell.execute_reply.started":"2023-06-29T23:16:47.940033Z","shell.execute_reply":"2023-06-29T23:16:47.948891Z"},"trusted":true},"execution_count":120,"outputs":[{"execution_count":120,"output_type":"execute_result","data":{"text/plain":"(28, 6)"},"metadata":{}}]},{"cell_type":"code","source":"#LSTM in B42021\ndf_others_B4_2021 = dfFinalEnglishPapersB42021[ dfFinalEnglishPapersB42021['Title'].apply(lambda abstract: any(keyword in abstract for keyword in others_keywords))|\n    dfFinalEnglishPapersB42021['Abstract'].apply(lambda abstract: any(keyword in abstract for keyword in others_keywords))]","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:16:47.952201Z","iopub.execute_input":"2023-06-29T23:16:47.953079Z","iopub.status.idle":"2023-06-29T23:16:47.968446Z","shell.execute_reply.started":"2023-06-29T23:16:47.953020Z","shell.execute_reply":"2023-06-29T23:16:47.966584Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"#LSTM in B42021\ndf_others_B42021 = dfFinalEnglishPapersB42021[ dfFinalEnglishPapersB42021['Title'].apply(lambda abstract: any(keyword in abstract for keyword in others_keywords))|\n    dfFinalEnglishPapersB42021['Abstract'].apply(lambda abstract: any(keyword in abstract for keyword in others_keywords))]","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:16:47.971781Z","iopub.execute_input":"2023-06-29T23:16:47.972258Z","iopub.status.idle":"2023-06-29T23:16:47.988495Z","shell.execute_reply.started":"2023-06-29T23:16:47.972225Z","shell.execute_reply":"2023-06-29T23:16:47.986868Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"df_others_B42021","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:16:47.991213Z","iopub.execute_input":"2023-06-29T23:16:47.992216Z","iopub.status.idle":"2023-06-29T23:16:48.029044Z","shell.execute_reply.started":"2023-06-29T23:16:47.992179Z","shell.execute_reply":"2023-06-29T23:16:48.027716Z"},"trusted":true},"execution_count":123,"outputs":[{"execution_count":123,"output_type":"execute_result","data":{"text/plain":"     Unnamed: 0  \\\n1             1   \n26           26   \n36           36   \n38           38   \n45           45   \n..          ...   \n248         248   \n256         256   \n257         257   \n267         267   \n272         272   \n\n                                                                                     Authors  \\\n1                                                     SJ Niharika, N Nityashree, RS Rachana…   \n26                                                          I-Ting Liu, Bhiksha Ramakrishnan   \n36                                                       Vasanth Kalingeri, Srikanth Grandhe   \n38                                                Mason Bretan, Gil Weinberg, and Larry Heck   \n45                                                Keunwoo Choi, George Fazekas, Mark Sandler   \n..                                                                                       ...   \n248                                    Tun-Min Hung, Bo-Yu Chen, Yen-Tung Yeh, Yi-Hsuan Yang   \n256                                                    Jin Li, Haibin Liu, Nan Yan, Lan Wang   \n257                                                              Ashis Pati, Alexander Lerch   \n267            Vaishali Ingale, Anush Mohan, Divit Adlakha, Krishan Kumar and Mohit\\n  Gupta   \n272  Mila Soares de Oliveira de Souza and Pedro Nuno de Souza Moura and\\n  Jean-Pierre Briot   \n\n                                                                                            Title  \\\n1                                Music Generation using Deep Learning and Implementation on Piano   \n26                                       Bach in  Music Composition with Recurrent Neural Network   \n36                                                            Music Generation with Deep Learning   \n38                  A Unit Selection Methodology for Music Generation Using Deep Neural  Networks   \n45                                        Textbased LSTM networks for Automatic Music Composition   \n..                                                                                            ...   \n248  A Benchmarking Initiative for AudioDomain Music Generation Using the  Freesound Loop Dataset   \n256            Enhanced Memory Network The novel network structure for Symbolic Music  Generation   \n257        Is Disentanglement enough On Latent Representations for Controllable  Music Generation   \n267                                                      Music Generation using Threelayered LSTM   \n272                            Music Tempo Estimation via Neural Networks  A Comparative Analysis   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Abstract  \\\n1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         … make AI creative. By learning to generate music through deep learning models, we can make AI … In this paper, we present the use of the LSTM model architecture to generate music. The …   \n26                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   We propose a framework for computer music composition that uses resilientpropagation RProp and long short term memory LSTM recurrent neural networkIn this paper we show that LSTM network learns the structure andcharacteristics of music pieces properly by demonstrating its ability torecreate music We also show that predicting existing music using RPropoutperforms Back propagation through time BPTT   \n36                                                                                                                                                                                                                               The use of deep learning to solve problems in literary arts has been a recenttrend that has gained a lot of attention and automated generation of music hasbeen an active area This project deals with the generation of music using rawaudio files in the frequency domain relying on various LSTM architecturesFully connected and convolutional layers are used along with LSTMs to capturerich features in the frequency domain and increase the quality of musicgenerated The work is focused on unconstrained music generation and uses noinformation about musical structurenotes or chords to aid learningThe musicgenerated from various architectures are compared using blind fold tests Usingthe raw audio to train models is the direction to tapping the enormous amountof mp files that exist over the internet without requiring the manual effortto make structured MIDI files Moreover not all audio files can be representedwith MIDI files making the study of these models an interesting prospect to thefuture of such models   \n38                                                                                                        Several methods exist for a computer to generate music based on dataincluding Markov chains recurrent neural networks recombinancy and grammarsWe explore the use of unit selection and concatenation as a means of generatingmusic using a procedure based on ranking where we consider a unit to be avariable length number of measures of music We first examine whether a unitselection method that is restricted to a finite size unit library can besufficient for encompassing a wide spectrum of music We do this by developinga deep autoencoder that encodes a musical input and reconstructs the input byselecting from the library We then describe a generative model that combines adeep structured semantic model DSSM with an LSTM to predict the next unitwhere units consist of four two and one measures of music We evaluate thegenerative model using objective metrics including mean rank and accuracy andwith a subjective listening test in which expert musicians are asked tocomplete a forcedchoiced ranking task We compare our model to a notelevelgenerative baseline that consists of a stacked LSTM trained to predict forwardby one note   \n45                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        In this paper we introduce new methods and discuss results of textbasedLSTM Long ShortTerm Memory networks for automatic music composition Theproposed network is designed to learn relationships within text documents thatrepresent chord progressions and drum tracks in two case studies In theexperiments wordRNNs Recurrent Neural Networks show good results for bothcases while characterbased RNNs charRNNs only succeed to learn chordprogressions The proposed system can be used for fully automatic compositionor as semiautomatic systems that help humans to compose music by controlling adiversity parameter of the model   \n..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ...   \n248                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       This paper proposes a new benchmark task for generating musical passages inthe audio domain by using thedrum loops from the FreeSound Loop Dataset whicharepublicly redistributable Moreover we use a larger collection of drumloops from Looperman to establish fourmodelbased objective metrics forevaluation releasingthese metrics as a library for quantifying andfacilitatingthe progress of musical audio generation Under this evaluationframework we benchmark the performance of threerecent deep generativeadversarial network GAN models we customize to generate loops includingStyleGANStyleGAN and UNAGAN We also report a subjectiveevaluation of thesemodels Our evaluation shows that theone based on StyleGAN performs the bestin both objective and subjective metrics   \n256  Symbolic melodies generation is one of the essential tasks for automaticmusic generation Recently models based on neural networks have had asignificant influence on generating symbolic melodies However the musicalcontext structure is complicated to capture through deep neural networksAlthough long shortterm memory LSTM is attempted to solve this problemthrough learning order dependence in the musical sequence it is not capable ofcapturing musical context with only one note as input for each time step ofLSTM In this paper we propose a novel Enhanced Memory Network EMN withseveral recurrent units named Enhanced Memory Unit EMU to explicitly modifythe internal architecture of LSTM for containing music beat information andreinforces the memory of the latest musical beat through aggregating beatinside the memory gate In addition to increase the diversity of generatedmusical notes cosine distance among adjacent time steps of hidden states isconsidered as part of loss functions to avoid a high similarity score thatharms the diversity of generated notes Objective and subjective evaluationresults show that the proposed method achieves stateoftheart performanceCode and music demo are available at httpsgithubcomqrqrqrqrEMU   \n257                                                                                Improving controllability or the ability to manipulate one or more attributesof the generated data has become a topic of interest in the context of deepgenerative models of music Recent attempts in this direction have relied onlearning disentangled representations from data such that the underlyingfactors of variation are well separated In this paper we focus on therelationship between disentanglement and controllability by conducting asystematic study using different supervised disentanglement learning algorithmsbased on the Variational AutoEncoder VAE architecture Our experiments showthat a high degree of disentanglement can be achieved by using different formsof supervision to train a strong discriminative encoder However in theabsence of a strong generative decoder disentanglement does not necessarilyimply controllability The structure of the latent space with respect to theVAEdecoder plays an important role in boosting the ability of a generativemodel to manipulate different attributes To this end we also propose methodsand metrics to help evaluate the quality of a latent space with respect to theafforded degree of controllability   \n267                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       This paper explores the idea of utilising Long ShortTerm Memory neuralnetworks LSTMNN for the generation of musical sequences in ABC notation Theproposed approach takes ABC notations from the Nottingham dataset and encodesit to be fed as input for the neural networks The primary objective is toinput the neural networks with an arbitrary note let the network process andaugment a sequence based on the note until a good piece of music is producedMultiple calibrations have been done to amend the parameters of the network foroptimal generation The output is assessed on the basis of rhythm harmony andgrammar accuracy   \n272                                                                                                                                         This paper presents a comparative analysis on two artificial neural networkswith different architectures for the task of tempo estimation For thispurpose it also proposes the modeling training and evaluation of a BRNNBidirectional Recurrent Neural Network model capable of estimating tempo inbpm beats per minutes of musical pieces without using external auxiliarymodules An extensive database  pieces in total was curated to conducta quantitative and qualitative analysis over the experiment Percussiononlytracks were also included in the dataset The performance of the BRNN iscompared to that of stateoftheart models For further comparison astateoftheart CNN was also retrained with the same datasets used for theBRNN training Evaluation results for each model and datasets are presentedand discussed as well as observations and ideas for future research Tempoestimation was more accurate for the percussion only dataset suggesting thatthe estimation can be more accurate for percussiononly tracks althoughfurther experiments with more of such datasets should be made to gatherstronger evidence   \n\n     Year          Dataset  \n1       0  Music_2022_2023  \n26   2014            ArXiV  \n36   2016            ArXiV  \n38   2016            ArXiV  \n45   2016            ArXiV  \n..    ...              ...  \n248  2021            ArXiV  \n256  2021            ArXiV  \n257  2021            ArXiV  \n267  2021            ArXiV  \n272  2021            ArXiV  \n\n[71 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Authors</th>\n      <th>Title</th>\n      <th>Abstract</th>\n      <th>Year</th>\n      <th>Dataset</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>SJ Niharika, N Nityashree, RS Rachana…</td>\n      <td>Music Generation using Deep Learning and Implementation on Piano</td>\n      <td>… make AI creative. By learning to generate music through deep learning models, we can make AI … In this paper, we present the use of the LSTM model architecture to generate music. The …</td>\n      <td>0</td>\n      <td>Music_2022_2023</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>26</td>\n      <td>I-Ting Liu, Bhiksha Ramakrishnan</td>\n      <td>Bach in  Music Composition with Recurrent Neural Network</td>\n      <td>We propose a framework for computer music composition that uses resilientpropagation RProp and long short term memory LSTM recurrent neural networkIn this paper we show that LSTM network learns the structure andcharacteristics of music pieces properly by demonstrating its ability torecreate music We also show that predicting existing music using RPropoutperforms Back propagation through time BPTT</td>\n      <td>2014</td>\n      <td>ArXiV</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>36</td>\n      <td>Vasanth Kalingeri, Srikanth Grandhe</td>\n      <td>Music Generation with Deep Learning</td>\n      <td>The use of deep learning to solve problems in literary arts has been a recenttrend that has gained a lot of attention and automated generation of music hasbeen an active area This project deals with the generation of music using rawaudio files in the frequency domain relying on various LSTM architecturesFully connected and convolutional layers are used along with LSTMs to capturerich features in the frequency domain and increase the quality of musicgenerated The work is focused on unconstrained music generation and uses noinformation about musical structurenotes or chords to aid learningThe musicgenerated from various architectures are compared using blind fold tests Usingthe raw audio to train models is the direction to tapping the enormous amountof mp files that exist over the internet without requiring the manual effortto make structured MIDI files Moreover not all audio files can be representedwith MIDI files making the study of these models an interesting prospect to thefuture of such models</td>\n      <td>2016</td>\n      <td>ArXiV</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>38</td>\n      <td>Mason Bretan, Gil Weinberg, and Larry Heck</td>\n      <td>A Unit Selection Methodology for Music Generation Using Deep Neural  Networks</td>\n      <td>Several methods exist for a computer to generate music based on dataincluding Markov chains recurrent neural networks recombinancy and grammarsWe explore the use of unit selection and concatenation as a means of generatingmusic using a procedure based on ranking where we consider a unit to be avariable length number of measures of music We first examine whether a unitselection method that is restricted to a finite size unit library can besufficient for encompassing a wide spectrum of music We do this by developinga deep autoencoder that encodes a musical input and reconstructs the input byselecting from the library We then describe a generative model that combines adeep structured semantic model DSSM with an LSTM to predict the next unitwhere units consist of four two and one measures of music We evaluate thegenerative model using objective metrics including mean rank and accuracy andwith a subjective listening test in which expert musicians are asked tocomplete a forcedchoiced ranking task We compare our model to a notelevelgenerative baseline that consists of a stacked LSTM trained to predict forwardby one note</td>\n      <td>2016</td>\n      <td>ArXiV</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>45</td>\n      <td>Keunwoo Choi, George Fazekas, Mark Sandler</td>\n      <td>Textbased LSTM networks for Automatic Music Composition</td>\n      <td>In this paper we introduce new methods and discuss results of textbasedLSTM Long ShortTerm Memory networks for automatic music composition Theproposed network is designed to learn relationships within text documents thatrepresent chord progressions and drum tracks in two case studies In theexperiments wordRNNs Recurrent Neural Networks show good results for bothcases while characterbased RNNs charRNNs only succeed to learn chordprogressions The proposed system can be used for fully automatic compositionor as semiautomatic systems that help humans to compose music by controlling adiversity parameter of the model</td>\n      <td>2016</td>\n      <td>ArXiV</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>248</th>\n      <td>248</td>\n      <td>Tun-Min Hung, Bo-Yu Chen, Yen-Tung Yeh, Yi-Hsuan Yang</td>\n      <td>A Benchmarking Initiative for AudioDomain Music Generation Using the  Freesound Loop Dataset</td>\n      <td>This paper proposes a new benchmark task for generating musical passages inthe audio domain by using thedrum loops from the FreeSound Loop Dataset whicharepublicly redistributable Moreover we use a larger collection of drumloops from Looperman to establish fourmodelbased objective metrics forevaluation releasingthese metrics as a library for quantifying andfacilitatingthe progress of musical audio generation Under this evaluationframework we benchmark the performance of threerecent deep generativeadversarial network GAN models we customize to generate loops includingStyleGANStyleGAN and UNAGAN We also report a subjectiveevaluation of thesemodels Our evaluation shows that theone based on StyleGAN performs the bestin both objective and subjective metrics</td>\n      <td>2021</td>\n      <td>ArXiV</td>\n    </tr>\n    <tr>\n      <th>256</th>\n      <td>256</td>\n      <td>Jin Li, Haibin Liu, Nan Yan, Lan Wang</td>\n      <td>Enhanced Memory Network The novel network structure for Symbolic Music  Generation</td>\n      <td>Symbolic melodies generation is one of the essential tasks for automaticmusic generation Recently models based on neural networks have had asignificant influence on generating symbolic melodies However the musicalcontext structure is complicated to capture through deep neural networksAlthough long shortterm memory LSTM is attempted to solve this problemthrough learning order dependence in the musical sequence it is not capable ofcapturing musical context with only one note as input for each time step ofLSTM In this paper we propose a novel Enhanced Memory Network EMN withseveral recurrent units named Enhanced Memory Unit EMU to explicitly modifythe internal architecture of LSTM for containing music beat information andreinforces the memory of the latest musical beat through aggregating beatinside the memory gate In addition to increase the diversity of generatedmusical notes cosine distance among adjacent time steps of hidden states isconsidered as part of loss functions to avoid a high similarity score thatharms the diversity of generated notes Objective and subjective evaluationresults show that the proposed method achieves stateoftheart performanceCode and music demo are available at httpsgithubcomqrqrqrqrEMU</td>\n      <td>2021</td>\n      <td>ArXiV</td>\n    </tr>\n    <tr>\n      <th>257</th>\n      <td>257</td>\n      <td>Ashis Pati, Alexander Lerch</td>\n      <td>Is Disentanglement enough On Latent Representations for Controllable  Music Generation</td>\n      <td>Improving controllability or the ability to manipulate one or more attributesof the generated data has become a topic of interest in the context of deepgenerative models of music Recent attempts in this direction have relied onlearning disentangled representations from data such that the underlyingfactors of variation are well separated In this paper we focus on therelationship between disentanglement and controllability by conducting asystematic study using different supervised disentanglement learning algorithmsbased on the Variational AutoEncoder VAE architecture Our experiments showthat a high degree of disentanglement can be achieved by using different formsof supervision to train a strong discriminative encoder However in theabsence of a strong generative decoder disentanglement does not necessarilyimply controllability The structure of the latent space with respect to theVAEdecoder plays an important role in boosting the ability of a generativemodel to manipulate different attributes To this end we also propose methodsand metrics to help evaluate the quality of a latent space with respect to theafforded degree of controllability</td>\n      <td>2021</td>\n      <td>ArXiV</td>\n    </tr>\n    <tr>\n      <th>267</th>\n      <td>267</td>\n      <td>Vaishali Ingale, Anush Mohan, Divit Adlakha, Krishan Kumar and Mohit\\n  Gupta</td>\n      <td>Music Generation using Threelayered LSTM</td>\n      <td>This paper explores the idea of utilising Long ShortTerm Memory neuralnetworks LSTMNN for the generation of musical sequences in ABC notation Theproposed approach takes ABC notations from the Nottingham dataset and encodesit to be fed as input for the neural networks The primary objective is toinput the neural networks with an arbitrary note let the network process andaugment a sequence based on the note until a good piece of music is producedMultiple calibrations have been done to amend the parameters of the network foroptimal generation The output is assessed on the basis of rhythm harmony andgrammar accuracy</td>\n      <td>2021</td>\n      <td>ArXiV</td>\n    </tr>\n    <tr>\n      <th>272</th>\n      <td>272</td>\n      <td>Mila Soares de Oliveira de Souza and Pedro Nuno de Souza Moura and\\n  Jean-Pierre Briot</td>\n      <td>Music Tempo Estimation via Neural Networks  A Comparative Analysis</td>\n      <td>This paper presents a comparative analysis on two artificial neural networkswith different architectures for the task of tempo estimation For thispurpose it also proposes the modeling training and evaluation of a BRNNBidirectional Recurrent Neural Network model capable of estimating tempo inbpm beats per minutes of musical pieces without using external auxiliarymodules An extensive database  pieces in total was curated to conducta quantitative and qualitative analysis over the experiment Percussiononlytracks were also included in the dataset The performance of the BRNN iscompared to that of stateoftheart models For further comparison astateoftheart CNN was also retrained with the same datasets used for theBRNN training Evaluation results for each model and datasets are presentedand discussed as well as observations and ideas for future research Tempoestimation was more accurate for the percussion only dataset suggesting thatthe estimation can be more accurate for percussiononly tracks althoughfurther experiments with more of such datasets should be made to gatherstronger evidence</td>\n      <td>2021</td>\n      <td>ArXiV</td>\n    </tr>\n  </tbody>\n</table>\n<p>71 rows × 6 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print(\"Papers_2022_2023          : \",dfFinalEnglishPapers_2022_2023.shape)\nprint(\"PapersB42021              : \",dfFinalEnglishPapersB42021.shape)\nprint(\"Transformers_2022_2023    : \",df_transformers_2022_2023.shape)\nprint(\"Transformers_B42021       : \",dfTransformers_B42021.shape)\nprint(\"LSTM etc_2022_2023        : \",df_others_2022_2023.shape)\nprint(\"LSTM etc_B42021           : \",df_others_B42021.shape)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:16:48.030917Z","iopub.execute_input":"2023-06-29T23:16:48.031333Z","iopub.status.idle":"2023-06-29T23:16:48.039720Z","shell.execute_reply.started":"2023-06-29T23:16:48.031302Z","shell.execute_reply":"2023-06-29T23:16:48.037876Z"},"trusted":true},"execution_count":124,"outputs":[{"name":"stdout","text":"Papers_2022_2023          :  (282, 6)\nPapersB42021              :  (275, 6)\nTransformers_2022_2023    :  (124, 6)\nTransformers_B42021       :  (147, 6)\nLSTM etc_2022_2023        :  (28, 6)\nLSTM etc_B42021           :  (71, 6)\n","output_type":"stream"}]},{"cell_type":"code","source":"dfTransformers = pd.concat([df_transformers_2022_2023, dfTransformers_B42021]).drop_duplicates(subset='Title')","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:16:48.041909Z","iopub.execute_input":"2023-06-29T23:16:48.042435Z","iopub.status.idle":"2023-06-29T23:16:48.056808Z","shell.execute_reply.started":"2023-06-29T23:16:48.042394Z","shell.execute_reply":"2023-06-29T23:16:48.055399Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"code","source":"dfTransformers = dfTransformers.drop(columns=\"Unnamed: 0\").reset_index(drop=True)\ndfTransformers.to_csv('/kaggle/working/dfTransformersAllYears.csv', index=True, header=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:16:48.060089Z","iopub.execute_input":"2023-06-29T23:16:48.060478Z","iopub.status.idle":"2023-06-29T23:16:48.086269Z","shell.execute_reply.started":"2023-06-29T23:16:48.060449Z","shell.execute_reply":"2023-06-29T23:16:48.084597Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"dfTransformers = pd.read_csv(\"/kaggle/input/TransformersAllYears/dfTransformersAllYears.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:16:48.088351Z","iopub.execute_input":"2023-06-29T23:16:48.089077Z","iopub.status.idle":"2023-06-29T23:16:48.124608Z","shell.execute_reply.started":"2023-06-29T23:16:48.089036Z","shell.execute_reply":"2023-06-29T23:16:48.123296Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"code","source":"dfTransformers","metadata":{"execution":{"iopub.status.busy":"2023-06-29T23:16:48.126565Z","iopub.execute_input":"2023-06-29T23:16:48.127038Z","iopub.status.idle":"2023-06-29T23:16:48.148617Z","shell.execute_reply.started":"2023-06-29T23:16:48.126973Z","shell.execute_reply":"2023-06-29T23:16:48.147073Z"},"trusted":true},"execution_count":128,"outputs":[{"execution_count":128,"output_type":"execute_result","data":{"text/plain":"     Unnamed: 0  \\\n0             0   \n1             1   \n2             2   \n3             3   \n4             4   \n..          ...   \n266         266   \n267         267   \n268         268   \n269         269   \n270         270   \n\n                                                                                 Authors  \\\n0                                               Carlos Hernandez-Olivan, Jose R. Beltran   \n1                       Zhihuan Kuang, Shi Zong, Jianbing Zhang, Jiajun Chen, Hongfu Liu   \n2    Yusong Wu, Josh Gardner, Ethan Manilow, Ian Simon, Curtis Hawthorne,\\n  Jesse Engel   \n3                                                         Jingwei Zhao, Gus Xia, Ye Wang   \n4                                   Sangjun Han, Hyeongrae Ihm, DaeHan Ahn, Woohyung Lim   \n..                                                                                   ...   \n266                     Pritish Chandna, Ant\\'onio Ramires, Xavier Serra, Emilia G\\'omez   \n267        Vaishali Ingale, Anush Mohan, Divit Adlakha, Krishan Kumar and Mohit\\n  Gupta   \n268           Yi-Jen Shih, Shih-Lun Wu, Frank Zalkow, Meinard M\\\"uller, Yi-Hsuan\\n  Yang   \n269                                                         Gunjan Aggarwal, Devi Parikh   \n270                                                            Ning Zhang and Junchi Yan   \n\n                                                                                                     Title  \\\n0                      musicaiz A Python Library for Symbolic Music Generation Analysis and  Visualization   \n1                              MusictoText Synaesthesia Generating Descriptive Text from Music  Recordings   \n2                   The Chamber Ensemble Generator Limitless HighQuality MIR Data via  Generative Modeling   \n3    Domain Adversarial Training on Conditional Variational AutoEncoder for  Controllable Music Generation   \n4                            Instrument Separation of Symbolic Music by Explicitly Guided Diffusion  Model   \n..                                                                                                     ...   \n266                            LoopNet Musical Loop Synthesis Conditioned On Intuitive Musical  Parameters   \n267                                                               Music Generation using Threelayered LSTM   \n268                         Theme Transformer Symbolic Music Generation with ThemeConditioned  Transformer   \n269                                                      DanceMusic Automatic Dancedriven Music Generation   \n270                       Melody Structure Transfer Network Generating Music with Separable  SelfAttention   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Abstract  \\\n0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            In this article we present musicaiz an objectoriented library foranalyzing generating and evaluating symbolic music The submodules of thepackage allow the user to create symbolic music data from scratch buildalgorithms to analyze symbolic music encode MIDI data as tokens to train deeplearning sequence models modify existing music data and evaluate musicgeneration systems The evaluation submodule builds on previous work toobjectively measure music generation systems and to be able to reproduce theresults of music generation models The library is publicly available onlineWe encourage the community to contribute and provide feedback   \n1                                                                                                                                                                                                                                                                           In this paper we consider a novel research problem musictotextsynaesthesia Different from the classical music tagging problem thatclassifies a music recording into predefined categories the musictotextsynaesthesia aims to generate descriptive texts from music recordings forfurther understanding Although this is a new and interesting application tothe machine learning community to our best knowledge the existingmusicrelated datasets do not contain the semantic descriptions on musicrecordings and cannot serve the musictotext synaesthesia task In light ofthis we collect a new dataset that contains  aligned pairs of classicalmusic recordings and text descriptions Based on this we build a computationalmodel to generate sentences that can describe the content of the musicrecording To tackle the highly nondiscriminative classical music we design agroup topologypreservation loss in our computational model which considersmore samples as a group reference and preserves the relative topology amongdifferent samples Extensive experimental results qualitatively andquantitatively demonstrate the effectiveness of our proposed model over fiveheuristics or pretrained competitive methods and their variants on ourcollected dataset   \n2                                                                                                                                                                                                                                                                                                                                                                                                                                                            Data is the lifeblood of modern machine learning systems including for thosein Music Information Retrieval MIR However MIR has long been mired by smalldatasets and unreliable labels In this work we propose to break thisbottleneck using generative modeling By pipelining a generative model of notesCoconet trained on Bach Chorales with a structured synthesis model of chamberensembles MIDIDDSP trained on URMP we demonstrate a system capable ofproducing unlimited amounts of realistic chorale music with rich annotationsincluding mixes stems MIDI notelevel performance attributes staccatovibrato etc and even finegrained synthesis parameters pitch amplitudeetc We call this system the Chamber Ensemble Generator CEG and use it togenerate a large dataset of chorales from four different chamber ensemblesCocoChorales We demonstrate that data generated using our approach improvesstateoftheart models for music transcription and source separation and werelease both the system and the dataset as an opensource foundation for futurework in the MIR community   \n3                                                                                                                                                                                                       The variational autoencoder has become a leading framework for symbolicmusic generation and a popular research direction is to study how toeffectively control the generation process A straightforward way is to controla model using different conditions during inference However in musicpractice conditions are usually sequential rather than simple categoricallabels involving rich information that overlaps with the learnedrepresentation Consequently the decoder gets confused about whether tolisten to the latent representation or the condition and sometimes justignores the condition To solve this problem we leverage domain adversarialtraining to disentangle the representation from condition cues for bettercontrol Specifically we propose a condition corruption objective that usesthe representation to denoise a corrupted condition Minimized by adiscriminator and maximized by the VAE encoder this objective adversariallyinduces a conditioninvariant representation In this paper we focus on thetask of melody harmonization to illustrate our idea while our methodology canbe generalized to other controllable generative tasks Demos and experimentsshow that our methodology facilitates not only conditioninvariantrepresentation learning but also higherquality controllability compared tobaselines   \n4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Similar to colorization in computer vision instrument separation is toassign instrument labels eg piano guitar to notes from unlabeledmixtures which contain only performance information To address the problem weadopt diffusion models and explicitly guide them to preserve consistencybetween mixtures and music The quantitative results show that our proposedmodel can generate highfidelity samples for multitrack symbolic music withcreativity   \n..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ...   \n266                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Loops seamlessly repeatable musical segments are a cornerstone of modernmusic production Contemporary artists often mix and match various sampled orprerecorded loops based on musical criteria such as rhythm harmony andtimbral texture to create compositions Taking such criteria into account wepresent LoopNet a feedforward generative model for creating loops conditionedon intuitive parameters We leverage Music Information Retrieval MIR modelsas well as a large collection of public loop samples in our study and use theWaveUNet architecture to map control parameters to audio We also evaluatethe quality of the generated audio and propose intuitive controls for composersto map the ideas in their minds to an audio loop   \n267                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             This paper explores the idea of utilising Long ShortTerm Memory neuralnetworks LSTMNN for the generation of musical sequences in ABC notation Theproposed approach takes ABC notations from the Nottingham dataset and encodesit to be fed as input for the neural networks The primary objective is toinput the neural networks with an arbitrary note let the network process andaugment a sequence based on the note until a good piece of music is producedMultiple calibrations have been done to amend the parameters of the network foroptimal generation The output is assessed on the basis of rhythm harmony andgrammar accuracy   \n268  Attentionbased Transformer models have been increasingly employed forautomatic music generation To condition the generation process of such a modelwith a userspecified sequence a popular approach is to take that conditioningsequence as a priming sequence and ask a Transformer decoder to generate acontinuation However this promptbased conditioning cannot guarantee that theconditioning sequence would develop or even simply repeat itself in thegenerated continuation In this paper we propose an alternative conditioningapproach called themebased conditioning that explicitly trains theTransformer to treat the conditioning sequence as a thematic material that hasto manifest itself multiple times in its generation result This is achievedwith two main technical contributions First we propose a deep learningbasedapproach that uses contrastive representation learning and clustering toautomatically retrieve thematic materials from music pieces in the trainingdata Second we propose a novel gated parallel attention module to be used ina sequencetosequence seqseq encoderdecoder architecture to moreeffectively account for a given conditioning thematic material in thegeneration process of the Transformer decoder We report on objective andsubjective evaluations of variants of the proposed Theme Transformer and theconventional promptbased baseline showing that our best model can generateto some extent polyphonic pop piano music with repetition and plausiblevariations of a given condition   \n269                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Dance and music typically go hand in hand The complexities in dance musicand their synchronisation make them fascinating to study from a computationalcreativity perspective While several works have looked at generating dance fora given music automatically generating music for a given dance remainsunderexplored This capability could have several creative expression andentertainment applications We present some early explorations in thisdirection We present a searchbased offline approach that generates musicafter processing the entire dance video and an online approach that uses a deepneural network to generate music onthefly as the video proceeds We comparethese approaches to a strong heuristic baseline via human studies and presentour findings We have integrated our online approach in a live demo A video ofthe demo can be found herehttpssitesgooglecomviewdancemusiclivedemo   \n270                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Symbolic music generation has attracted increasing attention while mostmethods focus on generating short piece mostly less than  bars and up to bars Generating long music calls for effective expression of the coherentmusic structure Despite their success on long sequences selfattentionarchitectures still have challenge in dealing with longterm music as itrequires additional care on the subtle music structure In this paper wepropose to transfer the structure of training samples for new music generationand develop a novel separable selfattention based model which enable thelearning and transferring of the structure embedding We show that our transfermodel can generate music sequences up to  bars with interpretablestructures which bears similar structures and composition techniques with thetemplate music from training set Extensive experiments show its ability ofgenerating music with target structure and well diversity The generated sets of music is uploaded as supplemental material   \n\n     Year Dataset  \n0    2022   ArXiV  \n1    2022   ArXiV  \n2    2022   ArXiV  \n3    2022   ArXiV  \n4    2022   ArXiV  \n..    ...     ...  \n266  2021   ArXiV  \n267  2021   ArXiV  \n268  2021   ArXiV  \n269  2021   ArXiV  \n270  2021   ArXiV  \n\n[271 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Authors</th>\n      <th>Title</th>\n      <th>Abstract</th>\n      <th>Year</th>\n      <th>Dataset</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Carlos Hernandez-Olivan, Jose R. Beltran</td>\n      <td>musicaiz A Python Library for Symbolic Music Generation Analysis and  Visualization</td>\n      <td>In this article we present musicaiz an objectoriented library foranalyzing generating and evaluating symbolic music The submodules of thepackage allow the user to create symbolic music data from scratch buildalgorithms to analyze symbolic music encode MIDI data as tokens to train deeplearning sequence models modify existing music data and evaluate musicgeneration systems The evaluation submodule builds on previous work toobjectively measure music generation systems and to be able to reproduce theresults of music generation models The library is publicly available onlineWe encourage the community to contribute and provide feedback</td>\n      <td>2022</td>\n      <td>ArXiV</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Zhihuan Kuang, Shi Zong, Jianbing Zhang, Jiajun Chen, Hongfu Liu</td>\n      <td>MusictoText Synaesthesia Generating Descriptive Text from Music  Recordings</td>\n      <td>In this paper we consider a novel research problem musictotextsynaesthesia Different from the classical music tagging problem thatclassifies a music recording into predefined categories the musictotextsynaesthesia aims to generate descriptive texts from music recordings forfurther understanding Although this is a new and interesting application tothe machine learning community to our best knowledge the existingmusicrelated datasets do not contain the semantic descriptions on musicrecordings and cannot serve the musictotext synaesthesia task In light ofthis we collect a new dataset that contains  aligned pairs of classicalmusic recordings and text descriptions Based on this we build a computationalmodel to generate sentences that can describe the content of the musicrecording To tackle the highly nondiscriminative classical music we design agroup topologypreservation loss in our computational model which considersmore samples as a group reference and preserves the relative topology amongdifferent samples Extensive experimental results qualitatively andquantitatively demonstrate the effectiveness of our proposed model over fiveheuristics or pretrained competitive methods and their variants on ourcollected dataset</td>\n      <td>2022</td>\n      <td>ArXiV</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Yusong Wu, Josh Gardner, Ethan Manilow, Ian Simon, Curtis Hawthorne,\\n  Jesse Engel</td>\n      <td>The Chamber Ensemble Generator Limitless HighQuality MIR Data via  Generative Modeling</td>\n      <td>Data is the lifeblood of modern machine learning systems including for thosein Music Information Retrieval MIR However MIR has long been mired by smalldatasets and unreliable labels In this work we propose to break thisbottleneck using generative modeling By pipelining a generative model of notesCoconet trained on Bach Chorales with a structured synthesis model of chamberensembles MIDIDDSP trained on URMP we demonstrate a system capable ofproducing unlimited amounts of realistic chorale music with rich annotationsincluding mixes stems MIDI notelevel performance attributes staccatovibrato etc and even finegrained synthesis parameters pitch amplitudeetc We call this system the Chamber Ensemble Generator CEG and use it togenerate a large dataset of chorales from four different chamber ensemblesCocoChorales We demonstrate that data generated using our approach improvesstateoftheart models for music transcription and source separation and werelease both the system and the dataset as an opensource foundation for futurework in the MIR community</td>\n      <td>2022</td>\n      <td>ArXiV</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Jingwei Zhao, Gus Xia, Ye Wang</td>\n      <td>Domain Adversarial Training on Conditional Variational AutoEncoder for  Controllable Music Generation</td>\n      <td>The variational autoencoder has become a leading framework for symbolicmusic generation and a popular research direction is to study how toeffectively control the generation process A straightforward way is to controla model using different conditions during inference However in musicpractice conditions are usually sequential rather than simple categoricallabels involving rich information that overlaps with the learnedrepresentation Consequently the decoder gets confused about whether tolisten to the latent representation or the condition and sometimes justignores the condition To solve this problem we leverage domain adversarialtraining to disentangle the representation from condition cues for bettercontrol Specifically we propose a condition corruption objective that usesthe representation to denoise a corrupted condition Minimized by adiscriminator and maximized by the VAE encoder this objective adversariallyinduces a conditioninvariant representation In this paper we focus on thetask of melody harmonization to illustrate our idea while our methodology canbe generalized to other controllable generative tasks Demos and experimentsshow that our methodology facilitates not only conditioninvariantrepresentation learning but also higherquality controllability compared tobaselines</td>\n      <td>2022</td>\n      <td>ArXiV</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Sangjun Han, Hyeongrae Ihm, DaeHan Ahn, Woohyung Lim</td>\n      <td>Instrument Separation of Symbolic Music by Explicitly Guided Diffusion  Model</td>\n      <td>Similar to colorization in computer vision instrument separation is toassign instrument labels eg piano guitar to notes from unlabeledmixtures which contain only performance information To address the problem weadopt diffusion models and explicitly guide them to preserve consistencybetween mixtures and music The quantitative results show that our proposedmodel can generate highfidelity samples for multitrack symbolic music withcreativity</td>\n      <td>2022</td>\n      <td>ArXiV</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>266</th>\n      <td>266</td>\n      <td>Pritish Chandna, Ant\\'onio Ramires, Xavier Serra, Emilia G\\'omez</td>\n      <td>LoopNet Musical Loop Synthesis Conditioned On Intuitive Musical  Parameters</td>\n      <td>Loops seamlessly repeatable musical segments are a cornerstone of modernmusic production Contemporary artists often mix and match various sampled orprerecorded loops based on musical criteria such as rhythm harmony andtimbral texture to create compositions Taking such criteria into account wepresent LoopNet a feedforward generative model for creating loops conditionedon intuitive parameters We leverage Music Information Retrieval MIR modelsas well as a large collection of public loop samples in our study and use theWaveUNet architecture to map control parameters to audio We also evaluatethe quality of the generated audio and propose intuitive controls for composersto map the ideas in their minds to an audio loop</td>\n      <td>2021</td>\n      <td>ArXiV</td>\n    </tr>\n    <tr>\n      <th>267</th>\n      <td>267</td>\n      <td>Vaishali Ingale, Anush Mohan, Divit Adlakha, Krishan Kumar and Mohit\\n  Gupta</td>\n      <td>Music Generation using Threelayered LSTM</td>\n      <td>This paper explores the idea of utilising Long ShortTerm Memory neuralnetworks LSTMNN for the generation of musical sequences in ABC notation Theproposed approach takes ABC notations from the Nottingham dataset and encodesit to be fed as input for the neural networks The primary objective is toinput the neural networks with an arbitrary note let the network process andaugment a sequence based on the note until a good piece of music is producedMultiple calibrations have been done to amend the parameters of the network foroptimal generation The output is assessed on the basis of rhythm harmony andgrammar accuracy</td>\n      <td>2021</td>\n      <td>ArXiV</td>\n    </tr>\n    <tr>\n      <th>268</th>\n      <td>268</td>\n      <td>Yi-Jen Shih, Shih-Lun Wu, Frank Zalkow, Meinard M\\\"uller, Yi-Hsuan\\n  Yang</td>\n      <td>Theme Transformer Symbolic Music Generation with ThemeConditioned  Transformer</td>\n      <td>Attentionbased Transformer models have been increasingly employed forautomatic music generation To condition the generation process of such a modelwith a userspecified sequence a popular approach is to take that conditioningsequence as a priming sequence and ask a Transformer decoder to generate acontinuation However this promptbased conditioning cannot guarantee that theconditioning sequence would develop or even simply repeat itself in thegenerated continuation In this paper we propose an alternative conditioningapproach called themebased conditioning that explicitly trains theTransformer to treat the conditioning sequence as a thematic material that hasto manifest itself multiple times in its generation result This is achievedwith two main technical contributions First we propose a deep learningbasedapproach that uses contrastive representation learning and clustering toautomatically retrieve thematic materials from music pieces in the trainingdata Second we propose a novel gated parallel attention module to be used ina sequencetosequence seqseq encoderdecoder architecture to moreeffectively account for a given conditioning thematic material in thegeneration process of the Transformer decoder We report on objective andsubjective evaluations of variants of the proposed Theme Transformer and theconventional promptbased baseline showing that our best model can generateto some extent polyphonic pop piano music with repetition and plausiblevariations of a given condition</td>\n      <td>2021</td>\n      <td>ArXiV</td>\n    </tr>\n    <tr>\n      <th>269</th>\n      <td>269</td>\n      <td>Gunjan Aggarwal, Devi Parikh</td>\n      <td>DanceMusic Automatic Dancedriven Music Generation</td>\n      <td>Dance and music typically go hand in hand The complexities in dance musicand their synchronisation make them fascinating to study from a computationalcreativity perspective While several works have looked at generating dance fora given music automatically generating music for a given dance remainsunderexplored This capability could have several creative expression andentertainment applications We present some early explorations in thisdirection We present a searchbased offline approach that generates musicafter processing the entire dance video and an online approach that uses a deepneural network to generate music onthefly as the video proceeds We comparethese approaches to a strong heuristic baseline via human studies and presentour findings We have integrated our online approach in a live demo A video ofthe demo can be found herehttpssitesgooglecomviewdancemusiclivedemo</td>\n      <td>2021</td>\n      <td>ArXiV</td>\n    </tr>\n    <tr>\n      <th>270</th>\n      <td>270</td>\n      <td>Ning Zhang and Junchi Yan</td>\n      <td>Melody Structure Transfer Network Generating Music with Separable  SelfAttention</td>\n      <td>Symbolic music generation has attracted increasing attention while mostmethods focus on generating short piece mostly less than  bars and up to bars Generating long music calls for effective expression of the coherentmusic structure Despite their success on long sequences selfattentionarchitectures still have challenge in dealing with longterm music as itrequires additional care on the subtle music structure In this paper wepropose to transfer the structure of training samples for new music generationand develop a novel separable selfattention based model which enable thelearning and transferring of the structure embedding We show that our transfermodel can generate music sequences up to  bars with interpretablestructures which bears similar structures and composition techniques with thetemplate music from training set Extensive experiments show its ability ofgenerating music with target structure and well diversity The generated sets of music is uploaded as supplemental material</td>\n      <td>2021</td>\n      <td>ArXiV</td>\n    </tr>\n  </tbody>\n</table>\n<p>271 rows × 6 columns</p>\n</div>"},"metadata":{}}]}]}